<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3 장 Tree-based 모형 | R에 의한 통계자료분석</title>
  <meta name="description" content="한신대학교 응용통계학과 전공 과목 통계자료분석실습의 교재입니다." />
  <meta name="generator" content="bookdown 0.35 and GitBook 2.6.7" />

  <meta property="og:title" content="3 장 Tree-based 모형 | R에 의한 통계자료분석" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="한신대학교 응용통계학과 전공 과목 통계자료분석실습의 교재입니다." />
  

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3 장 Tree-based 모형 | R에 의한 통계자료분석" />
  
  <meta name="twitter:description" content="한신대학교 응용통계학과 전공 과목 통계자료분석실습의 교재입니다." />
  

<meta name="author" content="박동련" />


<meta name="date" content="2023-09-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="chapter-Logistic.html"/>
<link rel="next" href="references.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<script src="libs/plotly-binding-4.10.2/plotly.js"></script>
<script src="libs/typedarray-0.1/typedarray.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>
<link href="libs/plotly-htmlwidgets-css-2.11.1/plotly-htmlwidgets.css" rel="stylesheet" />
<script src="libs/plotly-main-2.11.1/plotly-latest.min.js"></script>
<script src="libs/kePrint-0.0.1/kePrint.js"></script>
<link href="libs/lightable-0.0.1/lightable.css" rel="stylesheet" />


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">R에 의한 통계자료분석 실습</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>머리말</a></li>
<li class="chapter" data-level="1" data-path="chapter-regression.html"><a href="chapter-regression.html"><i class="fa fa-check"></i><b>1</b> 선형회귀모형</a>
<ul>
<li class="chapter" data-level="1.1" data-path="chapter-regression.html"><a href="chapter-regression.html#section-fitting"><i class="fa fa-check"></i><b>1.1</b> 회귀모형 적합</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="chapter-regression.html"><a href="chapter-regression.html#단순선형회귀모형"><i class="fa fa-check"></i><b>1.1.1</b> 단순선형회귀모형</a></li>
<li class="chapter" data-level="1.1.2" data-path="chapter-regression.html"><a href="chapter-regression.html#section-multi-reg-fitting"><i class="fa fa-check"></i><b>1.1.2</b> 다중선형회귀모형</a></li>
<li class="chapter" data-level="1.1.3" data-path="chapter-regression.html"><a href="chapter-regression.html#다항회귀모형"><i class="fa fa-check"></i><b>1.1.3</b> 다항회귀모형</a></li>
<li class="chapter" data-level="1.1.4" data-path="chapter-regression.html"><a href="chapter-regression.html#section-dummy"><i class="fa fa-check"></i><b>1.1.4</b> 가변수 회귀모형</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="chapter-regression.html"><a href="chapter-regression.html#section-infer"><i class="fa fa-check"></i><b>1.2</b> 회귀모형의 추론</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="chapter-regression.html"><a href="chapter-regression.html#회귀모형의-유의성-검정"><i class="fa fa-check"></i><b>1.2.1</b> 회귀모형의 유의성 검정</a></li>
<li class="chapter" data-level="1.2.2" data-path="chapter-regression.html"><a href="chapter-regression.html#개별회귀계수-유의성-검정"><i class="fa fa-check"></i><b>1.2.2</b> 개별회귀계수 유의성 검정</a></li>
<li class="chapter" data-level="1.2.3" data-path="chapter-regression.html"><a href="chapter-regression.html#두-회귀모형의-비교"><i class="fa fa-check"></i><b>1.2.3</b> 두 회귀모형의 비교</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="chapter-regression.html"><a href="chapter-regression.html#section-selection"><i class="fa fa-check"></i><b>1.3</b> 변수선택</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="chapter-regression.html"><a href="chapter-regression.html#section-fit-measure"><i class="fa fa-check"></i><b>1.3.1</b> 회귀모형의 평가 측도</a></li>
<li class="chapter" data-level="1.3.2" data-path="chapter-regression.html"><a href="chapter-regression.html#평가-측도에-의한-변수선택"><i class="fa fa-check"></i><b>1.3.2</b> 평가 측도에 의한 변수선택</a></li>
<li class="chapter" data-level="1.3.3" data-path="chapter-regression.html"><a href="chapter-regression.html#shrinkage-방법"><i class="fa fa-check"></i><b>1.3.3</b> Shrinkage 방법</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="chapter-regression.html"><a href="chapter-regression.html#section-diagnostic"><i class="fa fa-check"></i><b>1.4</b> 회귀진단</a>
<ul>
<li class="chapter" data-level="1.4.1" data-path="chapter-regression.html"><a href="chapter-regression.html#section-assumption-error"><i class="fa fa-check"></i><b>1.4.1</b> 회귀모형의 가정 만족 여부 확인</a></li>
<li class="chapter" data-level="1.4.2" data-path="chapter-regression.html"><a href="chapter-regression.html#section-influence-data"><i class="fa fa-check"></i><b>1.4.2</b> 특이한 관찰값 탐지</a></li>
</ul></li>
<li class="chapter" data-level="1.5" data-path="chapter-regression.html"><a href="chapter-regression.html#section-prediction"><i class="fa fa-check"></i><b>1.5</b> 예측</a></li>
<li class="chapter" data-level="1.6" data-path="chapter-regression.html"><a href="chapter-regression.html#실습-예제"><i class="fa fa-check"></i><b>1.6</b> 실습 예제</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html"><i class="fa fa-check"></i><b>2</b> 로지스틱 회귀모형</a>
<ul>
<li class="chapter" data-level="2.1" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#limitation"><i class="fa fa-check"></i><b>2.1</b> 이항반응변수에 대한 선형회귀모형의 한계</a></li>
<li class="chapter" data-level="2.2" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#로지스틱-회귀모형"><i class="fa fa-check"></i><b>2.2</b> 로지스틱 회귀모형</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#logistic-reg-fitting"><i class="fa fa-check"></i><b>2.2.1</b> 모형 적합</a></li>
<li class="chapter" data-level="2.2.2" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#확률-예측"><i class="fa fa-check"></i><b>2.2.2</b> 확률 예측</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#section-logistic-selection"><i class="fa fa-check"></i><b>2.3</b> 변수선택</a></li>
<li class="chapter" data-level="2.4" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#section-logistic-diagnostic"><i class="fa fa-check"></i><b>2.4</b> 회귀진단</a></li>
<li class="chapter" data-level="2.5" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#classification"><i class="fa fa-check"></i><b>2.5</b> 분류성능 평가</a>
<ul>
<li class="chapter" data-level="2.5.1" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#분류성능-평가-측도"><i class="fa fa-check"></i><b>2.5.1</b> 분류성능 평가 측도</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#희귀사건의-분류"><i class="fa fa-check"></i><b>2.6</b> 희귀사건의 분류</a></li>
<li class="chapter" data-level="2.7" data-path="chapter-Logistic.html"><a href="chapter-Logistic.html#실습-예제-1"><i class="fa fa-check"></i><b>2.7</b> 실습 예제</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html"><i class="fa fa-check"></i><b>3</b> Tree-based 모형</a>
<ul>
<li class="chapter" data-level="3.1" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#section-decision-tree"><i class="fa fa-check"></i><b>3.1</b> Decision tree 모형</a>
<ul>
<li class="chapter" data-level="3.1.1" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#section-reg-tree"><i class="fa fa-check"></i><b>3.1.1</b> Regression tree 모형</a></li>
<li class="chapter" data-level="3.1.2" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#section-classification-tree"><i class="fa fa-check"></i><b>3.1.2</b> Classification tree 모형</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#section-bagging"><i class="fa fa-check"></i><b>3.2</b> Bagging</a></li>
<li class="chapter" data-level="3.3" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#section-rf"><i class="fa fa-check"></i><b>3.3</b> Random Forest</a></li>
<li class="chapter" data-level="3.4" data-path="chapter-tree-model.html"><a href="chapter-tree-model.html#boosting"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">R에 의한 통계자료분석</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="chapter-tree-model" class="section level1 hasAnchor" number="3">
<h1><span class="header-section-number">3 장</span> Tree-based 모형<a href="chapter-tree-model.html#chapter-tree-model" class="anchor-section" aria-label="Anchor link to header"></a></h1>
<p><a href="chapter-regression.html#chapter-regression">1</a>장에서는 반응변수가 연속형인 경우에 적용되는 회귀모형에 대해 살펴보았고,
<a href="chapter-Logistic.html#chapter-Logistic">2</a>장에서는 반응변수가 이항변수인 경우에 적용되는 분류모형에 대해 살펴보았다.
지금부터는 연속형 반응변수에 적용되는 회귀모형과 이항반응변수에 적용되는 분류모형에 모두 적용될 수 있는 “tree-based” 모형에 대해 살펴보고자 한다.</p>
<p>Tree-based 모형은 기본적으로 설명변수 공간의 분할을 근거로 하고 있다.
설명변수의 공간의 분할 결과는 tree 형태로 표현되며,
모형의 예측은 동일 공간에 속한 자료의 평균값(회귀모형)이거나, 최빈값(분류모형)으로 이루어진다.</p>
<p>Tree-based 모형은 해석이 쉽고 편하다는 장점이 있으나,
<a href="chapter-regression.html#chapter-regression">1</a>장이나 <a href="chapter-Logistic.html#chapter-Logistic">2</a>장에서 살펴본 모형에 비하여 예측의 정확도나 효율성이 상당히 떨어진다는 문제가 있다.
예측의 정확도를 높이기 위해 대안으로 제시되는 tree-based 모형에는 <em>Bagging</em>, <em>random forest</em>, <em>boosting</em> 모형이 있다.
이 방식들의 공통점은 많은 수의 tree 모형을 적합시키고, 그 결과를 결합해서 예측하는 것이다.</p>
<div id="section-decision-tree" class="section level2 hasAnchor" number="3.1">
<h2><span class="header-section-number">3.1</span> Decision tree 모형<a href="chapter-tree-model.html#section-decision-tree" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<div id="section-reg-tree" class="section level3 hasAnchor" number="3.1.1">
<h3><span class="header-section-number">3.1.1</span> Regression tree 모형<a href="chapter-tree-model.html#section-reg-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>연속형 반응변수에 대한 tree 모형을 regression tree 모형이라고 한다.
Regression tree 모형에 대한 예제 자료로 패키지 <code>ISLR</code>에 있는 데이터 프레임 <code>Hitters</code>를 사용해 보자.
<code>Hitters</code>는 MLB 1986~87 시즌 322명 선수의 연봉에 대한 자료이다.</p>
<div class="sourceCode" id="cb194"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb194-1"><a href="chapter-tree-model.html#cb194-1" tabindex="-1"></a><span class="fu">data</span>(Hitters, <span class="at">package=</span><span class="st">&quot;ISLR&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb195"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb195-1"><a href="chapter-tree-model.html#cb195-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb195-2"><a href="chapter-tree-model.html#cb195-2" tabindex="-1"></a>Hitters <span class="sc">|&gt;</span> <span class="fu">as_tibble</span>() <span class="sc">|&gt;</span> <span class="fu">print</span>(<span class="at">n =</span> <span class="dv">5</span>)</span>
<span id="cb195-3"><a href="chapter-tree-model.html#cb195-3" tabindex="-1"></a><span class="do">## # A tibble: 322 × 20</span></span>
<span id="cb195-4"><a href="chapter-tree-model.html#cb195-4" tabindex="-1"></a><span class="do">##   AtBat  Hits HmRun  Runs   RBI Walks Years CAtBat CHits CHmRun CRuns  CRBI</span></span>
<span id="cb195-5"><a href="chapter-tree-model.html#cb195-5" tabindex="-1"></a><span class="do">##   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt; &lt;int&gt;</span></span>
<span id="cb195-6"><a href="chapter-tree-model.html#cb195-6" tabindex="-1"></a><span class="do">## 1   293    66     1    30    29    14     1    293    66      1    30    29</span></span>
<span id="cb195-7"><a href="chapter-tree-model.html#cb195-7" tabindex="-1"></a><span class="do">## 2   315    81     7    24    38    39    14   3449   835     69   321   414</span></span>
<span id="cb195-8"><a href="chapter-tree-model.html#cb195-8" tabindex="-1"></a><span class="do">## 3   479   130    18    66    72    76     3   1624   457     63   224   266</span></span>
<span id="cb195-9"><a href="chapter-tree-model.html#cb195-9" tabindex="-1"></a><span class="do">## 4   496   141    20    65    78    37    11   5628  1575    225   828   838</span></span>
<span id="cb195-10"><a href="chapter-tree-model.html#cb195-10" tabindex="-1"></a><span class="do">## 5   321    87    10    39    42    30     2    396   101     12    48    46</span></span>
<span id="cb195-11"><a href="chapter-tree-model.html#cb195-11" tabindex="-1"></a><span class="do">## # ℹ 317 more rows</span></span>
<span id="cb195-12"><a href="chapter-tree-model.html#cb195-12" tabindex="-1"></a><span class="do">## # ℹ 8 more variables: CWalks &lt;int&gt;, League &lt;fct&gt;, Division &lt;fct&gt;,</span></span>
<span id="cb195-13"><a href="chapter-tree-model.html#cb195-13" tabindex="-1"></a><span class="do">## #   PutOuts &lt;int&gt;, Assists &lt;int&gt;, Errors &lt;int&gt;, Salary &lt;dbl&gt;, NewLeague &lt;fct&gt;</span></span></code></pre></div>
<p><code>log(Salary)</code>를 반응변수, <code>Years</code>와 <code>Hits</code>를 설명변수로 하는 regression tree를 구성해 보자.
적합된 tree 모형은 그림 <a href="chapter-tree-model.html#fig:rtree-1">3.1</a>에서 볼 수 있다.
적합된 tree 모형은 ‘root node’를 시작으로 <code>Years &lt; 4.5</code>를 기준으로 첫 번째 분리가 이루어졌고,
<code>Hits &lt; 117.5</code>를 기준으로 두 번째 분리가 이루어진 것을 알 수 있다.
두 번의 분리로 새 개의 ’terminal node’ 혹은 ’leaves’가 생성되었다.</p>
<div class="figure"><span style="display:block;" id="fig:rtree-1"></span>
<img src="_main_files/figure-html/rtree-1-1.png" alt="`Hitters` 자료에 대한 regression tree 모형 적합 결과" width="576" />
<p class="caption">
그림 3.1: <code>Hitters</code> 자료에 대한 regression tree 모형 적합 결과
</p>
</div>
<p>Tree 모형의 적합 과정은 설명변수의 공간을 분할하여, 동일 공간에 속한 반응변수 자료의 동질성을 최대화하는 것이다.
그림 <a href="chapter-tree-model.html#fig:rtree-1">3.1</a>에 표현된 tree 모형은 설명변수 <code>Years</code>와 <code>Hits</code>로 구성된 2차원 공간을 <code>log(Salary)</code>가 동일 공간에서는 최대한 유사한 값을 갖도록 세 개 공간으로 분할한 것이다.
분할된 공간 R1, R2, R3를 표현한 그래프는 그림 <a href="chapter-tree-model.html#fig:rtree-2">3.2</a>에서 볼 수 있다.</p>
<div class="figure"><span style="display:block;" id="fig:rtree-2"></span>
<img src="_main_files/figure-html/rtree-2-1.png" alt="`Hitters` 자료에 대한 regression tree 모형의 설명변수 공간 분할" width="480" />
<p class="caption">
그림 3.2: <code>Hitters</code> 자료에 대한 regression tree 모형의 설명변수 공간 분할
</p>
</div>
<p><strong><span class="math inline">\(\bullet\)</span> Tree 모형 적합: 설명변수 공간의 분할</strong></p>
<p>Tree 모형 적합을 위한 설명변수의 공간 분할 방법에 대해 살펴보자.
공간 분할의 기본적인 목표는 동일 공간에 속한 자료들의 동질성을 최대화하는 것이며,
연속형 자료의 동질성은 분산의 개념을 이용해서 측정할 수 있다.
즉, 설명변수 <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{k}\)</span> 로 구성되는 전체 공간을 서로 겹치지 않는 <span class="math inline">\(J\)</span> 개의 공간으로 분리하고자 한다면,
예측오차의 평가측도인 식 <a href="chapter-tree-model.html#eq:tree-rss-1">(3.1)</a>의 RSS가 최소가 되도록 <span class="math inline">\(R_{1}, R_{2}, \ldots, R_{J}\)</span> 를 구성하는 것이다.</p>
<p><span class="math display" id="eq:tree-rss-1">\[\begin{equation}
RSS = \sum_{j=1}^{J} \sum_{i \in R_{j}} \left(y_{i}-\overline{y}_{R_{j}} \right)^{2}
\tag{3.1}
\end{equation}\]</span></p>
<p>그러나 공간 분할에 대한 구체적인 제약 조건이 없는 상태에서 식 <a href="chapter-tree-model.html#eq:tree-rss-1">(3.1)</a>을 최소화시키는 최적 분할을 찾는 것은 불가능한 문제이다.
우리가 적용하려는 방식은 이른바 “top-down” 방식 혹은 “recursive binary splitting”이라고 하는 방식이다.
단계적으로 RSS를 가장 크게 감소시킬 수 있는 분할을 실시하되, 이전 단계의 분할로 구성된 영역 중 하나를 선택해서 두 개의 영역으로 분리하는 것인데,
이러한 분할은 설명변수 <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{k}\)</span> 중 한 변수를 선택하고, 이어서 해당 변수의 분할 기준점을 찾는 작업으로 이루어진다.</p>
<p>분할 전 RSS는 전체 자료가 대상이므로 <span class="math inline">\(RSS = \sum \left(y_{i}-\overline{y}\right)^{2}\)</span> 가 된다.
첫 번째 분할은 식 <a href="chapter-tree-model.html#eq:tree-rss-2">(3.2)</a>의 RSS를 최소화시키는 분할 <span class="math inline">\(R_{1}\)</span> 과 <span class="math inline">\(R_{2}\)</span> 를 구성하는 것이다.
단, <span class="math inline">\(R_{1}\)</span>은 <span class="math inline">\(X_{j}&lt;s\)</span> 를 만족시키는 공간이고, <span class="math inline">\(R_{2}\)</span>는 <span class="math inline">\(X_{j} \geq s\)</span> 를 만족시키는 공간이다.</p>
<p><span class="math display" id="eq:tree-rss-2">\[\begin{equation}
RSS = \sum_{i \in R_{1}} \left(y_{i}-\overline{y}_{R_{1}}\right)^{2} + \sum_{i \in R_{2}} \left(y_{i}-\overline{y}_{R_{2}}\right)^{2}
\tag{3.2}
\end{equation}\]</span></p>
<p>두 번째 분할은 첫 번째 분할로 구성된 2개의 영역 중에서 RSS를 가장 크게 감소시킬 수 있도록 한 영역을 선택하고 분할 기준점을 찾아 두 영역으로 분리하는 것이다.
두 번째 분할이 이루어지면 총 3개의 영역이 구성된다.
이후의 분할 과정도 동일한 방식으로 진행되며, 미리 설정된 stopping rule이 만족될 때까지 반복된다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> Tree pruning</strong></p>
<p>예제로 <code>ISLR::Hitters</code>에서 <code>log(Salary)</code>를 반응변수로, <code>Years</code>, <code>Hits</code>, <code>RBI</code>, <code>Walks</code>, <code>PutOuts</code>를 설명변수로 하는 tree 모형의 적합결과를 그래프로 나태내 보자.</p>
<div class="figure"><span style="display:block;" id="fig:rtree-3"></span>
<img src="_main_files/figure-html/rtree-3-1.png" alt="`Hitters` 자료에 대한 regression tree 모형 적합 결과" width="768" />
<p class="caption">
그림 3.3: <code>Hitters</code> 자료에 대한 regression tree 모형 적합 결과
</p>
</div>
<p>적합된 tree 모형은 19개의 terminal nodes가 있는 비교적 복잡한 형태의 모형이다.
지나치게 많은 분할이 이루어진 tree 모형은 training data의 세밀한 특징을 잘 나타낼 수 있겠지만,
test data와 같은 새로운 자료에 대해서는 매우 부정확한 예측을 하게 되는 이른바 overfitting의 문제가 있을 수 있다.
반면에 작은 횟수의 분할로 적합된 작은 크기의 tree 모형은 training data에 대한 설명력이 조금은 떨어져서 bias가 커지는 문제가 있겠지만,
예측 결과의 변동성은 낮아지는, 즉 variance가 작아지는 효과를 볼 수 있다.
따라서 가장 적절한 크기의 tree 모형을 선택할 수 있는 방법이 필요하며, 이것은 tree pruning이라고 한다.</p>
<p>우리가 적용할 방법은 모형의 복잡성, 즉 tree 모형의 크기가 미치는 영향력을 조절하는 tuning parameter를 사용하는 방법이다.
식 <a href="chapter-tree-model.html#eq:tree-rss-1">(3.1)</a>에 정의된 RSS는 분할이 진행되어 모형의 복잡성이 높아질수록 감소하는 특징을 갖고 있는데,
여기에 모형의 복잡성을 penalty로 추가함으로써, 모형의 크기를 고려한 예측오차를 정의할 수 있다.</p>
<p><span class="math display" id="eq:tree-cp-1">\[\begin{equation}
RSS_{c_{p}} = RSS + c_{p} \cdot |T| \tag{3.3}
\end{equation}\]</span></p>
<p>단, <span class="math inline">\(|T|\)</span> 는 tree 모형의 terminal nodes의 개수이고, <span class="math inline">\(c_{p}\)</span> 는 0 또는 양수를 값으로 갖는 tuning parameter이다.
<span class="math inline">\(c_{p}\)</span> 의 역할은 모형의 복잡성, 즉 tree 모형 크기의 영향력을 조절하는 것으로써,
만일 <span class="math inline">\(c_{p}=0\)</span> 이면 <span class="math inline">\(RSS_{c_{p}}\)</span> 는 식 <a href="chapter-tree-model.html#eq:tree-rss-1">(3.1)</a>에 정의된 RSS와 동일한 것이 되며,
<span class="math inline">\(c_{p}\)</span> 의 값이 증가함에 따라 tree 모형의 크기는 감소하게 된다.
최적 <span class="math inline">\(c_{p}\)</span> 값은 cross-validataion에 의한 예측오차 계산으로 선택할 수 있다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> Cross-validation</strong></p>
<p>Test error는 통계모형의 적합에 사용되지 않은 새로운 자료에 대한 예측 오차를 의미하며,
특정 통계모형에 의한 예측의 정당성을 확보하기 위해서는 test error가 낮다는 것을 확인할 수 있어야 한다.
그렇다면 Test error를 효과적으로 추정할 수 있는 방법이 무엇인지 살펴보자.<br />
전체 data를 training data와 test data로 분리해서 test data set을 확보할 수 있지만,
충분한 양을 확보하기에는 현실적인 어려움이 있을 수 있다.</p>
<p>Cross-validation은 training data를 이용한 test error의 추정방법이다.
Training data 중 일부분을 모형적합 과정에서 제외해서 test 목적으로 사용하는 것인데,
자료의 제외 방식 등에 따라서 몇 가지 방법으로 구분된다.
그 중에서 “leave-one-out cross-validation”과 “k-fold cross-validation”에 대해 살펴보자.</p>
<ul>
<li>Leave-one-out cross-validation (LOOCV)</li>
</ul>
<p>전체 <span class="math inline">\(n\)</span> 개의 자료 중 한 개의 자료를 모형 적합에서 차례로 제외하고 test 목적으로 사용하는 방식이다.
즉, 자료 <span class="math inline">\((\mathbf{x}_{1}, y_{1}), \ldots, (\mathbf{x}_{n}, y_{n})\)</span> 중 <span class="math inline">\((\mathbf{x}_{i}, y_{i})\)</span>, <span class="math inline">\(i=1, 2, \ldots, n\)</span> 을 제외하고 적합한 모형으로 <span class="math inline">\(y_{i}\)</span> 를 예측하는 과정을 <span class="math inline">\(n\)</span> 번 반복하며,
그 과정에서 발생한 오차를 근거로 test error를 다음과 같이 추정하는 방식이다.</p>
<p><span class="math display" id="eq:loocv">\[\begin{equation}
\frac{1}{n} \sum_{i=1}^{n} \left(y_{i}-\hat{y}_{i}\right)^{2}
\tag{3.4}
\end{equation}\]</span></p>
<p>전체 자료 중 한 개의 자료만 제외되므로 자료의 크기가 큰 대규모의 자료인 경우에는 많은 계산 과정이 필요한 방식이다.
대부분의 자료가 사용되기 때문에 예측 bias는 작겠지만,
<span class="math inline">\(n\)</span> 번의 적합 과정에서 사용된 자료가 거의 비슷하기 떄문에, 각 모형의 예측 결과 사이에는 높은 상관관계가 존재하게 되고,
따라서 예측 결과의 분산이 커지는 문제가 있다.</p>
<ul>
<li>k-fold cross-validation</li>
</ul>
<p>먼저 training data를 비슷한 크기의 <span class="math inline">\(k\)</span> 개의 그룹(fold)으로 구분한다.
이어서 그 중 한 그룹의 자료를 제외하고 나머지 자료만으로 모형을 적합한 후에 제외된 한 그룹 자료의 반응변수를 예측하고 예측 오차를 계산한다.
이 과정을 <span class="math inline">\(k\)</span> 번 차례로 반복하면 <span class="math inline">\(k\)</span> 개의 예측 오차를 얻게 되는데,
그 오차의 평균으로 test error를 추정하는 방식이다.</p>
<p>LOOCV에 비해 계산 과정이 단순한 방식인데,
모형적합 과정에 사용된 자료의 개수가 LOOCV 보다 작기 때문에 bias는 더 클 수 있다.
그러나 한 그룹의 자료가 차례로 제외된 상태에서 <span class="math inline">\(k\)</span> 번의 모형 적합이 이루어지기 때문에,
각 모형 적합에 사용된 자료 중 겹치는 자료의 비율은 더 낮아지게 되고,
따라서 예측 결과 사이의 상관관계가 더 낮게 형성될 수 있어서,
예측의 분산을 낮출 수 있는 효과가 있다.</p>
<p><span class="math inline">\(k\)</span> 값을 조정하면 bias와 variance 사이의 trade-off가 가능한데,
일반적으로 사용되는 것은 <span class="math inline">\(k=5\)</span> 또는 <span class="math inline">\(k=10\)</span> 이다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> 예제: <code>MASS::Boston</code></strong></p>
<p><code>Boston</code>은 보스톤 지역의 주택 가격에 대한 자료이다.</p>
<div class="sourceCode" id="cb196"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb196-1"><a href="chapter-tree-model.html#cb196-1" tabindex="-1"></a><span class="fu">data</span>(Boston, <span class="at">package =</span> <span class="st">&quot;MASS&quot;</span>)</span>
<span id="cb196-2"><a href="chapter-tree-model.html#cb196-2" tabindex="-1"></a><span class="fu">str</span>(Boston)</span>
<span id="cb196-3"><a href="chapter-tree-model.html#cb196-3" tabindex="-1"></a><span class="do">## &#39;data.frame&#39;:    506 obs. of  14 variables:</span></span>
<span id="cb196-4"><a href="chapter-tree-model.html#cb196-4" tabindex="-1"></a><span class="do">##  $ crim   : num  0.00632 0.02731 0.02729 0.03237 0.06905 ...</span></span>
<span id="cb196-5"><a href="chapter-tree-model.html#cb196-5" tabindex="-1"></a><span class="do">##  $ zn     : num  18 0 0 0 0 0 12.5 12.5 12.5 12.5 ...</span></span>
<span id="cb196-6"><a href="chapter-tree-model.html#cb196-6" tabindex="-1"></a><span class="do">##  $ indus  : num  2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ...</span></span>
<span id="cb196-7"><a href="chapter-tree-model.html#cb196-7" tabindex="-1"></a><span class="do">##  $ chas   : int  0 0 0 0 0 0 0 0 0 0 ...</span></span>
<span id="cb196-8"><a href="chapter-tree-model.html#cb196-8" tabindex="-1"></a><span class="do">##  $ nox    : num  0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ...</span></span>
<span id="cb196-9"><a href="chapter-tree-model.html#cb196-9" tabindex="-1"></a><span class="do">##  $ rm     : num  6.58 6.42 7.18 7 7.15 ...</span></span>
<span id="cb196-10"><a href="chapter-tree-model.html#cb196-10" tabindex="-1"></a><span class="do">##  $ age    : num  65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ...</span></span>
<span id="cb196-11"><a href="chapter-tree-model.html#cb196-11" tabindex="-1"></a><span class="do">##  $ dis    : num  4.09 4.97 4.97 6.06 6.06 ...</span></span>
<span id="cb196-12"><a href="chapter-tree-model.html#cb196-12" tabindex="-1"></a><span class="do">##  $ rad    : int  1 2 2 3 3 3 5 5 5 5 ...</span></span>
<span id="cb196-13"><a href="chapter-tree-model.html#cb196-13" tabindex="-1"></a><span class="do">##  $ tax    : num  296 242 242 222 222 222 311 311 311 311 ...</span></span>
<span id="cb196-14"><a href="chapter-tree-model.html#cb196-14" tabindex="-1"></a><span class="do">##  $ ptratio: num  15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ...</span></span>
<span id="cb196-15"><a href="chapter-tree-model.html#cb196-15" tabindex="-1"></a><span class="do">##  $ black  : num  397 397 393 395 397 ...</span></span>
<span id="cb196-16"><a href="chapter-tree-model.html#cb196-16" tabindex="-1"></a><span class="do">##  $ lstat  : num  4.98 9.14 4.03 2.94 5.33 ...</span></span>
<span id="cb196-17"><a href="chapter-tree-model.html#cb196-17" tabindex="-1"></a><span class="do">##  $ medv   : num  24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ...</span></span></code></pre></div>
<p>변수 <code>medv</code>를 반응변수로 하는 tree 모형을 적합해 보자.
자료탐색 과정은 생략하고 tree 모형적합 절차만을 살펴보도록 하자.
분석의 첫 번째 단계는 전체 자료를 training data와 test data로 분리하는 것이다.</p>
<div class="sourceCode" id="cb197"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb197-1"><a href="chapter-tree-model.html#cb197-1" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb197-2"><a href="chapter-tree-model.html#cb197-2" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span></code></pre></div>
<div class="sourceCode" id="cb198"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb198-1"><a href="chapter-tree-model.html#cb198-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb198-2"><a href="chapter-tree-model.html#cb198-2" tabindex="-1"></a>train.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Boston<span class="sc">$</span>medv, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb198-3"><a href="chapter-tree-model.html#cb198-3" tabindex="-1"></a>train_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(train.id)</span>
<span id="cb198-4"><a href="chapter-tree-model.html#cb198-4" tabindex="-1"></a>test_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>train.id)</span></code></pre></div>
<p>Tree 모형의 적합은 패키지 <code>caret</code>의 함수 <code>train()</code>으로 진행할 것이다.
패키지 <code>caret</code>은 Machine learning에 최적화된 패키지로써 분석에 필수적인 기능을 모두 포함하고 있으며,
다양한 modelling 기법을 표준화된 방식으로 사용할 수 있다.</p>
<p>함수 <code>train()</code>은 ML 모형의 적합을 위해 사용되는 함수인데, 239 종류의 ML 모형 적합이 가능하다.
작동 방식은 각 ML 모형의 적합을 위한 패키지에서 필요한 함수를 불러와 적용시키는 것인데,
표준화된 방식을 사용하기 때문에 사용자가 편하게 사용할 수 있다.
모형 선택은 <code>method</code>에 각 ML 모형의 키워드를 지정하면 된다.
Tree 모형의 경우에는 <code>method = 'rpart'</code>를 입력하면 되며, 필요한 패키지인 <code>rpart</code>는 설치되어야 한다.
또한 각 ML 모형마다 필요한 tuning parameter를 <code>TuneLength</code> 또는 <code>TuneGrid</code>를 통해서 지정할 수 있다.</p>
<p>함수 <code>train()</code>을 사용해서 tree 모형을 적합해 보자.
모형 적합은 식 <a href="chapter-tree-model.html#eq:tree-cp-1">(3.3)</a>에 정의된 방식을 사용하되, 10-fold CV으로 tuning parameter <span class="math inline">\(c_{p}\)</span> 의 최적 값을 찾아보자.</p>
<div class="sourceCode" id="cb199"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb199-1"><a href="chapter-tree-model.html#cb199-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb199-2"><a href="chapter-tree-model.html#cb199-2" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb199-3"><a href="chapter-tree-model.html#cb199-3" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb199-4"><a href="chapter-tree-model.html#cb199-4" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>))</span></code></pre></div>
<p>함수 <code>train()</code>에서 모형 설정은 함수 <code>lm()</code>과 동일하게 R formula 방식을 사용할 수 있으며,
반응변수가 연속형이면 regression tree 모형이 적합된다.
요소 <code>method</code>에 <code>"rpart"</code>를 지정하면, 예측 오차가 최소인 tree 모형을 적합한다.
요소 <code>tuneLength</code>에는 최적 tuning parameter 값을 찾기 위한 grid의 길이를 지정할 수 있다.
요소 <code>trControl</code>은 모형의 적합 및 평가 방식 등을 설정하는 기능을 갖고 있는데,
함수 <code>trainControl()</code>을 사용해서 지정하게 된다.
함수 <code>trainControl()</code>의 요소 <code>method</code>에는 적용되는 CV 방식을 정할 수 있는데, <code>method = 'cv', number = 10</code>은 10-fold CV를 의미한다.</p>
<p>적합된 결과를 살펴보자.</p>
<div class="sourceCode" id="cb200"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb200-1"><a href="chapter-tree-model.html#cb200-1" tabindex="-1"></a>m1</span>
<span id="cb200-2"><a href="chapter-tree-model.html#cb200-2" tabindex="-1"></a><span class="do">## CART </span></span>
<span id="cb200-3"><a href="chapter-tree-model.html#cb200-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb200-4"><a href="chapter-tree-model.html#cb200-4" tabindex="-1"></a><span class="do">## 356 samples</span></span>
<span id="cb200-5"><a href="chapter-tree-model.html#cb200-5" tabindex="-1"></a><span class="do">##  13 predictor</span></span>
<span id="cb200-6"><a href="chapter-tree-model.html#cb200-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb200-7"><a href="chapter-tree-model.html#cb200-7" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb200-8"><a href="chapter-tree-model.html#cb200-8" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb200-9"><a href="chapter-tree-model.html#cb200-9" tabindex="-1"></a><span class="do">## Summary of sample sizes: 320, 320, 320, 321, 321, 320, ... </span></span>
<span id="cb200-10"><a href="chapter-tree-model.html#cb200-10" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb200-11"><a href="chapter-tree-model.html#cb200-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb200-12"><a href="chapter-tree-model.html#cb200-12" tabindex="-1"></a><span class="do">##   cp           RMSE      Rsquared   MAE     </span></span>
<span id="cb200-13"><a href="chapter-tree-model.html#cb200-13" tabindex="-1"></a><span class="do">##   0.007670005  4.357423  0.7517554  3.106724</span></span>
<span id="cb200-14"><a href="chapter-tree-model.html#cb200-14" tabindex="-1"></a><span class="do">##   0.008610415  4.397989  0.7485334  3.119257</span></span>
<span id="cb200-15"><a href="chapter-tree-model.html#cb200-15" tabindex="-1"></a><span class="do">##   0.011358068  4.557210  0.7303147  3.248633</span></span>
<span id="cb200-16"><a href="chapter-tree-model.html#cb200-16" tabindex="-1"></a><span class="do">##   0.011952330  4.559965  0.7298686  3.259467</span></span>
<span id="cb200-17"><a href="chapter-tree-model.html#cb200-17" tabindex="-1"></a><span class="do">##   0.021800452  4.646888  0.7229350  3.341879</span></span>
<span id="cb200-18"><a href="chapter-tree-model.html#cb200-18" tabindex="-1"></a><span class="do">##   0.027794282  4.731511  0.7157403  3.421950</span></span>
<span id="cb200-19"><a href="chapter-tree-model.html#cb200-19" tabindex="-1"></a><span class="do">##   0.034497434  4.879750  0.6953376  3.505237</span></span>
<span id="cb200-20"><a href="chapter-tree-model.html#cb200-20" tabindex="-1"></a><span class="do">##   0.080160736  5.497892  0.6318944  3.903550</span></span>
<span id="cb200-21"><a href="chapter-tree-model.html#cb200-21" tabindex="-1"></a><span class="do">##   0.165305341  6.862030  0.4455311  4.986384</span></span>
<span id="cb200-22"><a href="chapter-tree-model.html#cb200-22" tabindex="-1"></a><span class="do">##   0.462979695  8.276882  0.2938345  6.124176</span></span>
<span id="cb200-23"><a href="chapter-tree-model.html#cb200-23" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb200-24"><a href="chapter-tree-model.html#cb200-24" tabindex="-1"></a><span class="do">## RMSE was used to select the optimal model using the smallest value.</span></span>
<span id="cb200-25"><a href="chapter-tree-model.html#cb200-25" tabindex="-1"></a><span class="do">## The final value used for the model was cp = 0.007670005.</span></span></code></pre></div>
<p>10-fold CV로 계산된 예측 오차의 <code>RMSE</code>를 근거로 최적 모형이 선택되었다.
다른 측도인 <code>Rsquared</code> 또는 <code>MAE</code>를 근거로 모형을 선택하기 위해서는 요소 <code>metric</code>에 해당 문자를 지정해야 한다.</p>
<div class="sourceCode" id="cb201"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb201-1"><a href="chapter-tree-model.html#cb201-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb201-2"><a href="chapter-tree-model.html#cb201-2" tabindex="-1"></a><span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb201-3"><a href="chapter-tree-model.html#cb201-3" tabindex="-1"></a>      <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb201-4"><a href="chapter-tree-model.html#cb201-4" tabindex="-1"></a>      <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>),</span>
<span id="cb201-5"><a href="chapter-tree-model.html#cb201-5" tabindex="-1"></a>      <span class="at">metric =</span> <span class="st">&quot;Rsquared&quot;</span>)</span>
<span id="cb201-6"><a href="chapter-tree-model.html#cb201-6" tabindex="-1"></a><span class="do">## CART </span></span>
<span id="cb201-7"><a href="chapter-tree-model.html#cb201-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb201-8"><a href="chapter-tree-model.html#cb201-8" tabindex="-1"></a><span class="do">## 356 samples</span></span>
<span id="cb201-9"><a href="chapter-tree-model.html#cb201-9" tabindex="-1"></a><span class="do">##  13 predictor</span></span>
<span id="cb201-10"><a href="chapter-tree-model.html#cb201-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb201-11"><a href="chapter-tree-model.html#cb201-11" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb201-12"><a href="chapter-tree-model.html#cb201-12" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb201-13"><a href="chapter-tree-model.html#cb201-13" tabindex="-1"></a><span class="do">## Summary of sample sizes: 320, 320, 320, 321, 321, 320, ... </span></span>
<span id="cb201-14"><a href="chapter-tree-model.html#cb201-14" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb201-15"><a href="chapter-tree-model.html#cb201-15" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb201-16"><a href="chapter-tree-model.html#cb201-16" tabindex="-1"></a><span class="do">##   cp           RMSE      Rsquared   MAE     </span></span>
<span id="cb201-17"><a href="chapter-tree-model.html#cb201-17" tabindex="-1"></a><span class="do">##   0.007670005  4.357423  0.7517554  3.106724</span></span>
<span id="cb201-18"><a href="chapter-tree-model.html#cb201-18" tabindex="-1"></a><span class="do">##   0.008610415  4.397989  0.7485334  3.119257</span></span>
<span id="cb201-19"><a href="chapter-tree-model.html#cb201-19" tabindex="-1"></a><span class="do">##   0.011358068  4.557210  0.7303147  3.248633</span></span>
<span id="cb201-20"><a href="chapter-tree-model.html#cb201-20" tabindex="-1"></a><span class="do">##   0.011952330  4.559965  0.7298686  3.259467</span></span>
<span id="cb201-21"><a href="chapter-tree-model.html#cb201-21" tabindex="-1"></a><span class="do">##   0.021800452  4.646888  0.7229350  3.341879</span></span>
<span id="cb201-22"><a href="chapter-tree-model.html#cb201-22" tabindex="-1"></a><span class="do">##   0.027794282  4.731511  0.7157403  3.421950</span></span>
<span id="cb201-23"><a href="chapter-tree-model.html#cb201-23" tabindex="-1"></a><span class="do">##   0.034497434  4.879750  0.6953376  3.505237</span></span>
<span id="cb201-24"><a href="chapter-tree-model.html#cb201-24" tabindex="-1"></a><span class="do">##   0.080160736  5.497892  0.6318944  3.903550</span></span>
<span id="cb201-25"><a href="chapter-tree-model.html#cb201-25" tabindex="-1"></a><span class="do">##   0.165305341  6.862030  0.4455311  4.986384</span></span>
<span id="cb201-26"><a href="chapter-tree-model.html#cb201-26" tabindex="-1"></a><span class="do">##   0.462979695  8.276882  0.2938345  6.124176</span></span>
<span id="cb201-27"><a href="chapter-tree-model.html#cb201-27" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb201-28"><a href="chapter-tree-model.html#cb201-28" tabindex="-1"></a><span class="do">## Rsquared was used to select the optimal model using the largest value.</span></span>
<span id="cb201-29"><a href="chapter-tree-model.html#cb201-29" tabindex="-1"></a><span class="do">## The final value used for the model was cp = 0.007670005.</span></span></code></pre></div>
<p>적합된 tree 모형을 그래프로 표현해 보자.
패키지 <code>rpart.plot</code>의 함수 <code>rpart.plot()</code>을 사용하면 된다.</p>
<div class="sourceCode" id="cb202"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb202-1"><a href="chapter-tree-model.html#cb202-1" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb202-2"><a href="chapter-tree-model.html#cb202-2" tabindex="-1"></a><span class="fu">rpart.plot</span>(m1<span class="sc">$</span>finalModel, <span class="at">roundint=</span><span class="cn">FALSE</span>, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rtree-boston-1"></span>
<img src="_main_files/figure-html/rtree-boston-1-1.png" alt="`Boston` 자료의 tree 모형 적합 결과" width="576" />
<p class="caption">
그림 3.4: <code>Boston</code> 자료의 tree 모형 적합 결과
</p>
</div>
<p>함수 <code>rpart.plot()</code>의 요소 <code>roundint</code>는 분리 기준 숫자를 정수로 반올림해서 표시할 것인지를 정하는 것이고, <code>digits</code>는 유효숫자를 지정하는 것이다.</p>
<p>그림 <a href="chapter-tree-model.html#fig:rtree-boston-1">3.4</a>의 각 node에 숫자가 표시되어 있는데,
첫 번째 숫자는 각 node에 속한 자료의 변수 <code>medv</code>의 평균값이고,
두 번째 숫자는 각 node에 속한 자료의 비율이다.</p>
<p>최적 tree 모형을 선택하는 기준으로 ’최소예측오차’가 많이 사용되고 있는데,
’one-standard-error(1SE) rule’을 사용하는 것도 괜찮은 대안이 될 수 있다.
1SE rule은 (최소 예측 오차 + 1SE) 범위에 포함되는 tree 모형 중 크기가 가장 작은 모형을 최적 모형으로 선택하는 방법이다.
함수 <code>train()</code>에서는 <code>method = "rpart1SE"</code>를 지정하면 된다.
이 경우에는 적용되는 tuning parameter가 없기 때문에 <code>tuneLength</code>는 필요 없다.</p>
<div class="sourceCode" id="cb203"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb203-1"><a href="chapter-tree-model.html#cb203-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb203-2"><a href="chapter-tree-model.html#cb203-2" tabindex="-1"></a>m2 <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb203-3"><a href="chapter-tree-model.html#cb203-3" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;rpart1SE&quot;</span>,</span>
<span id="cb203-4"><a href="chapter-tree-model.html#cb203-4" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>))</span></code></pre></div>
<p>적합 결과를 그래프로 확인해 보자.</p>
<div class="sourceCode" id="cb204"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb204-1"><a href="chapter-tree-model.html#cb204-1" tabindex="-1"></a><span class="fu">rpart.plot</span>(m2<span class="sc">$</span>finalModel, <span class="at">roundint=</span><span class="cn">FALSE</span>, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:rtree-boston-2"></span>
<img src="_main_files/figure-html/rtree-boston-2-1.png" alt="`Boston` 자료의 1SE rule에 의한 tree 모형 적합 결과" width="576" />
<p class="caption">
그림 3.5: <code>Boston</code> 자료의 1SE rule에 의한 tree 모형 적합 결과
</p>
</div>
<p>그림 <a href="chapter-tree-model.html#fig:rtree-boston-1">3.4</a>에 표현된 최소 RMSE 모형보다 node의 개수가 하나 작은 것을 알 수 있다.
이제 적합된 두 tree 모형을 이용하여 test data에 대한 예측 실시해 보자.
예측은 함수 <code>predict()</code>로 할 수 있다.</p>
<div class="sourceCode" id="cb205"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb205-1"><a href="chapter-tree-model.html#cb205-1" tabindex="-1"></a>pred_1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1, <span class="at">newdata =</span> test_B)</span>
<span id="cb205-2"><a href="chapter-tree-model.html#cb205-2" tabindex="-1"></a>pred_2 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2, <span class="at">newdata =</span> test_B)</span></code></pre></div>
<p>예측 오차의 확인은 <code>caret</code>의 함수 <code>defaultSummary()</code>로 할 수 있다.
Test data의 반응변수와 예측 결과를 각각 <code>obs</code>와 <code>pred</code>라는 이름의 열로 구성한 데이터 프레임을 만들어서 입력하면 된다.</p>
<div class="sourceCode" id="cb206"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb206-1"><a href="chapter-tree-model.html#cb206-1" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred_1, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb206-2"><a href="chapter-tree-model.html#cb206-2" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb206-3"><a href="chapter-tree-model.html#cb206-3" tabindex="-1"></a><span class="do">## 5.4362182 0.6820792 3.2849165</span></span>
<span id="cb206-4"><a href="chapter-tree-model.html#cb206-4" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred_2, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb206-5"><a href="chapter-tree-model.html#cb206-5" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb206-6"><a href="chapter-tree-model.html#cb206-6" tabindex="-1"></a><span class="do">## 5.5684877 0.6671744 3.4993889</span></span></code></pre></div>
</div>
<div id="section-classification-tree" class="section level3 hasAnchor" number="3.1.2">
<h3><span class="header-section-number">3.1.2</span> Classification tree 모형<a href="chapter-tree-model.html#section-classification-tree" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Classification tree 모형은 반응변수가 범주형 변수인 경우에 적용되는 tree 모형으로써,
로지스틱 회귀모형처럼 분류가 주된 목적으로 사용되는 모형이다.
모형 설정 방법은 <a href="chapter-tree-model.html#section-reg-tree">3.1.1</a>절에서 살펴본 regression tree 모형의 경우와 동일하게 설명변수 공간에 대한 recursive binary splitting으로 같은 공간에 속한 자료의 동질성을 더 높이도록 분할이 이루어진다.</p>
<p>자료의 동질성 측도로 regression tree에서는 RSS를 사용했는데, 분류 목적에는 적합하지 않은 측도가 된다.
분류가 목적인 모형에서는 동일 범주에 속한 자료의 비율이 중요한 동질성 측도가 되는데,
classification tree에서는 Gini index와 entropy로 동질성을 측정한다.</p>
<p>두 개의 범주로 이루어진 이항 반응변수에 대한 Gini index와 entropy의 정의를 살펴보자.
<span class="math inline">\(p_{1}\)</span> 과 <span class="math inline">\(p_{2}\)</span> 를 각각 첫 번째 범주와 두 번째 범주에 속할 확률이라고 하면, Gini index는 다음과 같다.</p>
<p><span class="math display" id="eq:gini">\[\begin{equation}
p_{1}(1-p_{1}) + p_{2}(1-p_{2}) = 2p_{1}p_{2}
\tag{3.5}
\end{equation}\]</span></p>
<p>Entropy는 다음과 같다.</p>
<p><span class="math display" id="eq:entropy">\[\begin{equation}
-(p_{1} \log p_{1} + p_{2} \log p_{2})
\tag{3.6}
\end{equation}\]</span></p>
<p>Gini index와 entropy는 모두 <span class="math inline">\(p_{i}\)</span> 가 0 또는 1에 가까운 값을 가질수록 작은 값을 갖게 되는 측도인데,<br />
<span class="math inline">\(p_{i}\)</span> 가 0 또는 1에 가까운 값을 갖는다는 것은 곧 자료의 동질성이 높다는 것을 의미한다.
따라서 자료의 동질성이 높아질수록 작은 값을 갖게 되는 측도이다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> 예제: <code>Mroz</code> 자료</strong></p>
<p><a href="chapter-Logistic.html#section-logistic-selection">2.3</a>절에서 살펴본 <code>Mroz</code> 자료를 대상으로 classification tree 모형을 적합해 보자.
<code>Mroz</code> 자료는 미국 여성의 직업 참여에 대한 자료이며, 반응변수는 <code>lfp</code>이다.</p>
<div class="sourceCode" id="cb207"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb207-1"><a href="chapter-tree-model.html#cb207-1" tabindex="-1"></a><span class="fu">data</span>(Mroz, <span class="at">package =</span> <span class="st">&quot;carData&quot;</span>)</span>
<span id="cb207-2"><a href="chapter-tree-model.html#cb207-2" tabindex="-1"></a><span class="fu">str</span>(Mroz)</span>
<span id="cb207-3"><a href="chapter-tree-model.html#cb207-3" tabindex="-1"></a><span class="do">## &#39;data.frame&#39;:    753 obs. of  8 variables:</span></span>
<span id="cb207-4"><a href="chapter-tree-model.html#cb207-4" tabindex="-1"></a><span class="do">##  $ lfp : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ...</span></span>
<span id="cb207-5"><a href="chapter-tree-model.html#cb207-5" tabindex="-1"></a><span class="do">##  $ k5  : int  1 0 1 0 1 0 0 0 0 0 ...</span></span>
<span id="cb207-6"><a href="chapter-tree-model.html#cb207-6" tabindex="-1"></a><span class="do">##  $ k618: int  0 2 3 3 2 0 2 0 2 2 ...</span></span>
<span id="cb207-7"><a href="chapter-tree-model.html#cb207-7" tabindex="-1"></a><span class="do">##  $ age : int  32 30 35 34 31 54 37 54 48 39 ...</span></span>
<span id="cb207-8"><a href="chapter-tree-model.html#cb207-8" tabindex="-1"></a><span class="do">##  $ wc  : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 2 1 2 1 1 1 ...</span></span>
<span id="cb207-9"><a href="chapter-tree-model.html#cb207-9" tabindex="-1"></a><span class="do">##  $ hc  : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ...</span></span>
<span id="cb207-10"><a href="chapter-tree-model.html#cb207-10" tabindex="-1"></a><span class="do">##  $ lwg : num  1.2102 0.3285 1.5141 0.0921 1.5243 ...</span></span>
<span id="cb207-11"><a href="chapter-tree-model.html#cb207-11" tabindex="-1"></a><span class="do">##  $ inc : num  10.9 19.5 12 6.8 20.1 ...</span></span></code></pre></div>
<p>함수 <code>caret::createDataPartition()</code>으로 자료분리를 진행해 보자.</p>
<div class="sourceCode" id="cb208"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb208-1"><a href="chapter-tree-model.html#cb208-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb208-2"><a href="chapter-tree-model.html#cb208-2" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<div class="sourceCode" id="cb209"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb209-1"><a href="chapter-tree-model.html#cb209-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb209-2"><a href="chapter-tree-model.html#cb209-2" tabindex="-1"></a>x.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Mroz<span class="sc">$</span>lfp, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb209-3"><a href="chapter-tree-model.html#cb209-3" tabindex="-1"></a>train_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(x.id)</span>
<span id="cb209-4"><a href="chapter-tree-model.html#cb209-4" tabindex="-1"></a>test_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>x.id)</span></code></pre></div>
<p>함수 <code>caret::train()</code>으로 tree 모형을 적합시켜 보자.
요인이 반응변수로 지정되면 classification tree 모형이 적합된다.</p>
<div class="sourceCode" id="cb210"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb210-1"><a href="chapter-tree-model.html#cb210-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb210-2"><a href="chapter-tree-model.html#cb210-2" tabindex="-1"></a>m1 <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb210-3"><a href="chapter-tree-model.html#cb210-3" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb210-4"><a href="chapter-tree-model.html#cb210-4" tabindex="-1"></a>            <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>))</span></code></pre></div>
<p>적합 결과를 확인해 보자.</p>
<div class="sourceCode" id="cb211"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb211-1"><a href="chapter-tree-model.html#cb211-1" tabindex="-1"></a>m1</span>
<span id="cb211-2"><a href="chapter-tree-model.html#cb211-2" tabindex="-1"></a><span class="do">## CART </span></span>
<span id="cb211-3"><a href="chapter-tree-model.html#cb211-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb211-4"><a href="chapter-tree-model.html#cb211-4" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb211-5"><a href="chapter-tree-model.html#cb211-5" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb211-6"><a href="chapter-tree-model.html#cb211-6" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb211-7"><a href="chapter-tree-model.html#cb211-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb211-8"><a href="chapter-tree-model.html#cb211-8" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb211-9"><a href="chapter-tree-model.html#cb211-9" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb211-10"><a href="chapter-tree-model.html#cb211-10" tabindex="-1"></a><span class="do">## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... </span></span>
<span id="cb211-11"><a href="chapter-tree-model.html#cb211-11" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb211-12"><a href="chapter-tree-model.html#cb211-12" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb211-13"><a href="chapter-tree-model.html#cb211-13" tabindex="-1"></a><span class="do">##   cp           Accuracy   Kappa    </span></span>
<span id="cb211-14"><a href="chapter-tree-model.html#cb211-14" tabindex="-1"></a><span class="do">##   0.001282051  0.7261749  0.4461664</span></span>
<span id="cb211-15"><a href="chapter-tree-model.html#cb211-15" tabindex="-1"></a><span class="do">##   0.003846154  0.7345082  0.4641999</span></span>
<span id="cb211-16"><a href="chapter-tree-model.html#cb211-16" tabindex="-1"></a><span class="do">##   0.005769231  0.7312022  0.4571493</span></span>
<span id="cb211-17"><a href="chapter-tree-model.html#cb211-17" tabindex="-1"></a><span class="do">##   0.007692308  0.7362022  0.4661105</span></span>
<span id="cb211-18"><a href="chapter-tree-model.html#cb211-18" tabindex="-1"></a><span class="do">##   0.011538462  0.7395902  0.4742985</span></span>
<span id="cb211-19"><a href="chapter-tree-model.html#cb211-19" tabindex="-1"></a><span class="do">##   0.012820513  0.7495902  0.4974608</span></span>
<span id="cb211-20"><a href="chapter-tree-model.html#cb211-20" tabindex="-1"></a><span class="do">##   0.034615385  0.7047814  0.4029266</span></span>
<span id="cb211-21"><a href="chapter-tree-model.html#cb211-21" tabindex="-1"></a><span class="do">##   0.042307692  0.6997814  0.3951448</span></span>
<span id="cb211-22"><a href="chapter-tree-model.html#cb211-22" tabindex="-1"></a><span class="do">##   0.142307692  0.6485246  0.3273295</span></span>
<span id="cb211-23"><a href="chapter-tree-model.html#cb211-23" tabindex="-1"></a><span class="do">##   0.192307692  0.5804372  0.1005759</span></span>
<span id="cb211-24"><a href="chapter-tree-model.html#cb211-24" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb211-25"><a href="chapter-tree-model.html#cb211-25" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb211-26"><a href="chapter-tree-model.html#cb211-26" tabindex="-1"></a><span class="do">## The final value used for the model was cp = 0.01282051.</span></span></code></pre></div>
<p>정분류율인 <code>Accuracy</code>가 최적 모형선택의 디폴트 기준임을 알 수 있다.
다른 분류 평가 측도를 사용해서 최적 모형을 선택해 보자.
다른 평가 측도를 사용하기 위해서는 해당 측도의 값을 계산해야 하는데,
함수 <code>trainControl()</code>의 요소 <code>summaryFunction</code>에 키워드를 지정해야 해당 측도의 계산이 진행된다.</p>
<p><code>ROC</code>(area under ROC curve), <code>Sens</code>(Sensitivity), <code>Spec</code>(Specificity)의 계산을 위한 키워드는 <code>twoClassSummary</code>이고,
<code>AUC</code>(area under Precision-Recall curve), <code>Precision</code>, <code>Recall</code>, <code>F</code>(F1 score)의 계산을 위한 키워드는 <code>prSummary</code>이다.
또한 함수 <code>trainControl()</code>의 요소 <code>classProbs</code>에는 <code>TRUE</code>를 지정해야 ROC curve 및 Precision-Recall curve 작성을 위해 각 범주에 속할 확률을 계산한다.</p>
<p>평가 측도 <code>ROC</code>에 의한 최적 모형을 선택해 보자.</p>
<div class="sourceCode" id="cb212"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb212-1"><a href="chapter-tree-model.html#cb212-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb212-2"><a href="chapter-tree-model.html#cb212-2" tabindex="-1"></a><span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb212-3"><a href="chapter-tree-model.html#cb212-3" tabindex="-1"></a>      <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb212-4"><a href="chapter-tree-model.html#cb212-4" tabindex="-1"></a>      <span class="at">metric =</span> <span class="st">&quot;ROC&quot;</span>,</span>
<span id="cb212-5"><a href="chapter-tree-model.html#cb212-5" tabindex="-1"></a>      <span class="at">trControl=</span><span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb212-6"><a href="chapter-tree-model.html#cb212-6" tabindex="-1"></a>                             <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb212-7"><a href="chapter-tree-model.html#cb212-7" tabindex="-1"></a>                             <span class="at">summaryFunction =</span> twoClassSummary)</span>
<span id="cb212-8"><a href="chapter-tree-model.html#cb212-8" tabindex="-1"></a>      )</span>
<span id="cb212-9"><a href="chapter-tree-model.html#cb212-9" tabindex="-1"></a><span class="do">## CART </span></span>
<span id="cb212-10"><a href="chapter-tree-model.html#cb212-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb212-11"><a href="chapter-tree-model.html#cb212-11" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb212-12"><a href="chapter-tree-model.html#cb212-12" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb212-13"><a href="chapter-tree-model.html#cb212-13" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb212-14"><a href="chapter-tree-model.html#cb212-14" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb212-15"><a href="chapter-tree-model.html#cb212-15" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb212-16"><a href="chapter-tree-model.html#cb212-16" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb212-17"><a href="chapter-tree-model.html#cb212-17" tabindex="-1"></a><span class="do">## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... </span></span>
<span id="cb212-18"><a href="chapter-tree-model.html#cb212-18" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb212-19"><a href="chapter-tree-model.html#cb212-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb212-20"><a href="chapter-tree-model.html#cb212-20" tabindex="-1"></a><span class="do">##   cp           ROC        Sens       Spec     </span></span>
<span id="cb212-21"><a href="chapter-tree-model.html#cb212-21" tabindex="-1"></a><span class="do">##   0.001282051  0.8066290  0.7153846  0.7347899</span></span>
<span id="cb212-22"><a href="chapter-tree-model.html#cb212-22" tabindex="-1"></a><span class="do">##   0.003846154  0.8070249  0.7346154  0.7347899</span></span>
<span id="cb212-23"><a href="chapter-tree-model.html#cb212-23" tabindex="-1"></a><span class="do">##   0.005769231  0.7883904  0.7269231  0.7347899</span></span>
<span id="cb212-24"><a href="chapter-tree-model.html#cb212-24" tabindex="-1"></a><span class="do">##   0.007692308  0.7843132  0.7230769  0.7465546</span></span>
<span id="cb212-25"><a href="chapter-tree-model.html#cb212-25" tabindex="-1"></a><span class="do">##   0.011538462  0.7666451  0.7346154  0.7434454</span></span>
<span id="cb212-26"><a href="chapter-tree-model.html#cb212-26" tabindex="-1"></a><span class="do">##   0.012820513  0.7654040  0.7653846  0.7376471</span></span>
<span id="cb212-27"><a href="chapter-tree-model.html#cb212-27" tabindex="-1"></a><span class="do">##   0.034615385  0.7424580  0.6923077  0.7142017</span></span>
<span id="cb212-28"><a href="chapter-tree-model.html#cb212-28" tabindex="-1"></a><span class="do">##   0.042307692  0.7361797  0.7038462  0.6965546</span></span>
<span id="cb212-29"><a href="chapter-tree-model.html#cb212-29" tabindex="-1"></a><span class="do">##   0.142307692  0.6881464  0.8538462  0.4929412</span></span>
<span id="cb212-30"><a href="chapter-tree-model.html#cb212-30" tabindex="-1"></a><span class="do">##   0.192307692  0.5550679  0.3730769  0.7370588</span></span>
<span id="cb212-31"><a href="chapter-tree-model.html#cb212-31" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb212-32"><a href="chapter-tree-model.html#cb212-32" tabindex="-1"></a><span class="do">## ROC was used to select the optimal model using the largest value.</span></span>
<span id="cb212-33"><a href="chapter-tree-model.html#cb212-33" tabindex="-1"></a><span class="do">## The final value used for the model was cp = 0.003846154.</span></span></code></pre></div>
<p>평가 측도 <code>F</code>에 의한 최적 모형을 선택해 보자.</p>
<div class="sourceCode" id="cb213"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb213-1"><a href="chapter-tree-model.html#cb213-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1234</span>)</span>
<span id="cb213-2"><a href="chapter-tree-model.html#cb213-2" tabindex="-1"></a><span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb213-3"><a href="chapter-tree-model.html#cb213-3" tabindex="-1"></a>      <span class="at">method =</span> <span class="st">&quot;rpart&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb213-4"><a href="chapter-tree-model.html#cb213-4" tabindex="-1"></a>      <span class="at">metric =</span> <span class="st">&quot;F&quot;</span>,</span>
<span id="cb213-5"><a href="chapter-tree-model.html#cb213-5" tabindex="-1"></a>      <span class="at">trControl=</span><span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb213-6"><a href="chapter-tree-model.html#cb213-6" tabindex="-1"></a>                             <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb213-7"><a href="chapter-tree-model.html#cb213-7" tabindex="-1"></a>                             <span class="at">summaryFunction =</span> prSummary)</span>
<span id="cb213-8"><a href="chapter-tree-model.html#cb213-8" tabindex="-1"></a>      )</span>
<span id="cb213-9"><a href="chapter-tree-model.html#cb213-9" tabindex="-1"></a><span class="do">## CART </span></span>
<span id="cb213-10"><a href="chapter-tree-model.html#cb213-10" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-11"><a href="chapter-tree-model.html#cb213-11" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb213-12"><a href="chapter-tree-model.html#cb213-12" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb213-13"><a href="chapter-tree-model.html#cb213-13" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb213-14"><a href="chapter-tree-model.html#cb213-14" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-15"><a href="chapter-tree-model.html#cb213-15" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb213-16"><a href="chapter-tree-model.html#cb213-16" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb213-17"><a href="chapter-tree-model.html#cb213-17" tabindex="-1"></a><span class="do">## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... </span></span>
<span id="cb213-18"><a href="chapter-tree-model.html#cb213-18" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb213-19"><a href="chapter-tree-model.html#cb213-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-20"><a href="chapter-tree-model.html#cb213-20" tabindex="-1"></a><span class="do">##   cp           AUC         Precision  Recall     F        </span></span>
<span id="cb213-21"><a href="chapter-tree-model.html#cb213-21" tabindex="-1"></a><span class="do">##   0.001282051  0.62943815  0.6756925  0.7153846  0.6910425</span></span>
<span id="cb213-22"><a href="chapter-tree-model.html#cb213-22" tabindex="-1"></a><span class="do">##   0.003846154  0.60254829  0.6809178  0.7346154  0.7032688</span></span>
<span id="cb213-23"><a href="chapter-tree-model.html#cb213-23" tabindex="-1"></a><span class="do">##   0.005769231  0.60796211  0.6803280  0.7269231  0.6983203</span></span>
<span id="cb213-24"><a href="chapter-tree-model.html#cb213-24" tabindex="-1"></a><span class="do">##   0.007692308  0.60584754  0.6916301  0.7230769  0.7008195</span></span>
<span id="cb213-25"><a href="chapter-tree-model.html#cb213-25" tabindex="-1"></a><span class="do">##   0.011538462  0.56073958  0.6888776  0.7346154  0.7090688</span></span>
<span id="cb213-26"><a href="chapter-tree-model.html#cb213-26" tabindex="-1"></a><span class="do">##   0.012820513  0.51928620  0.6956948  0.7653846  0.7264468</span></span>
<span id="cb213-27"><a href="chapter-tree-model.html#cb213-27" tabindex="-1"></a><span class="do">##   0.034615385  0.41024975  0.6486295  0.6923077  0.6680326</span></span>
<span id="cb213-28"><a href="chapter-tree-model.html#cb213-28" tabindex="-1"></a><span class="do">##   0.042307692  0.35974154  0.6387577  0.7038462  0.6673466</span></span>
<span id="cb213-29"><a href="chapter-tree-model.html#cb213-29" tabindex="-1"></a><span class="do">##   0.142307692  0.07811750  0.5653613  0.8538462  0.6770412</span></span>
<span id="cb213-30"><a href="chapter-tree-model.html#cb213-30" tabindex="-1"></a><span class="do">##   0.192307692  0.01272258  0.5189849  0.3730769  0.6664240</span></span>
<span id="cb213-31"><a href="chapter-tree-model.html#cb213-31" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb213-32"><a href="chapter-tree-model.html#cb213-32" tabindex="-1"></a><span class="do">## F was used to select the optimal model using the largest value.</span></span>
<span id="cb213-33"><a href="chapter-tree-model.html#cb213-33" tabindex="-1"></a><span class="do">## The final value used for the model was cp = 0.01282051.</span></span></code></pre></div>
<p>모형 <code>m1</code>의 적합 결과를 그래프로 표현해 보자.</p>
<div class="sourceCode" id="cb214"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb214-1"><a href="chapter-tree-model.html#cb214-1" tabindex="-1"></a><span class="fu">library</span>(rpart.plot)</span>
<span id="cb214-2"><a href="chapter-tree-model.html#cb214-2" tabindex="-1"></a><span class="fu">rpart.plot</span>(m1<span class="sc">$</span>finalModel, <span class="at">roundint =</span> <span class="cn">FALSE</span>, <span class="at">digits =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:ctree-mroz-1"></span>
<img src="_main_files/figure-html/ctree-mroz-1-1.png" alt="`Mroz` 자료의 tree 모형 적합 결과" width="576" />
<p class="caption">
그림 3.6: <code>Mroz</code> 자료의 tree 모형 적합 결과
</p>
</div>
<p>그림 <a href="chapter-tree-model.html#fig:ctree-mroz-1">3.6</a>의 각 node에 3가지 결과가 표시되어 있는데,
첫 번째는 해당 node의 다수 범주를 표시하고 있고,
두 번째 숫자는 두 번째 범주인 “yes” 범주에 속한 자료의 비율이 표시되어 있다.
즉, root node인 전체 자료의 다수 범주는 “yes”이고, 그 비율이 0.569라는 것이다.
세 번째 백분율은 각 node에 속한 자료의 비율을 나타내고 있다.</p>
<p>모형 <code>m1</code>으로 test data에 대한 예측을 함수 <code>predict()</code>로 실시해 보자.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb215-1"><a href="chapter-tree-model.html#cb215-1" tabindex="-1"></a>pred_1 <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1, <span class="at">newdata =</span> test_M) </span>
<span id="cb215-2"><a href="chapter-tree-model.html#cb215-2" tabindex="-1"></a>pred_1[<span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>]</span>
<span id="cb215-3"><a href="chapter-tree-model.html#cb215-3" tabindex="-1"></a><span class="do">##  [1] yes no  yes yes yes no  yes yes no  yes</span></span>
<span id="cb215-4"><a href="chapter-tree-model.html#cb215-4" tabindex="-1"></a><span class="do">## Levels: no yes</span></span></code></pre></div>
<p>함수 <code>train()</code>으로 생성된 객체에 함수 <code>predict()</code>로 예측을 실시하면 <code>type = "raw"</code>가 디폴트로 적용되는데,
연속형 반응변수인 regression tree 모형의 경우에는 mean 값이 출력되고,
이항 반응변수인 classification tree 모형의 경우에는 class가 출력된다.
만일 각 그룹에 속할 확률을 출력하고자 하면, <code>type = "prob"</code>를 지정해야 한다.</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb216-1"><a href="chapter-tree-model.html#cb216-1" tabindex="-1"></a><span class="fu">predict</span>(m1, <span class="at">newdata =</span> test_M[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,], <span class="at">type =</span> <span class="st">&quot;prob&quot;</span>)</span>
<span id="cb216-2"><a href="chapter-tree-model.html#cb216-2" tabindex="-1"></a><span class="do">##           no       yes</span></span>
<span id="cb216-3"><a href="chapter-tree-model.html#cb216-3" tabindex="-1"></a><span class="do">## 3  0.1111111 0.8888889</span></span>
<span id="cb216-4"><a href="chapter-tree-model.html#cb216-4" tabindex="-1"></a><span class="do">## 13 0.7268519 0.2731481</span></span>
<span id="cb216-5"><a href="chapter-tree-model.html#cb216-5" tabindex="-1"></a><span class="do">## 18 0.1111111 0.8888889</span></span>
<span id="cb216-6"><a href="chapter-tree-model.html#cb216-6" tabindex="-1"></a><span class="do">## 22 0.2280702 0.7719298</span></span>
<span id="cb216-7"><a href="chapter-tree-model.html#cb216-7" tabindex="-1"></a><span class="do">## 27 0.1111111 0.8888889</span></span></code></pre></div>
<p>Test data에 대한 모형 <code>m1</code>의 분류 성능을 함수 <code>caret::confusionMatrix()</code>로 평가해 보자.</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb217-1"><a href="chapter-tree-model.html#cb217-1" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred_1, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp, </span>
<span id="cb217-2"><a href="chapter-tree-model.html#cb217-2" tabindex="-1"></a>                  <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb217-3"><a href="chapter-tree-model.html#cb217-3" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb217-4"><a href="chapter-tree-model.html#cb217-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb217-5"><a href="chapter-tree-model.html#cb217-5" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb217-6"><a href="chapter-tree-model.html#cb217-6" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb217-7"><a href="chapter-tree-model.html#cb217-7" tabindex="-1"></a><span class="do">##        no  45  25</span></span>
<span id="cb217-8"><a href="chapter-tree-model.html#cb217-8" tabindex="-1"></a><span class="do">##        yes 20  60</span></span>
<span id="cb217-9"><a href="chapter-tree-model.html#cb217-9" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb217-10"><a href="chapter-tree-model.html#cb217-10" tabindex="-1"></a><span class="do">##                Accuracy : 0.7            </span></span>
<span id="cb217-11"><a href="chapter-tree-model.html#cb217-11" tabindex="-1"></a><span class="do">##                  95% CI : (0.6199, 0.772)</span></span>
<span id="cb217-12"><a href="chapter-tree-model.html#cb217-12" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667         </span></span>
<span id="cb217-13"><a href="chapter-tree-model.html#cb217-13" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 0.0005439      </span></span>
<span id="cb217-14"><a href="chapter-tree-model.html#cb217-14" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb217-15"><a href="chapter-tree-model.html#cb217-15" tabindex="-1"></a><span class="do">##                   Kappa : 0.3946         </span></span>
<span id="cb217-16"><a href="chapter-tree-model.html#cb217-16" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb217-17"><a href="chapter-tree-model.html#cb217-17" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.5509850      </span></span>
<span id="cb217-18"><a href="chapter-tree-model.html#cb217-18" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb217-19"><a href="chapter-tree-model.html#cb217-19" tabindex="-1"></a><span class="do">##             Sensitivity : 0.7059         </span></span>
<span id="cb217-20"><a href="chapter-tree-model.html#cb217-20" tabindex="-1"></a><span class="do">##             Specificity : 0.6923         </span></span>
<span id="cb217-21"><a href="chapter-tree-model.html#cb217-21" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7500         </span></span>
<span id="cb217-22"><a href="chapter-tree-model.html#cb217-22" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6429         </span></span>
<span id="cb217-23"><a href="chapter-tree-model.html#cb217-23" tabindex="-1"></a><span class="do">##               Precision : 0.7500         </span></span>
<span id="cb217-24"><a href="chapter-tree-model.html#cb217-24" tabindex="-1"></a><span class="do">##                  Recall : 0.7059         </span></span>
<span id="cb217-25"><a href="chapter-tree-model.html#cb217-25" tabindex="-1"></a><span class="do">##                      F1 : 0.7273         </span></span>
<span id="cb217-26"><a href="chapter-tree-model.html#cb217-26" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667         </span></span>
<span id="cb217-27"><a href="chapter-tree-model.html#cb217-27" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4000         </span></span>
<span id="cb217-28"><a href="chapter-tree-model.html#cb217-28" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5333         </span></span>
<span id="cb217-29"><a href="chapter-tree-model.html#cb217-29" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.6991         </span></span>
<span id="cb217-30"><a href="chapter-tree-model.html#cb217-30" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb217-31"><a href="chapter-tree-model.html#cb217-31" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes            </span></span>
<span id="cb217-32"><a href="chapter-tree-model.html#cb217-32" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
</div>
</div>
<div id="section-bagging" class="section level2 hasAnchor" number="3.2">
<h2><span class="header-section-number">3.2</span> Bagging<a href="chapter-tree-model.html#section-bagging" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="chapter-tree-model.html#section-decision-tree">3.1</a>절에서 살펴본 decision tree 모형의 가장 큰 문제는 분산이 매우 크다는 점이다.
분산이 큰 모형은 자료가 조금만 달라져도 적합 결과에 큰 변동이 발생하게 되는데,
예측 모형에게는 심각한 결합이 되는 문제라 할 수 있다.</p>
<p>분산을 낮추는 방법으로 여러 개의 독립된 training data를 생성해서 각각의 training data에 대한 tree 모형을 적합하고, 추정된 각각의 모형에서 생성된 예측값들의 평균을 최종 예측 결과로 사용하는 것을 생각해 볼 수 있다.
이 방법은 분산이 <span class="math inline">\(\sigma^{2}\)</span> 인 독립된 <span class="math inline">\(n\)</span> 개의 관찰값 <span class="math inline">\(X_{1}, X_{2}, \ldots, X_{n}\)</span> 의 평균 <span class="math inline">\(\overline{X}\)</span> 는 분산이 <span class="math inline">\(\sigma^{2}/n\)</span> 이 되어서 개별 관찰값보다 더 작은 분산을 가질 수 있다는 사실에 근거로 두고 있다.</p>
<p>문제는 여러 개의 독립된 training data를 모집단에서 다시 생성하는 것이 사실상 불가능하다는 것이다.
따라서 기존의 자료에서 여러 개의 표본을 다시 생성하는 방법을 고려해야 하는데,
Bootstrap이 좋은 대안이 될 수 있다.</p>
<p>Bootstrap은 추정량이나 예측모형 등의 불확실성을 탐색하기 위한 매우 유용한 도구로 사용되는 resampling 기법이다.
어떤 추정량의 추정 결과에 대한 정확도 등을 평가하기 위해서는 해당 추정량의 표본분포가 반드시 필요하다.
추정량의 표본분포란 모집단에서 반복적으로 추출한 임의표본으로 계산한 추정량 값의 분포를 의미한다.
일반적으로는 이론적으로 추정량의 표본분포를 유도하지만, 이론적으로 유도하기 어려운 형태의 추정량도 많이 있다.
이런 경우에 대안으로 사용할 수 있는 방법이 Bootstrap인데,
기본 개념은 모집단에서 독립된 임의표본을 반복적으로 추출하는 것 대신에 원자료, 즉 원래의 표본자료에서 독립된 임의표본을 반복적으로 추출하는 것이다.
즉, 크기가 <span class="math inline">\(n\)</span> 인 원자료에서 복원추출로 크기가 <span class="math inline">\(n\)</span> 인 임의표본을 추출하는 과정을 <span class="math inline">\(B\)</span> 번 반복해서, <span class="math inline">\(B\)</span> 세트의 독립된 표본을 구성하는 것이다.
복원추출을 사용한 이유는 독립된 임의표본을 추출하기 위함이다.</p>
<p><em>Bootstrap aggregating</em> 또는 Bagging은 training data에서 <span class="math inline">\(B\)</span> 개의 bootstrap sample을 추출하는 것으로 시작한다.
<span class="math inline">\(B\)</span> 개의 bootstrap sample은 독립된 training data로 간주할 수 있으며, 각 bootstrap sample에 대한 full size tree 모형을 적합하고, 예측 결과를 통합함으로써 모형의 분산을 크게 낮출수 있는 것이다.
<span class="math inline">\(B\)</span> 개의 tree 모형의 예측 결과를 통합해서 최종 예측 결과를 산출하는 방식으로 regression tree에서는 예측값의 평균을 사용하며, classification tree에서는 다수로 분류된 범주를 사용한다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> <code>caret</code>에 의한 Bagging</strong></p>
<p><code>caret</code>에서 함수 <code>train()</code>으로 bagging을 실행하기 위해서 필요한 패키지는 <code>ipred</code>와 <code>e1071</code>이다.</p>
<ul>
<li>예제 : <code>MASS::Boston</code></li>
</ul>
<p>Regression tree 모형에 대한 bagging 예제로써 <a href="chapter-tree-model.html#section-reg-tree">3.1.1</a>절에서 살펴본 <code>MASS::Boston</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb218-1"><a href="chapter-tree-model.html#cb218-1" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb218-2"><a href="chapter-tree-model.html#cb218-2" tabindex="-1"></a><span class="fu">library</span>(caret)</span></code></pre></div>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb219-1"><a href="chapter-tree-model.html#cb219-1" tabindex="-1"></a><span class="fu">data</span>(Boston, <span class="at">package=</span><span class="st">&quot;MASS&quot;</span>)</span></code></pre></div>
<p>자료분리를 실시하자.</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb220-1"><a href="chapter-tree-model.html#cb220-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb220-2"><a href="chapter-tree-model.html#cb220-2" tabindex="-1"></a>train.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Boston<span class="sc">$</span>medv, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb220-3"><a href="chapter-tree-model.html#cb220-3" tabindex="-1"></a>train_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(train.id)</span>
<span id="cb220-4"><a href="chapter-tree-model.html#cb220-4" tabindex="-1"></a>test_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>train.id)</span></code></pre></div>
<p>함수 <code>train()</code>으로 bagged tree 모형을 적합하기 위해서는 <code>method = "treebag"</code>을 지정해야 한다.</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb221-1"><a href="chapter-tree-model.html#cb221-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb222-1"><a href="chapter-tree-model.html#cb222-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb222-2"><a href="chapter-tree-model.html#cb222-2" tabindex="-1"></a>m1_bag <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb222-3"><a href="chapter-tree-model.html#cb222-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;treebag&quot;</span>, </span>
<span id="cb222-4"><a href="chapter-tree-model.html#cb222-4" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl)</span></code></pre></div>
<p>모형 <code>m1_bag</code>에 입력된 적합 결과를 확인해 보자.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb223-1"><a href="chapter-tree-model.html#cb223-1" tabindex="-1"></a>m1_bag</span>
<span id="cb223-2"><a href="chapter-tree-model.html#cb223-2" tabindex="-1"></a><span class="do">## Bagged CART </span></span>
<span id="cb223-3"><a href="chapter-tree-model.html#cb223-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb223-4"><a href="chapter-tree-model.html#cb223-4" tabindex="-1"></a><span class="do">## 356 samples</span></span>
<span id="cb223-5"><a href="chapter-tree-model.html#cb223-5" tabindex="-1"></a><span class="do">##  13 predictor</span></span>
<span id="cb223-6"><a href="chapter-tree-model.html#cb223-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb223-7"><a href="chapter-tree-model.html#cb223-7" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb223-8"><a href="chapter-tree-model.html#cb223-8" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb223-9"><a href="chapter-tree-model.html#cb223-9" tabindex="-1"></a><span class="do">## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... </span></span>
<span id="cb223-10"><a href="chapter-tree-model.html#cb223-10" tabindex="-1"></a><span class="do">## Resampling results:</span></span>
<span id="cb223-11"><a href="chapter-tree-model.html#cb223-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb223-12"><a href="chapter-tree-model.html#cb223-12" tabindex="-1"></a><span class="do">##   RMSE      Rsquared   MAE     </span></span>
<span id="cb223-13"><a href="chapter-tree-model.html#cb223-13" tabindex="-1"></a><span class="do">##   3.943609  0.8147499  2.780642</span></span></code></pre></div>
<p>Bagged tree 모형에서 tuning parameter는 bootstrap sample 추출 반복 횟수 <span class="math inline">\(B\)</span> 이다.
<code>nbagg</code>에서 그 횟수를 지정할 수 있으며, 디폴트 횟수는 <code>nbagg = 25</code>이다.
<code>nbagg = 30</code>과 <code>nbagg = 50</code>에서 적합해서, 그 결과를 확인해 보자.
<code>RMSE</code>, <code>Rsquared</code>와 <code>MAE</code>에서 큰 차이가 없음을 알 수 있다.</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb224-1"><a href="chapter-tree-model.html#cb224-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb224-2"><a href="chapter-tree-model.html#cb224-2" tabindex="-1"></a><span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb224-3"><a href="chapter-tree-model.html#cb224-3" tabindex="-1"></a>      <span class="at">method =</span> <span class="st">&quot;treebag&quot;</span>,</span>
<span id="cb224-4"><a href="chapter-tree-model.html#cb224-4" tabindex="-1"></a>      <span class="at">nbagg =</span> <span class="dv">30</span>, <span class="at">trControl =</span> ctrl)<span class="sc">$</span>results</span>
<span id="cb224-5"><a href="chapter-tree-model.html#cb224-5" tabindex="-1"></a><span class="do">##   parameter     RMSE  Rsquared      MAE   RMSESD RsquaredSD     MAESD</span></span>
<span id="cb224-6"><a href="chapter-tree-model.html#cb224-6" tabindex="-1"></a><span class="do">## 1      none 3.955934 0.8135511 2.793155 1.389929  0.1221695 0.7202972</span></span></code></pre></div>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb225-1"><a href="chapter-tree-model.html#cb225-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb225-2"><a href="chapter-tree-model.html#cb225-2" tabindex="-1"></a><span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb225-3"><a href="chapter-tree-model.html#cb225-3" tabindex="-1"></a>      <span class="at">method =</span> <span class="st">&quot;treebag&quot;</span>,</span>
<span id="cb225-4"><a href="chapter-tree-model.html#cb225-4" tabindex="-1"></a>      <span class="at">nbagg =</span> <span class="dv">50</span>, <span class="at">trControl =</span> ctrl)<span class="sc">$</span>results</span>
<span id="cb225-5"><a href="chapter-tree-model.html#cb225-5" tabindex="-1"></a><span class="do">##   parameter     RMSE  Rsquared      MAE   RMSESD RsquaredSD     MAESD</span></span>
<span id="cb225-6"><a href="chapter-tree-model.html#cb225-6" tabindex="-1"></a><span class="do">## 1      none 3.880447 0.8206622 2.741153 1.403033  0.1215254 0.7128191</span></span></code></pre></div>
<p>단일 tree 모형에 비해 bagged tree 모형은 예측 정확도에서 큰 폭의 개선이 이루어졌지만, 예측 결과에 대한 해석이 쉽다는 tree 모형의 장점은 완전히 사라졌고, 사실상 해석이 불가능한 모형이 되었다.
즉, 개별 변수가 최종 예측에 어떤 영향을 미쳤는지 알 수 없다는 것이다.
비록 최종 예측 결과에 대한 정확한 해석은 힘들어졌지만, 최종 모형에서 각 변수가 차지하는 중요도를 측정할 수는 있다.
개별 변수의 중요도는 각 변수가 tree 분할에 사용되며 감소시킨 RSS의 값으로 측정할 수 있는데,
bagged tree 모형에서는 최종 예측에 다수의 tree 모형이 사용되고 있으므로,
각 tree 모형에서 개별 변수가 tree 분할에 사용되며 감소시킨 RSS의 평균값으로 중요도를 측정할 수 있다.
평균 감소폭이 가장 큰 변수가 가장 중요한 변수라고 하겠다.</p>
<p>변수의 중요도를 측정할 수 있는 함수는 <code>caret::varImp()</code>이다.
모형 <code>m1_bag</code>에 대한 변수 중요도를 측정해서 그래프로 표현해 보자.</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb226-1"><a href="chapter-tree-model.html#cb226-1" tabindex="-1"></a><span class="fu">varImp</span>(m1_bag) <span class="sc">|&gt;</span> </span>
<span id="cb226-2"><a href="chapter-tree-model.html#cb226-2" tabindex="-1"></a>  <span class="fu">ggplot</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-bag-varimp"></span>
<img src="_main_files/figure-html/m1-bag-varimp-1.png" alt="`Boston` 자료에 대한 bagged tree 모형의 변수 중요도" width="576" />
<p class="caption">
그림 3.7: <code>Boston</code> 자료에 대한 bagged tree 모형의 변수 중요도
</p>
</div>
<p>변수 <code>lstat</code>이 가장 중요한 변수로 측정되었다.
그림 <a href="chapter-tree-model.html#fig:m1-bag-varimp">3.7</a>의 X축에 표시된 각 변수의 중요도 값은 가장 중요한 변수의 평균 RSS 감소폭에 대한 비율을 표시하고 있다.</p>
<p>모형 <code>m1_bag</code>을 사용하여 test data에 대한 예측을 실시하고 결과를 평가해 보자.</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb227-1"><a href="chapter-tree-model.html#cb227-1" tabindex="-1"></a>pred_bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1_bag, test_B)</span>
<span id="cb227-2"><a href="chapter-tree-model.html#cb227-2" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred_bag, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb227-3"><a href="chapter-tree-model.html#cb227-3" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb227-4"><a href="chapter-tree-model.html#cb227-4" tabindex="-1"></a><span class="do">## 4.8180947 0.7548929 2.9468445</span></span></code></pre></div>
<p>예측 결과를 그래프로 표현해 보자.</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb228-1"><a href="chapter-tree-model.html#cb228-1" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">pred =</span> pred_bag, <span class="at">obs =</span> test_B<span class="sc">$</span>medv) <span class="sc">|&gt;</span> </span>
<span id="cb228-2"><a href="chapter-tree-model.html#cb228-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> obs, <span class="at">y =</span> pred)) <span class="sc">+</span></span>
<span id="cb228-3"><a href="chapter-tree-model.html#cb228-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb228-4"><a href="chapter-tree-model.html#cb228-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb228-5"><a href="chapter-tree-model.html#cb228-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Observed data&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Predicted data&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-bag-predict"></span>
<img src="_main_files/figure-html/m1-bag-predict-1.png" alt="`Boston` 자료에 대한 bagged tree 모형의 예측 결과" width="576" />
<p class="caption">
그림 3.8: <code>Boston</code> 자료에 대한 bagged tree 모형의 예측 결과
</p>
</div>
<ul>
<li>예제 : <code>carData::Mroz</code></li>
</ul>
<p>Classification tree 모형에 대한 bagging 예제로써 <a href="chapter-tree-model.html#section-classification-tree">3.1.2</a>절에서 살펴본 <code>carData::Mroz</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb229-1"><a href="chapter-tree-model.html#cb229-1" tabindex="-1"></a><span class="fu">data</span>(Mroz, <span class="at">package=</span><span class="st">&quot;carData&quot;</span>)</span></code></pre></div>
<p>자료분리를 진행하자.</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb230-1"><a href="chapter-tree-model.html#cb230-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb230-2"><a href="chapter-tree-model.html#cb230-2" tabindex="-1"></a>x.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Mroz<span class="sc">$</span>lfp, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb230-3"><a href="chapter-tree-model.html#cb230-3" tabindex="-1"></a>train_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(x.id)</span>
<span id="cb230-4"><a href="chapter-tree-model.html#cb230-4" tabindex="-1"></a>test_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>x.id)</span></code></pre></div>
<p>Bagged tree 모형을 적합해 보자.</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb231-1"><a href="chapter-tree-model.html#cb231-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb232-1"><a href="chapter-tree-model.html#cb232-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb232-2"><a href="chapter-tree-model.html#cb232-2" tabindex="-1"></a>m2_bag <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb232-3"><a href="chapter-tree-model.html#cb232-3" tabindex="-1"></a>            <span class="at">method =</span> <span class="st">&quot;treebag&quot;</span>,</span>
<span id="cb232-4"><a href="chapter-tree-model.html#cb232-4" tabindex="-1"></a>            <span class="at">trControl =</span> ctrl)</span></code></pre></div>
<p>적합 결과를 확인해 보자.</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb233-1"><a href="chapter-tree-model.html#cb233-1" tabindex="-1"></a>m2_bag</span>
<span id="cb233-2"><a href="chapter-tree-model.html#cb233-2" tabindex="-1"></a><span class="do">## Bagged CART </span></span>
<span id="cb233-3"><a href="chapter-tree-model.html#cb233-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb233-4"><a href="chapter-tree-model.html#cb233-4" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb233-5"><a href="chapter-tree-model.html#cb233-5" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb233-6"><a href="chapter-tree-model.html#cb233-6" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb233-7"><a href="chapter-tree-model.html#cb233-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb233-8"><a href="chapter-tree-model.html#cb233-8" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb233-9"><a href="chapter-tree-model.html#cb233-9" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb233-10"><a href="chapter-tree-model.html#cb233-10" tabindex="-1"></a><span class="do">## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... </span></span>
<span id="cb233-11"><a href="chapter-tree-model.html#cb233-11" tabindex="-1"></a><span class="do">## Resampling results:</span></span>
<span id="cb233-12"><a href="chapter-tree-model.html#cb233-12" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb233-13"><a href="chapter-tree-model.html#cb233-13" tabindex="-1"></a><span class="do">##   Accuracy   Kappa    </span></span>
<span id="cb233-14"><a href="chapter-tree-model.html#cb233-14" tabindex="-1"></a><span class="do">##   0.7596995  0.5101126</span></span></code></pre></div>
<p>모형 <code>m2_bag</code>의 변수 중요도를 그래프로 나타내자.
Classification 문제에 대한 bagged tree 모형에서 변수의 중요도는 Gini index의 평균 감소폭으로 측정한다.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb234-1"><a href="chapter-tree-model.html#cb234-1" tabindex="-1"></a><span class="fu">varImp</span>(m2_bag) <span class="sc">|&gt;</span> </span>
<span id="cb234-2"><a href="chapter-tree-model.html#cb234-2" tabindex="-1"></a>  <span class="fu">ggplot</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m2-bag-varimp"></span>
<img src="_main_files/figure-html/m2-bag-varimp-1.png" alt="`Mroz` 자료에 대한 bagged tree 모형의 변수 중요도" width="576" />
<p class="caption">
그림 3.9: <code>Mroz</code> 자료에 대한 bagged tree 모형의 변수 중요도
</p>
</div>
<p>모형 <code>m2_bag</code>을 사용해서 test data에 대한 예측 및 분류를 실시하고 결과를 평가해 보자.</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb235-1"><a href="chapter-tree-model.html#cb235-1" tabindex="-1"></a>pred2_bag <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2_bag, <span class="at">newdata =</span> test_M) </span>
<span id="cb235-2"><a href="chapter-tree-model.html#cb235-2" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred2_bag, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp, </span>
<span id="cb235-3"><a href="chapter-tree-model.html#cb235-3" tabindex="-1"></a>                <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb235-4"><a href="chapter-tree-model.html#cb235-4" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb235-5"><a href="chapter-tree-model.html#cb235-5" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb235-6"><a href="chapter-tree-model.html#cb235-6" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb235-7"><a href="chapter-tree-model.html#cb235-7" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb235-8"><a href="chapter-tree-model.html#cb235-8" tabindex="-1"></a><span class="do">##        no  47  26</span></span>
<span id="cb235-9"><a href="chapter-tree-model.html#cb235-9" tabindex="-1"></a><span class="do">##        yes 18  59</span></span>
<span id="cb235-10"><a href="chapter-tree-model.html#cb235-10" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb235-11"><a href="chapter-tree-model.html#cb235-11" tabindex="-1"></a><span class="do">##                Accuracy : 0.7067          </span></span>
<span id="cb235-12"><a href="chapter-tree-model.html#cb235-12" tabindex="-1"></a><span class="do">##                  95% CI : (0.6269, 0.7781)</span></span>
<span id="cb235-13"><a href="chapter-tree-model.html#cb235-13" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667          </span></span>
<span id="cb235-14"><a href="chapter-tree-model.html#cb235-14" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 0.0002919       </span></span>
<span id="cb235-15"><a href="chapter-tree-model.html#cb235-15" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb235-16"><a href="chapter-tree-model.html#cb235-16" tabindex="-1"></a><span class="do">##                   Kappa : 0.4112          </span></span>
<span id="cb235-17"><a href="chapter-tree-model.html#cb235-17" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb235-18"><a href="chapter-tree-model.html#cb235-18" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.2912928       </span></span>
<span id="cb235-19"><a href="chapter-tree-model.html#cb235-19" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb235-20"><a href="chapter-tree-model.html#cb235-20" tabindex="-1"></a><span class="do">##             Sensitivity : 0.6941          </span></span>
<span id="cb235-21"><a href="chapter-tree-model.html#cb235-21" tabindex="-1"></a><span class="do">##             Specificity : 0.7231          </span></span>
<span id="cb235-22"><a href="chapter-tree-model.html#cb235-22" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7662          </span></span>
<span id="cb235-23"><a href="chapter-tree-model.html#cb235-23" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6438          </span></span>
<span id="cb235-24"><a href="chapter-tree-model.html#cb235-24" tabindex="-1"></a><span class="do">##               Precision : 0.7662          </span></span>
<span id="cb235-25"><a href="chapter-tree-model.html#cb235-25" tabindex="-1"></a><span class="do">##                  Recall : 0.6941          </span></span>
<span id="cb235-26"><a href="chapter-tree-model.html#cb235-26" tabindex="-1"></a><span class="do">##                      F1 : 0.7284          </span></span>
<span id="cb235-27"><a href="chapter-tree-model.html#cb235-27" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667          </span></span>
<span id="cb235-28"><a href="chapter-tree-model.html#cb235-28" tabindex="-1"></a><span class="do">##          Detection Rate : 0.3933          </span></span>
<span id="cb235-29"><a href="chapter-tree-model.html#cb235-29" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5133          </span></span>
<span id="cb235-30"><a href="chapter-tree-model.html#cb235-30" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.7086          </span></span>
<span id="cb235-31"><a href="chapter-tree-model.html#cb235-31" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb235-32"><a href="chapter-tree-model.html#cb235-32" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes             </span></span>
<span id="cb235-33"><a href="chapter-tree-model.html#cb235-33" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
</div>
<div id="section-rf" class="section level2 hasAnchor" number="3.3">
<h2><span class="header-section-number">3.3</span> Random Forest<a href="chapter-tree-model.html#section-rf" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><a href="chapter-tree-model.html#section-bagging">3.2</a>절에서 살펴본 bagged tree 모형은 bootstrap으로 생성된 다수의 training data를 대상으로 tree 모형을 적합하고 통합하여 최종 예측을 실시하는 방법으로써 single tree 모형보다 예측 분산을 많이 낮출 수 있다는 장점이 있다.
하지만 나름의 한계가 있는데, 그것은 bootstrap으로 생성된 training data 사이에는 유사성이 존재하기 때문에
모든 training data의 tree 모형 구조가 몇몇 중요한 설명변수에 의하여 공통적으로 결정되어 거의 비슷해질 가능성이 높다는 점이다.
비슷한 구조의 tree 모형을 통합하면 예측 결과 사이의 상관관계를 높이는 효과가 있기 때문에, 분산을 더 낮추기 위해서는 tree 모형 사이의 상관관계를 더 낮추어야 한다.</p>
<p>Random forest는 bagging과 동일하게 bootstrap으로 생성된 training data를 대상으로 tree 모형을 적합하고 통합하여 예측을 실시하는 방법이다.
차이점은 통합되는 tree 모형 사이에 상관관계를 더 낮출 수 있는 방법이 적용된다는 점이다.
Tree 모형의 building 과정은 설명변수의 공간 분할로 이루어지는데,
매번 분할을 실시할 때마다 설명변수 중 임의로 추출한 <span class="math inline">\(m\)</span> 개의 설명변수만을 대상으로 최적 분할을 실시하게 되면 중요 변수가 제외되어 다른 구조의 tree 모형이 생성될 가능성이 있다.
이렇게 되면 통합되는 tree 모형 사이에 상관관계를 많이 낮출 수 있어서 예측 분산이 더 작아질 수 있게 된다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> <code>caret</code>에 의한 Random Forest</strong></p>
<p><code>caret</code>에서 함수 <code>train()</code>으로 random forest를 실행하기 위해서는 패키지 <code>randomForest</code>가 설치되어야 하며,
<code>method = 'rf'</code>를 지정해야 한다.</p>
<ul>
<li>예제 : <code>MASS::Boston</code></li>
</ul>
<p>Regression tree 모형에 대한 random forest 예제로써 <a href="chapter-tree-model.html#section-reg-tree">3.1.1</a>절에서 살펴본 <code>MASS::Boston</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb236-1"><a href="chapter-tree-model.html#cb236-1" tabindex="-1"></a><span class="fu">data</span>(Boston, <span class="at">package=</span><span class="st">&quot;MASS&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb237-1"><a href="chapter-tree-model.html#cb237-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb237-2"><a href="chapter-tree-model.html#cb237-2" tabindex="-1"></a>train.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Boston<span class="sc">$</span>medv, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb237-3"><a href="chapter-tree-model.html#cb237-3" tabindex="-1"></a>train_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(train.id)</span>
<span id="cb237-4"><a href="chapter-tree-model.html#cb237-4" tabindex="-1"></a>test_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>train.id)</span></code></pre></div>
<p>Tuning parameter는 임의로 선택할 설명변수의 개수인 <code>mtry</code>이며 <code>tuneLength</code>로 적용될 grid의 길이를 조절한다.
Bootstrap sample의 개수는 <code>ntree</code>로 조절하는데, 디폴트는 <code>ntree = 500</code>이다.
<code>importance</code>에는 변수 중요도의 계산이 필요하면 <code>TRUE</code>를 지정해야 하는데, 적합되는 tree 모형의 개수가 많기 떄문에 시간이 더 걸리게 된다.</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb238-1"><a href="chapter-tree-model.html#cb238-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&quot;cv&quot;</span>, <span class="at">number=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb239-1"><a href="chapter-tree-model.html#cb239-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb239-2"><a href="chapter-tree-model.html#cb239-2" tabindex="-1"></a>m1_rf <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb239-3"><a href="chapter-tree-model.html#cb239-3" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb239-4"><a href="chapter-tree-model.html#cb239-4" tabindex="-1"></a>               <span class="at">importance =</span> <span class="cn">TRUE</span>, <span class="at">trControl =</span> ctrl)</span></code></pre></div>
<p>모형의 적합 결과를 살펴보자.</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb240-1"><a href="chapter-tree-model.html#cb240-1" tabindex="-1"></a>m1_rf</span>
<span id="cb240-2"><a href="chapter-tree-model.html#cb240-2" tabindex="-1"></a><span class="do">## Random Forest </span></span>
<span id="cb240-3"><a href="chapter-tree-model.html#cb240-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb240-4"><a href="chapter-tree-model.html#cb240-4" tabindex="-1"></a><span class="do">## 356 samples</span></span>
<span id="cb240-5"><a href="chapter-tree-model.html#cb240-5" tabindex="-1"></a><span class="do">##  13 predictor</span></span>
<span id="cb240-6"><a href="chapter-tree-model.html#cb240-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb240-7"><a href="chapter-tree-model.html#cb240-7" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb240-8"><a href="chapter-tree-model.html#cb240-8" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb240-9"><a href="chapter-tree-model.html#cb240-9" tabindex="-1"></a><span class="do">## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... </span></span>
<span id="cb240-10"><a href="chapter-tree-model.html#cb240-10" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb240-11"><a href="chapter-tree-model.html#cb240-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb240-12"><a href="chapter-tree-model.html#cb240-12" tabindex="-1"></a><span class="do">##   mtry  RMSE      Rsquared   MAE     </span></span>
<span id="cb240-13"><a href="chapter-tree-model.html#cb240-13" tabindex="-1"></a><span class="do">##    2    3.635893  0.8544814  2.456726</span></span>
<span id="cb240-14"><a href="chapter-tree-model.html#cb240-14" tabindex="-1"></a><span class="do">##    4    3.324916  0.8730989  2.268706</span></span>
<span id="cb240-15"><a href="chapter-tree-model.html#cb240-15" tabindex="-1"></a><span class="do">##    7    3.252076  0.8762433  2.243742</span></span>
<span id="cb240-16"><a href="chapter-tree-model.html#cb240-16" tabindex="-1"></a><span class="do">##   10    3.347749  0.8674172  2.311114</span></span>
<span id="cb240-17"><a href="chapter-tree-model.html#cb240-17" tabindex="-1"></a><span class="do">##   13    3.425963  0.8596083  2.341488</span></span>
<span id="cb240-18"><a href="chapter-tree-model.html#cb240-18" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb240-19"><a href="chapter-tree-model.html#cb240-19" tabindex="-1"></a><span class="do">## RMSE was used to select the optimal model using the smallest value.</span></span>
<span id="cb240-20"><a href="chapter-tree-model.html#cb240-20" tabindex="-1"></a><span class="do">## The final value used for the model was mtry = 7.</span></span></code></pre></div>
<p>Tuning parameter인 <code>mtry</code>에 따른 RMSE의 변화를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb241-1"><a href="chapter-tree-model.html#cb241-1" tabindex="-1"></a><span class="fu">ggplot</span>(m1_rf)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-rf-mtry"></span>
<img src="_main_files/figure-html/m1-rf-mtry-1.png" alt="`Boston` 자료에 대한 Random Forest의 `mtry`에 따른 RMSE 변화" width="576" />
<p class="caption">
그림 3.10: <code>Boston</code> 자료에 대한 Random Forest의 <code>mtry</code>에 따른 RMSE 변화
</p>
</div>
<p>모형 <code>m1_rf</code>의 변수 중요도를 측정해서 그래프로 표현해 보자.</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb242-1"><a href="chapter-tree-model.html#cb242-1" tabindex="-1"></a><span class="fu">varImp</span>(m1_rf) <span class="sc">|&gt;</span> </span>
<span id="cb242-2"><a href="chapter-tree-model.html#cb242-2" tabindex="-1"></a>  <span class="fu">ggplot</span>()</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-rf-varimp"></span>
<img src="_main_files/figure-html/m1-rf-varimp-1.png" alt="`Boston` 자료에 대한 Random Forest의 변수 중요도" width="576" />
<p class="caption">
그림 3.11: <code>Boston</code> 자료에 대한 Random Forest의 변수 중요도
</p>
</div>
<p>Test data에 대한 예측을 실시하고 평가해 보자.</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb243-1"><a href="chapter-tree-model.html#cb243-1" tabindex="-1"></a>pred_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1_rf, test_B)</span>
<span id="cb243-2"><a href="chapter-tree-model.html#cb243-2" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred_rf, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb243-3"><a href="chapter-tree-model.html#cb243-3" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb243-4"><a href="chapter-tree-model.html#cb243-4" tabindex="-1"></a><span class="do">## 3.6502337 0.8629376 2.3044936</span></span></code></pre></div>
<p>예측 결과를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb244-1"><a href="chapter-tree-model.html#cb244-1" tabindex="-1"></a><span class="fu">tibble</span>(<span class="at">pred =</span> pred_rf, <span class="at">obs =</span> test_B<span class="sc">$</span>medv) <span class="sc">|&gt;</span> </span>
<span id="cb244-2"><a href="chapter-tree-model.html#cb244-2" tabindex="-1"></a>  <span class="fu">ggplot</span>(<span class="fu">aes</span>(<span class="at">x =</span> obs, <span class="at">y =</span> pred)) <span class="sc">+</span></span>
<span id="cb244-3"><a href="chapter-tree-model.html#cb244-3" tabindex="-1"></a>  <span class="fu">geom_point</span>() <span class="sc">+</span></span>
<span id="cb244-4"><a href="chapter-tree-model.html#cb244-4" tabindex="-1"></a>  <span class="fu">geom_abline</span>(<span class="at">intercept =</span> <span class="dv">0</span>, <span class="at">slope =</span> <span class="dv">1</span>) <span class="sc">+</span></span>
<span id="cb244-5"><a href="chapter-tree-model.html#cb244-5" tabindex="-1"></a>  <span class="fu">labs</span>(<span class="at">x =</span> <span class="st">&quot;Observed data&quot;</span>, <span class="at">y =</span> <span class="st">&quot;Predicted data&quot;</span>)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-rf-predict"></span>
<img src="_main_files/figure-html/m1-rf-predict-1.png" alt="`Boston` 자료에 대한 Random Forest의 예측 결과" width="576" />
<p class="caption">
그림 3.12: <code>Boston</code> 자료에 대한 Random Forest의 예측 결과
</p>
</div>
<ul>
<li>예제 : <code>carData::Mroz</code></li>
</ul>
<p>Classification tree 모형에 대한 Random Forest 예제로써 <a href="chapter-tree-model.html#section-classification-tree">3.1.2</a>절에서 살펴본 <code>carData::Mroz</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb245"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb245-1"><a href="chapter-tree-model.html#cb245-1" tabindex="-1"></a><span class="fu">data</span>(Mroz, <span class="at">package=</span><span class="st">&quot;carData&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb246-1"><a href="chapter-tree-model.html#cb246-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb246-2"><a href="chapter-tree-model.html#cb246-2" tabindex="-1"></a>x.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Mroz<span class="sc">$</span>lfp, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb246-3"><a href="chapter-tree-model.html#cb246-3" tabindex="-1"></a>train_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(x.id)</span>
<span id="cb246-4"><a href="chapter-tree-model.html#cb246-4" tabindex="-1"></a>test_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>x.id)</span></code></pre></div>
<p>Random Forest 모형을 적합해 보자.</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb247-1"><a href="chapter-tree-model.html#cb247-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb248"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb248-1"><a href="chapter-tree-model.html#cb248-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb248-2"><a href="chapter-tree-model.html#cb248-2" tabindex="-1"></a>m2_rf <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb248-3"><a href="chapter-tree-model.html#cb248-3" tabindex="-1"></a>               <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">5</span>, </span>
<span id="cb248-4"><a href="chapter-tree-model.html#cb248-4" tabindex="-1"></a>               <span class="at">importance =</span> <span class="cn">TRUE</span>, <span class="at">trControl =</span> ctrl)</span></code></pre></div>
<p>적합 결과를 확인해 보자.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb249-1"><a href="chapter-tree-model.html#cb249-1" tabindex="-1"></a>m2_rf</span>
<span id="cb249-2"><a href="chapter-tree-model.html#cb249-2" tabindex="-1"></a><span class="do">## Random Forest </span></span>
<span id="cb249-3"><a href="chapter-tree-model.html#cb249-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb249-4"><a href="chapter-tree-model.html#cb249-4" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb249-5"><a href="chapter-tree-model.html#cb249-5" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb249-6"><a href="chapter-tree-model.html#cb249-6" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb249-7"><a href="chapter-tree-model.html#cb249-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb249-8"><a href="chapter-tree-model.html#cb249-8" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb249-9"><a href="chapter-tree-model.html#cb249-9" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb249-10"><a href="chapter-tree-model.html#cb249-10" tabindex="-1"></a><span class="do">## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... </span></span>
<span id="cb249-11"><a href="chapter-tree-model.html#cb249-11" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb249-12"><a href="chapter-tree-model.html#cb249-12" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb249-13"><a href="chapter-tree-model.html#cb249-13" tabindex="-1"></a><span class="do">##   mtry  Accuracy   Kappa    </span></span>
<span id="cb249-14"><a href="chapter-tree-model.html#cb249-14" tabindex="-1"></a><span class="do">##   2     0.7729508  0.5381514</span></span>
<span id="cb249-15"><a href="chapter-tree-model.html#cb249-15" tabindex="-1"></a><span class="do">##   3     0.7713115  0.5339499</span></span>
<span id="cb249-16"><a href="chapter-tree-model.html#cb249-16" tabindex="-1"></a><span class="do">##   4     0.7713388  0.5341556</span></span>
<span id="cb249-17"><a href="chapter-tree-model.html#cb249-17" tabindex="-1"></a><span class="do">##   5     0.7680874  0.5275613</span></span>
<span id="cb249-18"><a href="chapter-tree-model.html#cb249-18" tabindex="-1"></a><span class="do">##   7     0.7646721  0.5218882</span></span>
<span id="cb249-19"><a href="chapter-tree-model.html#cb249-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb249-20"><a href="chapter-tree-model.html#cb249-20" tabindex="-1"></a><span class="do">## Accuracy was used to select the optimal model using the largest value.</span></span>
<span id="cb249-21"><a href="chapter-tree-model.html#cb249-21" tabindex="-1"></a><span class="do">## The final value used for the model was mtry = 2.</span></span></code></pre></div>
<p>Tuning parameter인 <code>mtry</code>의 값에 따른 모형 <code>m2_rf</code>의 CV Accuracy의 그래프를 작성해 보자.
<code>mtry = 2</code>에서 최대값을 갖고 있다.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb250-1"><a href="chapter-tree-model.html#cb250-1" tabindex="-1"></a><span class="fu">ggplot</span>(m2_rf)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m2-rf-mtry-1"></span>
<img src="_main_files/figure-html/m2-rf-mtry-1-1.png" alt="`Mroz` 자료에 대한 Random Forest의 `mtry`에 따른 Accuracy의 변화" width="576" />
<p class="caption">
그림 3.13: <code>Mroz</code> 자료에 대한 Random Forest의 <code>mtry</code>에 따른 Accuracy의 변화
</p>
</div>
<p>모형 <code>m2_rf</code>는 Accuracy를 근거로 선택된 모형이다.
다른 평가 측도인 F1 score를 근거로 모형을 선택해 보자.</p>
<div class="sourceCode" id="cb251"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb251-1"><a href="chapter-tree-model.html#cb251-1" tabindex="-1"></a>ctrl_1 <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>,</span>
<span id="cb251-2"><a href="chapter-tree-model.html#cb251-2" tabindex="-1"></a>                         <span class="at">classProbs =</span> <span class="cn">TRUE</span>,</span>
<span id="cb251-3"><a href="chapter-tree-model.html#cb251-3" tabindex="-1"></a>                         <span class="at">summaryFunction =</span> prSummary)</span></code></pre></div>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb252-1"><a href="chapter-tree-model.html#cb252-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb252-2"><a href="chapter-tree-model.html#cb252-2" tabindex="-1"></a>m21_rf <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M, </span>
<span id="cb252-3"><a href="chapter-tree-model.html#cb252-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="at">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb252-4"><a href="chapter-tree-model.html#cb252-4" tabindex="-1"></a>                <span class="at">metric =</span> <span class="st">&quot;F&quot;</span>,</span>
<span id="cb252-5"><a href="chapter-tree-model.html#cb252-5" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl_1)</span></code></pre></div>
<p>적합 결과를 확인해 보자.</p>
<div class="sourceCode" id="cb253"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb253-1"><a href="chapter-tree-model.html#cb253-1" tabindex="-1"></a>m21_rf</span>
<span id="cb253-2"><a href="chapter-tree-model.html#cb253-2" tabindex="-1"></a><span class="do">## Random Forest </span></span>
<span id="cb253-3"><a href="chapter-tree-model.html#cb253-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb253-4"><a href="chapter-tree-model.html#cb253-4" tabindex="-1"></a><span class="do">## 603 samples</span></span>
<span id="cb253-5"><a href="chapter-tree-model.html#cb253-5" tabindex="-1"></a><span class="do">##   7 predictor</span></span>
<span id="cb253-6"><a href="chapter-tree-model.html#cb253-6" tabindex="-1"></a><span class="do">##   2 classes: &#39;no&#39;, &#39;yes&#39; </span></span>
<span id="cb253-7"><a href="chapter-tree-model.html#cb253-7" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb253-8"><a href="chapter-tree-model.html#cb253-8" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb253-9"><a href="chapter-tree-model.html#cb253-9" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb253-10"><a href="chapter-tree-model.html#cb253-10" tabindex="-1"></a><span class="do">## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... </span></span>
<span id="cb253-11"><a href="chapter-tree-model.html#cb253-11" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb253-12"><a href="chapter-tree-model.html#cb253-12" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb253-13"><a href="chapter-tree-model.html#cb253-13" tabindex="-1"></a><span class="do">##   mtry  AUC        Precision  Recall     F        </span></span>
<span id="cb253-14"><a href="chapter-tree-model.html#cb253-14" tabindex="-1"></a><span class="do">##   2     0.7045656  0.7366678  0.7576923  0.7444784</span></span>
<span id="cb253-15"><a href="chapter-tree-model.html#cb253-15" tabindex="-1"></a><span class="do">##   3     0.7230455  0.7368191  0.7384615  0.7345407</span></span>
<span id="cb253-16"><a href="chapter-tree-model.html#cb253-16" tabindex="-1"></a><span class="do">##   4     0.7306750  0.7452474  0.7538462  0.7466735</span></span>
<span id="cb253-17"><a href="chapter-tree-model.html#cb253-17" tabindex="-1"></a><span class="do">##   5     0.7348559  0.7449061  0.7500000  0.7450132</span></span>
<span id="cb253-18"><a href="chapter-tree-model.html#cb253-18" tabindex="-1"></a><span class="do">##   7     0.7323882  0.7315206  0.7461538  0.7366194</span></span>
<span id="cb253-19"><a href="chapter-tree-model.html#cb253-19" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb253-20"><a href="chapter-tree-model.html#cb253-20" tabindex="-1"></a><span class="do">## F was used to select the optimal model using the largest value.</span></span>
<span id="cb253-21"><a href="chapter-tree-model.html#cb253-21" tabindex="-1"></a><span class="do">## The final value used for the model was mtry = 4.</span></span></code></pre></div>
<p>Tuning parameter인 <code>mtry</code>의 값에 따른 모형 <code>m21_rf</code>의 CV F1 score의 그래프를 작성해 보자.
<code>mtry = 4</code>에서 최대값을 갖고 있다.</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb254-1"><a href="chapter-tree-model.html#cb254-1" tabindex="-1"></a><span class="fu">ggplot</span>(m21_rf)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m2-rf-mtry-2"></span>
<img src="_main_files/figure-html/m2-rf-mtry-2-1.png" alt="`Mroz` 자료에 대한 Random Forest의 `mtry`에 따른 Accuracy의 변화" width="576" />
<p class="caption">
그림 3.14: <code>Mroz</code> 자료에 대한 Random Forest의 <code>mtry</code>에 따른 Accuracy의 변화
</p>
</div>
<p>모형 <code>m2_rf</code>와 <code>m21_rf</code>의 변수 중요도를 각각 측정해서 그래프로 나타내자.</p>
<div class="figure"><span style="display:block;" id="fig:unnamed-chunk-207"></span>
<img src="_main_files/figure-html/unnamed-chunk-207-1.png" alt="`Mroz` 자료에 대한 Random Forest의 변수 중요도" width="768" />
<p class="caption">
그림 3.15: <code>Mroz</code> 자료에 대한 Random Forest의 변수 중요도
</p>
</div>
<p>Test data에 대한 모형 <code>m2_rf</code>와 <code>m21_rf</code>의 예측을 실시하고 분류 성능을 평가해 보자.</p>
<div class="sourceCode" id="cb255"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb255-1"><a href="chapter-tree-model.html#cb255-1" tabindex="-1"></a>pred2_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2_rf, test_M)</span>
<span id="cb255-2"><a href="chapter-tree-model.html#cb255-2" tabindex="-1"></a>pred21_rf <span class="ot">&lt;-</span> <span class="fu">predict</span>(m21_rf, test_M)</span></code></pre></div>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb256-1"><a href="chapter-tree-model.html#cb256-1" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred2_rf, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp,</span>
<span id="cb256-2"><a href="chapter-tree-model.html#cb256-2" tabindex="-1"></a>                <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb256-3"><a href="chapter-tree-model.html#cb256-3" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb256-4"><a href="chapter-tree-model.html#cb256-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb256-5"><a href="chapter-tree-model.html#cb256-5" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb256-6"><a href="chapter-tree-model.html#cb256-6" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb256-7"><a href="chapter-tree-model.html#cb256-7" tabindex="-1"></a><span class="do">##        no  45  27</span></span>
<span id="cb256-8"><a href="chapter-tree-model.html#cb256-8" tabindex="-1"></a><span class="do">##        yes 20  58</span></span>
<span id="cb256-9"><a href="chapter-tree-model.html#cb256-9" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb256-10"><a href="chapter-tree-model.html#cb256-10" tabindex="-1"></a><span class="do">##                Accuracy : 0.6867          </span></span>
<span id="cb256-11"><a href="chapter-tree-model.html#cb256-11" tabindex="-1"></a><span class="do">##                  95% CI : (0.6059, 0.7598)</span></span>
<span id="cb256-12"><a href="chapter-tree-model.html#cb256-12" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667          </span></span>
<span id="cb256-13"><a href="chapter-tree-model.html#cb256-13" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 0.001728        </span></span>
<span id="cb256-14"><a href="chapter-tree-model.html#cb256-14" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb256-15"><a href="chapter-tree-model.html#cb256-15" tabindex="-1"></a><span class="do">##                   Kappa : 0.37            </span></span>
<span id="cb256-16"><a href="chapter-tree-model.html#cb256-16" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb256-17"><a href="chapter-tree-model.html#cb256-17" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.381471        </span></span>
<span id="cb256-18"><a href="chapter-tree-model.html#cb256-18" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb256-19"><a href="chapter-tree-model.html#cb256-19" tabindex="-1"></a><span class="do">##             Sensitivity : 0.6824          </span></span>
<span id="cb256-20"><a href="chapter-tree-model.html#cb256-20" tabindex="-1"></a><span class="do">##             Specificity : 0.6923          </span></span>
<span id="cb256-21"><a href="chapter-tree-model.html#cb256-21" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7436          </span></span>
<span id="cb256-22"><a href="chapter-tree-model.html#cb256-22" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6250          </span></span>
<span id="cb256-23"><a href="chapter-tree-model.html#cb256-23" tabindex="-1"></a><span class="do">##               Precision : 0.7436          </span></span>
<span id="cb256-24"><a href="chapter-tree-model.html#cb256-24" tabindex="-1"></a><span class="do">##                  Recall : 0.6824          </span></span>
<span id="cb256-25"><a href="chapter-tree-model.html#cb256-25" tabindex="-1"></a><span class="do">##                      F1 : 0.7117          </span></span>
<span id="cb256-26"><a href="chapter-tree-model.html#cb256-26" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667          </span></span>
<span id="cb256-27"><a href="chapter-tree-model.html#cb256-27" tabindex="-1"></a><span class="do">##          Detection Rate : 0.3867          </span></span>
<span id="cb256-28"><a href="chapter-tree-model.html#cb256-28" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5200          </span></span>
<span id="cb256-29"><a href="chapter-tree-model.html#cb256-29" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.6873          </span></span>
<span id="cb256-30"><a href="chapter-tree-model.html#cb256-30" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb256-31"><a href="chapter-tree-model.html#cb256-31" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes             </span></span>
<span id="cb256-32"><a href="chapter-tree-model.html#cb256-32" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
<div class="sourceCode" id="cb257"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb257-1"><a href="chapter-tree-model.html#cb257-1" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred21_rf, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp,</span>
<span id="cb257-2"><a href="chapter-tree-model.html#cb257-2" tabindex="-1"></a>                <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb257-3"><a href="chapter-tree-model.html#cb257-3" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb257-4"><a href="chapter-tree-model.html#cb257-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb257-5"><a href="chapter-tree-model.html#cb257-5" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb257-6"><a href="chapter-tree-model.html#cb257-6" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb257-7"><a href="chapter-tree-model.html#cb257-7" tabindex="-1"></a><span class="do">##        no  47  27</span></span>
<span id="cb257-8"><a href="chapter-tree-model.html#cb257-8" tabindex="-1"></a><span class="do">##        yes 18  58</span></span>
<span id="cb257-9"><a href="chapter-tree-model.html#cb257-9" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb257-10"><a href="chapter-tree-model.html#cb257-10" tabindex="-1"></a><span class="do">##                Accuracy : 0.7            </span></span>
<span id="cb257-11"><a href="chapter-tree-model.html#cb257-11" tabindex="-1"></a><span class="do">##                  95% CI : (0.6199, 0.772)</span></span>
<span id="cb257-12"><a href="chapter-tree-model.html#cb257-12" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667         </span></span>
<span id="cb257-13"><a href="chapter-tree-model.html#cb257-13" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 0.0005439      </span></span>
<span id="cb257-14"><a href="chapter-tree-model.html#cb257-14" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb257-15"><a href="chapter-tree-model.html#cb257-15" tabindex="-1"></a><span class="do">##                   Kappa : 0.3989         </span></span>
<span id="cb257-16"><a href="chapter-tree-model.html#cb257-16" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb257-17"><a href="chapter-tree-model.html#cb257-17" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.2330380      </span></span>
<span id="cb257-18"><a href="chapter-tree-model.html#cb257-18" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb257-19"><a href="chapter-tree-model.html#cb257-19" tabindex="-1"></a><span class="do">##             Sensitivity : 0.6824         </span></span>
<span id="cb257-20"><a href="chapter-tree-model.html#cb257-20" tabindex="-1"></a><span class="do">##             Specificity : 0.7231         </span></span>
<span id="cb257-21"><a href="chapter-tree-model.html#cb257-21" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7632         </span></span>
<span id="cb257-22"><a href="chapter-tree-model.html#cb257-22" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6351         </span></span>
<span id="cb257-23"><a href="chapter-tree-model.html#cb257-23" tabindex="-1"></a><span class="do">##               Precision : 0.7632         </span></span>
<span id="cb257-24"><a href="chapter-tree-model.html#cb257-24" tabindex="-1"></a><span class="do">##                  Recall : 0.6824         </span></span>
<span id="cb257-25"><a href="chapter-tree-model.html#cb257-25" tabindex="-1"></a><span class="do">##                      F1 : 0.7205         </span></span>
<span id="cb257-26"><a href="chapter-tree-model.html#cb257-26" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667         </span></span>
<span id="cb257-27"><a href="chapter-tree-model.html#cb257-27" tabindex="-1"></a><span class="do">##          Detection Rate : 0.3867         </span></span>
<span id="cb257-28"><a href="chapter-tree-model.html#cb257-28" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5067         </span></span>
<span id="cb257-29"><a href="chapter-tree-model.html#cb257-29" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.7027         </span></span>
<span id="cb257-30"><a href="chapter-tree-model.html#cb257-30" tabindex="-1"></a><span class="do">##                                          </span></span>
<span id="cb257-31"><a href="chapter-tree-model.html#cb257-31" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes            </span></span>
<span id="cb257-32"><a href="chapter-tree-model.html#cb257-32" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
</div>
<div id="boosting" class="section level2 hasAnchor" number="3.4">
<h2><span class="header-section-number">3.4</span> Boosting<a href="chapter-tree-model.html#boosting" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Bagging과 random forest는 decision tree 모형의 분산을 낮추기 위해 제안된 모형이다.
공통적으로 적용되는 방식은 bootstrap으로 생성된 다수의 training data를 대상으로 tree 모형을 각각 적합하고 통합하여 최종 예측 결과를 산출하는 것이다.
Boosting도 decision tree 모형의 분산을 낮추기 위해 제안된 모형이며, 다수의 tree 모형을 적합하지만 전혀 다른 방법이 적용된다.
우선 bootstrap을 사용하지 않기 때문에 여러 개의 독립된 tree 모형을 통합하는 과정이 없다.
대신 순차적인 통합 방식을 사용하는데, 우선 첫 번째 단계에서 원자료를 대상으로 단순한 형태의 tree 모형을 적합한다.
단순한 형태의 tree 모형이란 작은 횟수의 분할이 이루어진 모형을 의미한다.
이어서 두 번째 단계에서는 원자료가 아닌 첫 번째 단계에서 생성된 잔차를 대상으로 다시 단순한 형태의 tree 모형을 적합한다.
이후 단계에서도 바로 전 단계 모형에서 생성된 잔차를 대상으로 단순한 형태의 tree 모형의 적합 과정을 반복한다.
단순한 tree 모형은 정확도가 낮은 예측 모형이지만, 정확도가 낮은 여러 개의 모형을 순차적으로 통합하여 정확도가 높은 모형을 생성하는 것이 boosting의 기본 개념이다.</p>
<p>Boosting에는 몇 가지 다른 방식이 있는데, 여기에서는 Stochastic gradient boosting 기법에 대해 살펴보겠다.
회귀 모형 <span class="math inline">\(Y = f(X) + \varepsilon\)</span> 에 대한 boosting 기법은 다음과 같다.</p>
<ol style="list-style-type: decimal">
<li><p>Tuning parameter인 분할 횟수 <span class="math inline">\(d\)</span> 와 반복 횟수 <span class="math inline">\(B\)</span>, 그리고 <span class="math inline">\(\lambda\)</span> 값 지정</p></li>
<li><p>반응변수의 평균 <span class="math inline">\(\overline{y}\)</span> 를 함수 <span class="math inline">\(f(x)\)</span> 의 첫 번째 추정값으로 지정: <span class="math inline">\(\hat{f}(x) = \overline{y}\)</span></p></li>
<li><p>다음의 절차를 <span class="math inline">\(b = 1, \ldots, B\)</span> 에 대해 반복</p></li>
</ol>
<ul>
<li><p>Training data 중 일부분 선택</p></li>
<li><p>관찰값과 현재 모형의 추정값 차이(잔차) 계산: <span class="math inline">\(r = y - \hat{f}(x)\)</span></p></li>
<li><p>잔차를 대상으로 분할 횟수 <span class="math inline">\(d\)</span> 에 의한 regression tree 모형 <span class="math inline">\(\hat{f}^{b}(x)\)</span> 적합</p></li>
<li><p>추정 모형 업데이트: <span class="math inline">\(\hat{f}(x) \leftarrow \hat{f}(x) + \lambda \hat{f}^{b}(x)\)</span></p></li>
</ul>
<p>최종 boosting 모형의 적합 결과는 다음과 같다.</p>
<p><span class="math display">\[\begin{equation}
\hat{f}(x) = \sum_{b=1}^{B} \lambda \hat{f}^{b}(x)
\end{equation}\]</span></p>
<p>위에서 살펴본 boosting 절차에서 사용된 3가지 tuning parameter의 기능을 살펴보자.
첫 번째 tuning parameter인 <span class="math inline">\(d\)</span> 는 tree 모형의 분할 횟수를 지정하는 것으로써,
대부분 단순한 형태의 tree 모형을 적합하는 것이 좋은 결과를 보이는 것으로 알려져 있다.
<span class="math inline">\(d=1\)</span> 으로도 좋은 결과를 얻는 경우도 가끔 있다.</p>
<p>두 번째 tuning parameter인 <span class="math inline">\(B\)</span> 는 boosting 반복 횟수를 지정한다.
큰 값을 지정할수록 training data에 대한 더 정확한 적합이 이루어지겠지만,
지나치게 많은 반복을 실행하는 것은 overfitting의 가능성을 높이는 것이 된다.</p>
<p>세 번째 tuning parameter인 <span class="math inline">\(\lambda\)</span> 는 추정 모형의 업데이트 속도, 즉 learning rate를 조절하는 역할을 하는 shrinkage parameter이다.
큰 값을 지정하면 작은 횟수의 반복으로도 정확도가 높은 모형을 빠르게 만들 수 있지만,
overfitting의 가능성이 매우 높아질 수 있다.
가능한 작은 값을 지정하여 천천히 최적 모형을 만드는 것이 일반적으로 적용되는 방식이다.</p>
<p><strong><span class="math inline">\(\bullet\)</span> <code>caret</code>에 의한 Stochastic Gradient Boosting</strong></p>
<p><code>caret</code>에서 함수 <code>train()</code>으로 stochastic gradient boosting을 실행하기 위해서는 패키지 <code>gbm</code>이 설치되어야 하며,
<code>method = 'gbm'</code>를 지정해야 한다.</p>
<ul>
<li>예제 : <code>MASS::Boston</code></li>
</ul>
<p>Regression tree 모형에 대한 random forest 예제로써 <a href="chapter-tree-model.html#section-reg-tree">3.1.1</a>절에서 살펴본 <code>MASS::Boston</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb258-1"><a href="chapter-tree-model.html#cb258-1" tabindex="-1"></a><span class="fu">data</span>(Boston, <span class="at">package=</span><span class="st">&quot;MASS&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb259"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb259-1"><a href="chapter-tree-model.html#cb259-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb259-2"><a href="chapter-tree-model.html#cb259-2" tabindex="-1"></a>train.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Boston<span class="sc">$</span>medv, <span class="at">p =</span> <span class="fl">0.7</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb259-3"><a href="chapter-tree-model.html#cb259-3" tabindex="-1"></a>train_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(train.id)</span>
<span id="cb259-4"><a href="chapter-tree-model.html#cb259-4" tabindex="-1"></a>test_B <span class="ot">&lt;-</span> Boston <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>train.id)</span></code></pre></div>
<p>함수 <code>train()</code>에서 사용되는 tuning parameter에는 boosting 반복 횟수를 지정하는 <code>n.trees</code>와 분할 횟수를 지정하는 <code>interaction.depth</code>, <span class="math inline">\(\lambda\)</span> 값을 지정하는 <code>shrinkage</code>, 그리고 terminal node를 구성하는 최소 자료 수를 지정하는 <code>n.minobsinnode</code>가 있다.
네 종류의 tuning parameter 값을 지정하는 방법은 <code>tuneLength</code>로 할 수 있지만,
사용자가 grid를 조절할 수 있는 <code>tuneGrid</code>를 사용할 수 있다.</p>
<p>먼저 <code>tuneLength</code>에 의한 grid 조절로 boosting 모형을 적합해 보자.</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb260-1"><a href="chapter-tree-model.html#cb260-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&#39;cv&#39;</span>, <span class="at">number =</span> <span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb261"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb261-1"><a href="chapter-tree-model.html#cb261-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb261-2"><a href="chapter-tree-model.html#cb261-2" tabindex="-1"></a>m1_gbm <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B,</span>
<span id="cb261-3"><a href="chapter-tree-model.html#cb261-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>, <span class="at">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb261-4"><a href="chapter-tree-model.html#cb261-4" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><code>verbose = FALSE</code>을 지정해서 추정 과정에 대한 출력을 생략했다.
<code>m1_gbm</code>의 적합 결과를 살펴보자.</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb262-1"><a href="chapter-tree-model.html#cb262-1" tabindex="-1"></a>m1_gbm</span>
<span id="cb262-2"><a href="chapter-tree-model.html#cb262-2" tabindex="-1"></a><span class="do">## Stochastic Gradient Boosting </span></span>
<span id="cb262-3"><a href="chapter-tree-model.html#cb262-3" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb262-4"><a href="chapter-tree-model.html#cb262-4" tabindex="-1"></a><span class="do">## 356 samples</span></span>
<span id="cb262-5"><a href="chapter-tree-model.html#cb262-5" tabindex="-1"></a><span class="do">##  13 predictor</span></span>
<span id="cb262-6"><a href="chapter-tree-model.html#cb262-6" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb262-7"><a href="chapter-tree-model.html#cb262-7" tabindex="-1"></a><span class="do">## No pre-processing</span></span>
<span id="cb262-8"><a href="chapter-tree-model.html#cb262-8" tabindex="-1"></a><span class="do">## Resampling: Cross-Validated (10 fold) </span></span>
<span id="cb262-9"><a href="chapter-tree-model.html#cb262-9" tabindex="-1"></a><span class="do">## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... </span></span>
<span id="cb262-10"><a href="chapter-tree-model.html#cb262-10" tabindex="-1"></a><span class="do">## Resampling results across tuning parameters:</span></span>
<span id="cb262-11"><a href="chapter-tree-model.html#cb262-11" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb262-12"><a href="chapter-tree-model.html#cb262-12" tabindex="-1"></a><span class="do">##   interaction.depth  n.trees  RMSE      Rsquared   MAE     </span></span>
<span id="cb262-13"><a href="chapter-tree-model.html#cb262-13" tabindex="-1"></a><span class="do">##   1                   50      3.858615  0.8189152  2.730638</span></span>
<span id="cb262-14"><a href="chapter-tree-model.html#cb262-14" tabindex="-1"></a><span class="do">##   1                  100      3.696736  0.8309254  2.585992</span></span>
<span id="cb262-15"><a href="chapter-tree-model.html#cb262-15" tabindex="-1"></a><span class="do">##   1                  150      3.702450  0.8322276  2.604707</span></span>
<span id="cb262-16"><a href="chapter-tree-model.html#cb262-16" tabindex="-1"></a><span class="do">##   1                  200      3.706732  0.8329330  2.627899</span></span>
<span id="cb262-17"><a href="chapter-tree-model.html#cb262-17" tabindex="-1"></a><span class="do">##   1                  250      3.658608  0.8373381  2.587262</span></span>
<span id="cb262-18"><a href="chapter-tree-model.html#cb262-18" tabindex="-1"></a><span class="do">##   2                   50      3.627679  0.8402229  2.525312</span></span>
<span id="cb262-19"><a href="chapter-tree-model.html#cb262-19" tabindex="-1"></a><span class="do">##   2                  100      3.506705  0.8505777  2.462068</span></span>
<span id="cb262-20"><a href="chapter-tree-model.html#cb262-20" tabindex="-1"></a><span class="do">##   2                  150      3.460677  0.8560659  2.484307</span></span>
<span id="cb262-21"><a href="chapter-tree-model.html#cb262-21" tabindex="-1"></a><span class="do">##   2                  200      3.429762  0.8590447  2.455930</span></span>
<span id="cb262-22"><a href="chapter-tree-model.html#cb262-22" tabindex="-1"></a><span class="do">##   2                  250      3.390288  0.8628167  2.460451</span></span>
<span id="cb262-23"><a href="chapter-tree-model.html#cb262-23" tabindex="-1"></a><span class="do">##   3                   50      3.509498  0.8480217  2.466777</span></span>
<span id="cb262-24"><a href="chapter-tree-model.html#cb262-24" tabindex="-1"></a><span class="do">##   3                  100      3.381205  0.8607337  2.394221</span></span>
<span id="cb262-25"><a href="chapter-tree-model.html#cb262-25" tabindex="-1"></a><span class="do">##   3                  150      3.322271  0.8658518  2.360338</span></span>
<span id="cb262-26"><a href="chapter-tree-model.html#cb262-26" tabindex="-1"></a><span class="do">##   3                  200      3.312736  0.8672853  2.355567</span></span>
<span id="cb262-27"><a href="chapter-tree-model.html#cb262-27" tabindex="-1"></a><span class="do">##   3                  250      3.307265  0.8669862  2.372043</span></span>
<span id="cb262-28"><a href="chapter-tree-model.html#cb262-28" tabindex="-1"></a><span class="do">##   4                   50      3.383680  0.8607782  2.390064</span></span>
<span id="cb262-29"><a href="chapter-tree-model.html#cb262-29" tabindex="-1"></a><span class="do">##   4                  100      3.262512  0.8716540  2.248759</span></span>
<span id="cb262-30"><a href="chapter-tree-model.html#cb262-30" tabindex="-1"></a><span class="do">##   4                  150      3.206835  0.8760505  2.239821</span></span>
<span id="cb262-31"><a href="chapter-tree-model.html#cb262-31" tabindex="-1"></a><span class="do">##   4                  200      3.189270  0.8767745  2.223124</span></span>
<span id="cb262-32"><a href="chapter-tree-model.html#cb262-32" tabindex="-1"></a><span class="do">##   4                  250      3.207784  0.8760043  2.239024</span></span>
<span id="cb262-33"><a href="chapter-tree-model.html#cb262-33" tabindex="-1"></a><span class="do">##   5                   50      3.432479  0.8589551  2.382405</span></span>
<span id="cb262-34"><a href="chapter-tree-model.html#cb262-34" tabindex="-1"></a><span class="do">##   5                  100      3.311370  0.8685337  2.346983</span></span>
<span id="cb262-35"><a href="chapter-tree-model.html#cb262-35" tabindex="-1"></a><span class="do">##   5                  150      3.308600  0.8696146  2.383152</span></span>
<span id="cb262-36"><a href="chapter-tree-model.html#cb262-36" tabindex="-1"></a><span class="do">##   5                  200      3.298083  0.8702767  2.363054</span></span>
<span id="cb262-37"><a href="chapter-tree-model.html#cb262-37" tabindex="-1"></a><span class="do">##   5                  250      3.277067  0.8718841  2.350316</span></span>
<span id="cb262-38"><a href="chapter-tree-model.html#cb262-38" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb262-39"><a href="chapter-tree-model.html#cb262-39" tabindex="-1"></a><span class="do">## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1</span></span>
<span id="cb262-40"><a href="chapter-tree-model.html#cb262-40" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb262-41"><a href="chapter-tree-model.html#cb262-41" tabindex="-1"></a><span class="do">## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10</span></span>
<span id="cb262-42"><a href="chapter-tree-model.html#cb262-42" tabindex="-1"></a><span class="do">## RMSE was used to select the optimal model using the smallest value.</span></span>
<span id="cb262-43"><a href="chapter-tree-model.html#cb262-43" tabindex="-1"></a><span class="do">## The final values used for the model were n.trees = 200, interaction.depth =</span></span>
<span id="cb262-44"><a href="chapter-tree-model.html#cb262-44" tabindex="-1"></a><span class="do">##  4, shrinkage = 0.1 and n.minobsinnode = 10.</span></span></code></pre></div>
<p>네 종류의 tuning parameter 중 <code>shrinkage</code>와 <code>n.minobsinnode</code>는 0.1과 10으로 각각 고정되었음을 알 수 있다.
Tuning parameter <code>n.trees</code>와 <code>interaction.depth</code>의 값에 따른 CV RMSE의 변화를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb263-1"><a href="chapter-tree-model.html#cb263-1" tabindex="-1"></a><span class="fu">ggplot</span>(m1_gbm)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m1-gbm-tune"></span>
<img src="_main_files/figure-html/m1-gbm-tune-1.png" alt="`Boston` 자료에 대한 boosting 모형의 `n.trees`와 `interaction.depth`의 값에 따른 CV RMSE의 변화" width="576" />
<p class="caption">
그림 3.16: <code>Boston</code> 자료에 대한 boosting 모형의 <code>n.trees</code>와 <code>interaction.depth</code>의 값에 따른 CV RMSE의 변화
</p>
</div>
<p><code>m1_gbm</code>에서 최적 tuning parameter 값은 다음과 같이 알아볼 수 있다.</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb264-1"><a href="chapter-tree-model.html#cb264-1" tabindex="-1"></a>m1_gbm<span class="sc">$</span>bestTune</span>
<span id="cb264-2"><a href="chapter-tree-model.html#cb264-2" tabindex="-1"></a><span class="do">##    n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb264-3"><a href="chapter-tree-model.html#cb264-3" tabindex="-1"></a><span class="do">## 19     200                 4       0.1             10</span></span></code></pre></div>
<p><code>tuneGrid</code>를 이용하여 사용자가 지정한 tuning parameter의 grid를 근거로 모형을 적합해 보자.
<code>tuneGrid</code>에는 네 종류 tuning parameter의 grid로 구성된 데이터 프레임을 지정해야 하는데,
함수 <code>expand.grid()</code>를 사용하는 것이 편하다.</p>
<div class="sourceCode" id="cb265"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb265-1"><a href="chapter-tree-model.html#cb265-1" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">n.trees =</span> <span class="fu">seq</span>(<span class="dv">50</span>, <span class="dv">350</span>, <span class="at">by =</span> <span class="dv">50</span>), </span>
<span id="cb265-2"><a href="chapter-tree-model.html#cb265-2" tabindex="-1"></a>                    <span class="at">interaction.depth =</span> <span class="dv">2</span><span class="sc">:</span><span class="dv">7</span>, </span>
<span id="cb265-3"><a href="chapter-tree-model.html#cb265-3" tabindex="-1"></a>                    <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>),</span>
<span id="cb265-4"><a href="chapter-tree-model.html#cb265-4" tabindex="-1"></a>                    <span class="at">n.minobsinnode =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))</span></code></pre></div>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb266-1"><a href="chapter-tree-model.html#cb266-1" tabindex="-1"></a><span class="fu">head</span>(grid)</span>
<span id="cb266-2"><a href="chapter-tree-model.html#cb266-2" tabindex="-1"></a><span class="do">##   n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb266-3"><a href="chapter-tree-model.html#cb266-3" tabindex="-1"></a><span class="do">## 1      50                 2      0.01              5</span></span>
<span id="cb266-4"><a href="chapter-tree-model.html#cb266-4" tabindex="-1"></a><span class="do">## 2     100                 2      0.01              5</span></span>
<span id="cb266-5"><a href="chapter-tree-model.html#cb266-5" tabindex="-1"></a><span class="do">## 3     150                 2      0.01              5</span></span>
<span id="cb266-6"><a href="chapter-tree-model.html#cb266-6" tabindex="-1"></a><span class="do">## 4     200                 2      0.01              5</span></span>
<span id="cb266-7"><a href="chapter-tree-model.html#cb266-7" tabindex="-1"></a><span class="do">## 5     250                 2      0.01              5</span></span>
<span id="cb266-8"><a href="chapter-tree-model.html#cb266-8" tabindex="-1"></a><span class="do">## 6     300                 2      0.01              5</span></span>
<span id="cb266-9"><a href="chapter-tree-model.html#cb266-9" tabindex="-1"></a><span class="fu">tail</span>(grid)</span>
<span id="cb266-10"><a href="chapter-tree-model.html#cb266-10" tabindex="-1"></a><span class="do">##     n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb266-11"><a href="chapter-tree-model.html#cb266-11" tabindex="-1"></a><span class="do">## 163     100                 7       0.1             10</span></span>
<span id="cb266-12"><a href="chapter-tree-model.html#cb266-12" tabindex="-1"></a><span class="do">## 164     150                 7       0.1             10</span></span>
<span id="cb266-13"><a href="chapter-tree-model.html#cb266-13" tabindex="-1"></a><span class="do">## 165     200                 7       0.1             10</span></span>
<span id="cb266-14"><a href="chapter-tree-model.html#cb266-14" tabindex="-1"></a><span class="do">## 166     250                 7       0.1             10</span></span>
<span id="cb266-15"><a href="chapter-tree-model.html#cb266-15" tabindex="-1"></a><span class="do">## 167     300                 7       0.1             10</span></span>
<span id="cb266-16"><a href="chapter-tree-model.html#cb266-16" tabindex="-1"></a><span class="do">## 168     350                 7       0.1             10</span></span></code></pre></div>
<p><code>tuneGrid</code>에 의한 모형을 적합해 보자.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb267-1"><a href="chapter-tree-model.html#cb267-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb267-2"><a href="chapter-tree-model.html#cb267-2" tabindex="-1"></a>m2_gbm <span class="ot">&lt;-</span> <span class="fu">train</span>(medv <span class="sc">~</span> ., <span class="at">data =</span> train_B, </span>
<span id="cb267-3"><a href="chapter-tree-model.html#cb267-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>, <span class="at">tuneGrid =</span> grid,</span>
<span id="cb267-4"><a href="chapter-tree-model.html#cb267-4" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><code>m2_gbm</code>에는 4종류 tuning parameter의 조합에 따른 168번의 모형 적합 결과가 입력되어 있다.
Tuning parameter 값에 따른 모형 <code>m2_gbm</code>의 CV RMSE의 변화를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb268-1"><a href="chapter-tree-model.html#cb268-1" tabindex="-1"></a><span class="fu">ggplot</span>(m2_gbm)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m2-gbm-tune"></span>
<img src="_main_files/figure-html/m2-gbm-tune-1.png" alt="`Boston` 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV RMSE의 변화" width="576" />
<p class="caption">
그림 3.17: <code>Boston</code> 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV RMSE의 변화
</p>
</div>
<p><code>m2_gbm</code>에서 최적 tuning parameter 값을 알아보자.</p>
<div class="sourceCode" id="cb269"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb269-1"><a href="chapter-tree-model.html#cb269-1" tabindex="-1"></a>m2_gbm<span class="sc">$</span>bestTune</span>
<span id="cb269-2"><a href="chapter-tree-model.html#cb269-2" tabindex="-1"></a><span class="do">##     n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb269-3"><a href="chapter-tree-model.html#cb269-3" tabindex="-1"></a><span class="do">## 157     150                 7       0.1              5</span></span></code></pre></div>
<p>두 모형 <code>m1_gbm</code>과 <code>m2_gbm</code>을 사용해서 test data에 대한 예측을 실시하고 결과를 평가해 보자.</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb270-1"><a href="chapter-tree-model.html#cb270-1" tabindex="-1"></a>pred1_gbm <span class="ot">&lt;-</span> <span class="fu">predict</span>(m1_gbm, test_B)</span>
<span id="cb270-2"><a href="chapter-tree-model.html#cb270-2" tabindex="-1"></a>pred2_gbm <span class="ot">&lt;-</span> <span class="fu">predict</span>(m2_gbm, test_B)</span></code></pre></div>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb271-1"><a href="chapter-tree-model.html#cb271-1" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred1_gbm, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb271-2"><a href="chapter-tree-model.html#cb271-2" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb271-3"><a href="chapter-tree-model.html#cb271-3" tabindex="-1"></a><span class="do">## 3.6560793 0.8553609 2.4901670</span></span>
<span id="cb271-4"><a href="chapter-tree-model.html#cb271-4" tabindex="-1"></a><span class="fu">defaultSummary</span>(<span class="fu">data.frame</span>(<span class="at">pred =</span> pred2_gbm, <span class="at">obs =</span> test_B<span class="sc">$</span>medv))</span>
<span id="cb271-5"><a href="chapter-tree-model.html#cb271-5" tabindex="-1"></a><span class="do">##      RMSE  Rsquared       MAE </span></span>
<span id="cb271-6"><a href="chapter-tree-model.html#cb271-6" tabindex="-1"></a><span class="do">## 3.5168430 0.8682371 2.3597790</span></span></code></pre></div>
<p>두 모형의 예측 결과를 그래프로 나타내 보자.</p>
<div class="figure"><span style="display:block;" id="fig:m12-gbm-predict"></span>
<img src="_main_files/figure-html/m12-gbm-predict-1.png" alt="`Boston` 자료에 대한 boosting 모형의 예측 결과" width="768" />
<p class="caption">
그림 3.18: <code>Boston</code> 자료에 대한 boosting 모형의 예측 결과
</p>
</div>
<ul>
<li>예제 : <code>carData::Mroz</code></li>
</ul>
<p>Classification tree 모형에 대한 Boosting 예제로써 <a href="chapter-tree-model.html#section-classification-tree">3.1.2</a>절에서 살펴본 <code>carData::Mroz</code>을 사용해 보자.</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="chapter-tree-model.html#cb272-1" tabindex="-1"></a><span class="fu">data</span>(Mroz, <span class="at">package=</span><span class="st">&quot;carData&quot;</span>)</span></code></pre></div>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="chapter-tree-model.html#cb273-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb273-2"><a href="chapter-tree-model.html#cb273-2" tabindex="-1"></a>x.id <span class="ot">&lt;-</span> <span class="fu">createDataPartition</span>(Mroz<span class="sc">$</span>lfp, <span class="at">p =</span> <span class="fl">0.8</span>, <span class="at">list =</span> <span class="cn">FALSE</span>)[,<span class="dv">1</span>]</span>
<span id="cb273-3"><a href="chapter-tree-model.html#cb273-3" tabindex="-1"></a>train_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(x.id)</span>
<span id="cb273-4"><a href="chapter-tree-model.html#cb273-4" tabindex="-1"></a>test_M <span class="ot">&lt;-</span> Mroz <span class="sc">|&gt;</span> <span class="fu">slice</span>(<span class="sc">-</span>x.id)</span></code></pre></div>
<p><code>tuneLength</code>에 의한 모형 적합을 진행해 보자.</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="chapter-tree-model.html#cb274-1" tabindex="-1"></a>ctrl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method=</span><span class="st">&#39;cv&#39;</span>, <span class="at">number=</span><span class="dv">10</span>)</span></code></pre></div>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb275-1"><a href="chapter-tree-model.html#cb275-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb275-2"><a href="chapter-tree-model.html#cb275-2" tabindex="-1"></a>m3_gbm <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M,</span>
<span id="cb275-3"><a href="chapter-tree-model.html#cb275-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>, <span class="at">tuneLength =</span> <span class="dv">10</span>,</span>
<span id="cb275-4"><a href="chapter-tree-model.html#cb275-4" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p><code>m3_gbm</code>에서 최적 tuning parameter 값을 알아보자.</p>
<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="chapter-tree-model.html#cb276-1" tabindex="-1"></a>m3_gbm<span class="sc">$</span>bestTune</span>
<span id="cb276-2"><a href="chapter-tree-model.html#cb276-2" tabindex="-1"></a><span class="do">##    n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb276-3"><a href="chapter-tree-model.html#cb276-3" tabindex="-1"></a><span class="do">## 73     150                 8       0.1             10</span></span></code></pre></div>
<p>네 종류의 tuning parameter 중 <code>shrinkage</code>와 <code>n.minobsinnode</code>는 0.1과 10으로 각각 고정되어 있다.
<code>n.trees</code>와 <code>interaction.depth</code>의 값에 따른 CV RMSE의 변화를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="chapter-tree-model.html#cb277-1" tabindex="-1"></a><span class="fu">ggplot</span>(m3_gbm)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m3-gbm-tune"></span>
<img src="_main_files/figure-html/m3-gbm-tune-1.png" alt="`Mroz` 자료에 대한 boosting 모형의 `n.trees`와 `interaction.depth`의 값에 따른 CV Accuracy의 변화" width="576" />
<p class="caption">
그림 3.19: <code>Mroz</code> 자료에 대한 boosting 모형의 <code>n.trees</code>와 <code>interaction.depth</code>의 값에 따른 CV Accuracy의 변화
</p>
</div>
<p><code>tuneGrid</code>를 이용하여 사용자가 지정한 tuning parameter의 grid를 근거로 모형을 적합해 보자.</p>
<div class="sourceCode" id="cb278"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb278-1"><a href="chapter-tree-model.html#cb278-1" tabindex="-1"></a>grid <span class="ot">&lt;-</span> <span class="fu">expand.grid</span>(<span class="at">n.trees =</span> <span class="fu">seq</span>(<span class="dv">50</span>, <span class="dv">450</span>, <span class="at">by =</span> <span class="dv">50</span>),</span>
<span id="cb278-2"><a href="chapter-tree-model.html#cb278-2" tabindex="-1"></a>                    <span class="at">interaction.depth =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">10</span>,</span>
<span id="cb278-3"><a href="chapter-tree-model.html#cb278-3" tabindex="-1"></a>                    <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.01</span>, <span class="fl">0.1</span>),</span>
<span id="cb278-4"><a href="chapter-tree-model.html#cb278-4" tabindex="-1"></a>                    <span class="at">n.minobsinnode =</span> <span class="fu">c</span>(<span class="dv">5</span>, <span class="dv">10</span>))</span></code></pre></div>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="chapter-tree-model.html#cb279-1" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">123</span>)</span>
<span id="cb279-2"><a href="chapter-tree-model.html#cb279-2" tabindex="-1"></a>m4_gbm <span class="ot">&lt;-</span> <span class="fu">train</span>(lfp <span class="sc">~</span> ., <span class="at">data =</span> train_M,</span>
<span id="cb279-3"><a href="chapter-tree-model.html#cb279-3" tabindex="-1"></a>                <span class="at">method =</span> <span class="st">&#39;gbm&#39;</span>, <span class="at">tuneGrid =</span> grid,</span>
<span id="cb279-4"><a href="chapter-tree-model.html#cb279-4" tabindex="-1"></a>                <span class="at">trControl =</span> ctrl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span></code></pre></div>
<p>Tuning parameter 값에 따른 모형 <code>m4_gbm</code>의 CV Accuracy의 변화를 그래프로 나타내자.</p>
<div class="sourceCode" id="cb280"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb280-1"><a href="chapter-tree-model.html#cb280-1" tabindex="-1"></a><span class="fu">ggplot</span>(m4_gbm)</span></code></pre></div>
<div class="figure"><span style="display:block;" id="fig:m4-gbm-tune"></span>
<img src="_main_files/figure-html/m4-gbm-tune-1.png" alt="`Boston` 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV Accuracy의 변화" width="576" />
<p class="caption">
그림 3.20: <code>Boston</code> 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV Accuracy의 변화
</p>
</div>
<p><code>m4_gbm</code>에서 최적 tuning parameter 값을 알아보자.</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="chapter-tree-model.html#cb281-1" tabindex="-1"></a>m4_gbm<span class="sc">$</span>bestTune</span>
<span id="cb281-2"><a href="chapter-tree-model.html#cb281-2" tabindex="-1"></a><span class="do">##     n.trees interaction.depth shrinkage n.minobsinnode</span></span>
<span id="cb281-3"><a href="chapter-tree-model.html#cb281-3" tabindex="-1"></a><span class="do">## 282     150                 6       0.1             10</span></span></code></pre></div>
<p>Test data에 대한 모형 <code>m3_gbm</code>과 <code>m4_gbm</code>에 의한 예측을 실시하고 분류 성능을 평가해 보자.</p>
<div class="sourceCode" id="cb282"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb282-1"><a href="chapter-tree-model.html#cb282-1" tabindex="-1"></a>pred3_gbm <span class="ot">&lt;-</span> <span class="fu">predict</span>(m3_gbm, test_M)</span>
<span id="cb282-2"><a href="chapter-tree-model.html#cb282-2" tabindex="-1"></a>pred4_gbm <span class="ot">&lt;-</span> <span class="fu">predict</span>(m4_gbm, test_M)</span></code></pre></div>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="chapter-tree-model.html#cb283-1" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred3_gbm, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp, </span>
<span id="cb283-2"><a href="chapter-tree-model.html#cb283-2" tabindex="-1"></a>                <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb283-3"><a href="chapter-tree-model.html#cb283-3" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb283-4"><a href="chapter-tree-model.html#cb283-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb283-5"><a href="chapter-tree-model.html#cb283-5" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb283-6"><a href="chapter-tree-model.html#cb283-6" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb283-7"><a href="chapter-tree-model.html#cb283-7" tabindex="-1"></a><span class="do">##        no  47  28</span></span>
<span id="cb283-8"><a href="chapter-tree-model.html#cb283-8" tabindex="-1"></a><span class="do">##        yes 18  57</span></span>
<span id="cb283-9"><a href="chapter-tree-model.html#cb283-9" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb283-10"><a href="chapter-tree-model.html#cb283-10" tabindex="-1"></a><span class="do">##                Accuracy : 0.6933          </span></span>
<span id="cb283-11"><a href="chapter-tree-model.html#cb283-11" tabindex="-1"></a><span class="do">##                  95% CI : (0.6129, 0.7659)</span></span>
<span id="cb283-12"><a href="chapter-tree-model.html#cb283-12" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667          </span></span>
<span id="cb283-13"><a href="chapter-tree-model.html#cb283-13" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 0.0009839       </span></span>
<span id="cb283-14"><a href="chapter-tree-model.html#cb283-14" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb283-15"><a href="chapter-tree-model.html#cb283-15" tabindex="-1"></a><span class="do">##                   Kappa : 0.3867          </span></span>
<span id="cb283-16"><a href="chapter-tree-model.html#cb283-16" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb283-17"><a href="chapter-tree-model.html#cb283-17" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.1845161       </span></span>
<span id="cb283-18"><a href="chapter-tree-model.html#cb283-18" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb283-19"><a href="chapter-tree-model.html#cb283-19" tabindex="-1"></a><span class="do">##             Sensitivity : 0.6706          </span></span>
<span id="cb283-20"><a href="chapter-tree-model.html#cb283-20" tabindex="-1"></a><span class="do">##             Specificity : 0.7231          </span></span>
<span id="cb283-21"><a href="chapter-tree-model.html#cb283-21" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7600          </span></span>
<span id="cb283-22"><a href="chapter-tree-model.html#cb283-22" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6267          </span></span>
<span id="cb283-23"><a href="chapter-tree-model.html#cb283-23" tabindex="-1"></a><span class="do">##               Precision : 0.7600          </span></span>
<span id="cb283-24"><a href="chapter-tree-model.html#cb283-24" tabindex="-1"></a><span class="do">##                  Recall : 0.6706          </span></span>
<span id="cb283-25"><a href="chapter-tree-model.html#cb283-25" tabindex="-1"></a><span class="do">##                      F1 : 0.7125          </span></span>
<span id="cb283-26"><a href="chapter-tree-model.html#cb283-26" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667          </span></span>
<span id="cb283-27"><a href="chapter-tree-model.html#cb283-27" tabindex="-1"></a><span class="do">##          Detection Rate : 0.3800          </span></span>
<span id="cb283-28"><a href="chapter-tree-model.html#cb283-28" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5000          </span></span>
<span id="cb283-29"><a href="chapter-tree-model.html#cb283-29" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.6968          </span></span>
<span id="cb283-30"><a href="chapter-tree-model.html#cb283-30" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb283-31"><a href="chapter-tree-model.html#cb283-31" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes             </span></span>
<span id="cb283-32"><a href="chapter-tree-model.html#cb283-32" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb284-1"><a href="chapter-tree-model.html#cb284-1" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="at">data =</span> pred4_gbm, <span class="at">reference =</span> test_M<span class="sc">$</span>lfp, </span>
<span id="cb284-2"><a href="chapter-tree-model.html#cb284-2" tabindex="-1"></a>                <span class="at">positive =</span> <span class="st">&quot;yes&quot;</span>, <span class="at">mode =</span> <span class="st">&quot;everything&quot;</span>)</span>
<span id="cb284-3"><a href="chapter-tree-model.html#cb284-3" tabindex="-1"></a><span class="do">## Confusion Matrix and Statistics</span></span>
<span id="cb284-4"><a href="chapter-tree-model.html#cb284-4" tabindex="-1"></a><span class="do">## </span></span>
<span id="cb284-5"><a href="chapter-tree-model.html#cb284-5" tabindex="-1"></a><span class="do">##           Reference</span></span>
<span id="cb284-6"><a href="chapter-tree-model.html#cb284-6" tabindex="-1"></a><span class="do">## Prediction no yes</span></span>
<span id="cb284-7"><a href="chapter-tree-model.html#cb284-7" tabindex="-1"></a><span class="do">##        no  48  23</span></span>
<span id="cb284-8"><a href="chapter-tree-model.html#cb284-8" tabindex="-1"></a><span class="do">##        yes 17  62</span></span>
<span id="cb284-9"><a href="chapter-tree-model.html#cb284-9" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb284-10"><a href="chapter-tree-model.html#cb284-10" tabindex="-1"></a><span class="do">##                Accuracy : 0.7333          </span></span>
<span id="cb284-11"><a href="chapter-tree-model.html#cb284-11" tabindex="-1"></a><span class="do">##                  95% CI : (0.6551, 0.8022)</span></span>
<span id="cb284-12"><a href="chapter-tree-model.html#cb284-12" tabindex="-1"></a><span class="do">##     No Information Rate : 0.5667          </span></span>
<span id="cb284-13"><a href="chapter-tree-model.html#cb284-13" tabindex="-1"></a><span class="do">##     P-Value [Acc &gt; NIR] : 1.777e-05       </span></span>
<span id="cb284-14"><a href="chapter-tree-model.html#cb284-14" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb284-15"><a href="chapter-tree-model.html#cb284-15" tabindex="-1"></a><span class="do">##                   Kappa : 0.4628          </span></span>
<span id="cb284-16"><a href="chapter-tree-model.html#cb284-16" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb284-17"><a href="chapter-tree-model.html#cb284-17" tabindex="-1"></a><span class="do">##  Mcnemar&#39;s Test P-Value : 0.4292          </span></span>
<span id="cb284-18"><a href="chapter-tree-model.html#cb284-18" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb284-19"><a href="chapter-tree-model.html#cb284-19" tabindex="-1"></a><span class="do">##             Sensitivity : 0.7294          </span></span>
<span id="cb284-20"><a href="chapter-tree-model.html#cb284-20" tabindex="-1"></a><span class="do">##             Specificity : 0.7385          </span></span>
<span id="cb284-21"><a href="chapter-tree-model.html#cb284-21" tabindex="-1"></a><span class="do">##          Pos Pred Value : 0.7848          </span></span>
<span id="cb284-22"><a href="chapter-tree-model.html#cb284-22" tabindex="-1"></a><span class="do">##          Neg Pred Value : 0.6761          </span></span>
<span id="cb284-23"><a href="chapter-tree-model.html#cb284-23" tabindex="-1"></a><span class="do">##               Precision : 0.7848          </span></span>
<span id="cb284-24"><a href="chapter-tree-model.html#cb284-24" tabindex="-1"></a><span class="do">##                  Recall : 0.7294          </span></span>
<span id="cb284-25"><a href="chapter-tree-model.html#cb284-25" tabindex="-1"></a><span class="do">##                      F1 : 0.7561          </span></span>
<span id="cb284-26"><a href="chapter-tree-model.html#cb284-26" tabindex="-1"></a><span class="do">##              Prevalence : 0.5667          </span></span>
<span id="cb284-27"><a href="chapter-tree-model.html#cb284-27" tabindex="-1"></a><span class="do">##          Detection Rate : 0.4133          </span></span>
<span id="cb284-28"><a href="chapter-tree-model.html#cb284-28" tabindex="-1"></a><span class="do">##    Detection Prevalence : 0.5267          </span></span>
<span id="cb284-29"><a href="chapter-tree-model.html#cb284-29" tabindex="-1"></a><span class="do">##       Balanced Accuracy : 0.7339          </span></span>
<span id="cb284-30"><a href="chapter-tree-model.html#cb284-30" tabindex="-1"></a><span class="do">##                                           </span></span>
<span id="cb284-31"><a href="chapter-tree-model.html#cb284-31" tabindex="-1"></a><span class="do">##        &#39;Positive&#39; Class : yes             </span></span>
<span id="cb284-32"><a href="chapter-tree-model.html#cb284-32" tabindex="-1"></a><span class="do">## </span></span></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="chapter-Logistic.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="references.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": null,
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["_main.pdf", "_main.epub"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
