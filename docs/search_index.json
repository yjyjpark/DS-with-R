[["index.html", "R에 의한 통계자료분석 머리말", " R에 의한 통계자료분석 박동련 2023-09-09 머리말 최근 다양한 분야에서 데이터의 중요성이 재발견되고 있고, 데이터가 품고 있는 정보를 캐내려는 시도가 빈번하게 이루어지고 있다. 이러한 이유로 인하여 통계분석이 다양한 분야에서 중요한 화두로 떠오르고 있다. Statistical learning은 데이터를 이해하려는 통계적인 접근 방법을 의미하는데, 두 가지 방식으로 구분할 수 있다. 하나는 Supervised learning으로 한 개 이상의 input(설명변수)을 근거로 하나의 output(반응변수) 예측을 위한 모형 수립과 관련된 방법을 의미한다. 다른 하나는 Unsupervised learning으로 input은 있으나, output이 특정되지 않는 상황에서 데이터 사이의 관계 및 구조 수립과 관련된 방법을 의미한다. Supervised learning은 반응변수의 유형에 따라 연속형 반응변수의 경우에 적용되는 regression 모형과 범주형 반응변수의 경우에 적용되는 classification 모형으로 구분할 수 있다. 이 책에서는 supervised statistical learning에서 선형회귀모형과 로지스틱 회귀모형, 그리고 tree-based 모형 중 bagging과 random forest, 그리고 boosting에 대한 기본적인 내용을 살펴보고, R을 사용해서 예제 문제를 함께 풀어보도록 하겠다. 이 책에서는 선형회귀모형 및 로지스틱 회귀모형의 이론적인 내용을 자세하게 다루고 있지 않다. 이론적인 부분을 포함해서 statistical learning에 대한 포괄적인 내용은 An Introduction to Statistical Learning 에서 찾아 볼 수 있으며, 선형회귀모형에 대한 조금 더 자세한 설명은 R과 회귀분석에서 볼 수 있다. 또한 R의 기초적인 사용법 및 패키지 tidyverse에 대한 소개 없이 사용하고 있으며, R code에는 프롬프트(&gt; 또는 +)를 제거하였고, console 창에 출력되는 실행 결과물은 ##으로 시작되도록 하였다. dplyr과 ggplot2 등을 포함한 R 사용법에 대한 소개는 R과 통계분석 에서 볼 수 있다. 이 책을 작성할 때의 R 세션 정보는 다음과 같다. sessionInfo() ## R version 4.3.1 (2023-06-16 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 11 x64 (build 22621) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Korean_Korea.utf8 LC_CTYPE=Korean_Korea.utf8 ## [3] LC_MONETARY=Korean_Korea.utf8 LC_NUMERIC=C ## [5] LC_TIME=Korean_Korea.utf8 ## ## time zone: Asia/Seoul ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## loaded via a namespace (and not attached): ## [1] digest_0.6.33 R6_2.5.1 bookdown_0.35 fastmap_1.1.1 ## [5] xfun_0.39 cachem_1.0.8 knitr_1.43 htmltools_0.5.5 ## [9] rmarkdown_2.24 cli_3.6.1 sass_0.4.7 jquerylib_0.1.4 ## [13] compiler_4.3.1 rstudioapi_0.15.0 tools_4.3.1 evaluate_0.21 ## [17] bslib_0.5.1 yaml_2.3.7 jsonlite_1.8.7 rlang_1.1.1 "],["chapter-regression.html", "1 장 선형회귀모형 1.1 회귀모형 적합 1.2 회귀모형의 추론 1.3 변수선택 1.4 회귀진단 1.5 예측 1.6 실습 예제", " 1 장 선형회귀모형 선형회귀모형은 통계분석 분야에서 가장 중요한 역할을 담당하고 있는 분석 도구 중 하나로서 양적 반응변수의 예측에 유용하게 사용되고 있다. 신빙성 있는 예측이 이루어지기 위해서는 우선 최적의 설명변수들을 선택하여 모형을 적합해야 하며, 모형에 대한 진단 과정 등을 통해 가정 만족 여부를 확인해야 한다. 이런 과정을 거쳐서 선정된 회귀모형만이 예측의 신빙성을 통계적으로 보장받을 수 있는 모형이 된다. 1.1 회귀모형 적합 반응변수는 우리가 파악하고 예측하고자 하는 특정 현상을 나타내는 변수를 의미하며, 일반적으로 \\(Y\\) 로 표시한다. 또한 설명변수는 반응변수가 나타내는 특정 현상에 연관되어 있을 것으로 판단되는 변수를 의미하며, 일반적으로 \\(X_{1}, X_{2}, \\ldots, X_{k}\\) 로 표시한다. 이러한 반응변수와 설명변수들 사이의 확률적 관계를 다음과 같이 하나의 함수 형태로 나타내려는 시도를 회귀분석이라고 할 수 있다. \\[ Y = f(X) + \\varepsilon \\] 함수 \\(f\\) 에 하나의 설명변수만이 포함되면 단순회귀모형, 여러 개의 설명변수가 포함되면 다중회귀모형이라고 한다. 1.1.1 단순선형회귀모형 단순선형회귀모형에서는 반응변수 \\(Y\\) 와 설명변수 \\(X\\) 사이에 다음의 관계가 존재한다고 가정한다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\varepsilon \\tag{1.1} \\end{equation}\\] 회귀계수 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 은 ‘모집단 회귀직선’ 을 정의하고 있는 모수이며, 두 모수의 관계가 ’선형’이기 때문에 식 (1.1)을 선형회귀모형이라고 한다. 오차항 \\(\\varepsilon\\) 은 반응변수의 변동 중 설명변수로 설명이 되지 않은 부분을 나타내는 확률변수이다. 설정된 회귀모형이 두 변수의 관계를 적절하게 설명하고 있다면, 오차항에는 랜덤 요소만이 남게 된다. 일반적으로 오차항 \\(\\varepsilon\\) 은 \\(N(0, \\sigma^{2})\\) 분포를 따른다고 가정한다. \\(\\bullet\\) 회귀계수의 추정 식 (1.1)을 이용해서 \\(Y\\) 의 변동을 설명하고 예측하려면, 값이 알려져 있지 않는 모수 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 을 추정해야 한다. 두 변수 \\(X\\) 와 \\(Y\\) 에 대해 관측된 \\(n\\) 개의 자료를 \\((x_{1},y_{1})\\), \\((x_{2}, y_{2})\\), \\(\\ldots\\), \\((x_{n}, y_{n})\\) 이라고 하자. 두 모수 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 의 추정은 \\(n\\) 개의 자료를 가장 잘 설명할 수 있는 직선을 구하는 과정이라고 할 수 있는데, ’자료를 가장 잘 설명하는 직선’이란 자료와 직선 사이에 간격이 가장 작은 직선을 의미한다. 두 모수의 추정값을 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 라 하면, \\(X\\) 변수의 \\(i\\) 번째 관찰값에 대한 \\(Y\\) 변수의 \\(i\\) 번째 예측값은 \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\\) 가 되며, \\(Y\\) 변수의 관찰값과 예측값의 차이인 잔차 \\(e_{i}=y_{i}-\\hat{y}_{i}\\) 가 자료와 직선 사이의 간격이 된다. 따라서 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 은 잔차의 제곱합(Residual sum of squares; RSS)을 최소화시키도록 구해야 한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left( y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} \\right)^{2} \\tag{1.2} \\end{equation}\\] 식 (1.2)를 최소화시키는 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 은 다음과 같이 유도된다. \\[\\begin{align*} \\hat{\\beta}_{1} &amp;= \\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i=1}^{n}(x_{i}-\\overline{x})^{2}} \\\\ \\hat{\\beta}_{0} &amp;= \\overline{y}-\\hat{\\beta}_{1}\\overline{x} \\end{align*}\\] 추정된 회귀직선의 기울기인 \\(\\hat{\\beta}_{1}\\)의 의미는 설명변수를 한 단위 증가시켰을 때 반응변수의 평균 변화량의 추정값이 되어서, \\(\\hat{\\beta}_{1}&gt;0\\)이면 설명변수 값의 증가가 반응변수 값의 증가로 연결되는 관계를 의미하고, 반면에 \\(\\hat{\\beta}_{1}&lt;0\\)이면 설명변수 값의 증가가 반응변수 값의 감소로 연결되는 관계를 의미한다. 절편인 \\(\\hat{\\beta}_{0}\\) 은 설명변수의 값이 0일 때, 반응변수의 평균값의 추정값이 되는데, 만일 설명변수의 값 범위에 0이 포함되지 않는다면 특별한 의미를 부여하기는 어렵다. 식 (1.1)에 포함된 오차항 \\(\\varepsilon\\) 는 자료를 통해서 관측될 수 없는 변량이다. 하지만 회귀모형에서는 오차항에 대해 몇 가지 가정을 하고 있으며, 이 가정이 만족되지 않는 자료를 대상으로 식 (1.1)의 회귀모형을 적용시켜 분석하게 되면, 그 결과에 심각한 문제가 발생할 수 있다. 따라서 가정 만족 여부를 확인하는 것은 매우 중요한 분석 과정이 되는데, 이 경우 오차항 대신 사용할 수 있는 것이 잔차이다. 잔차분석에 대해서는 1.4절에서 살펴보겠다. \\(\\bullet\\) 예제 : 모집단 회귀직선과 추정된 회귀직선 모집단 회귀직선과 오차항의 분포는 일반적으로 알려져 있지 않지만, 회귀모형의 의미를 살펴보기 위해서 \\(E(Y|X)=5+2X\\) 라고 가정하고, 오차항 \\(\\varepsilon\\)은 평균이 0이고 분산이 1인 표준정규분포라고 가정하자. 설명변수의 값이 1, 3, 2, 7, 12, 6, 1, 3, 6, 6, 7로 주어졌을 때, 표준정규분포에서 발생시킨 오차값을 추가해서 반응변수의 값을 생성해 보자. 단순선형회귀모형을 자료에 적합시켜서 절편과 기울기를 추정하고 모집단 회귀직선과 비교해 보자. 주어진 설명변수의 값에 대해 정규 난수를 추가한 반응변수 값을 생성해서 데이터 프레임으로 만들어 보자. 표준정규분포에서 난수 생성은 함수 rnorm()으로 수행할 수 있다. library(tidyverse) set.seed(12) df1.1 &lt;- tibble(x = c(1, 3, 2, 7, 12, 6, 1, 3, 6, 6, 7), y = 5 + 2*x + rnorm(length(x)) ) 생성된 모의자료는 다음과 같다. df1.1 ## # A tibble: 11 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5.52 ## 2 3 12.6 ## 3 2 8.04 ## 4 7 18.1 ## 5 12 27.0 ## 6 6 16.7 ## 7 1 6.68 ## 8 3 10.4 ## 9 6 16.9 ## 10 6 17.4 ## 11 7 18.2 선형화귀모형의 적합은 함수 lm()으로 할 수 있으며, 일반적인 사용법은 lm(formula, data, ...)이다. formula는 설정된 회귀모형을 나타내는 R 모형 공식으로써, 단순회귀모형의 경우에는 y ~ x와 같이 물결표(~)의 왼쪽에는 반응변수, 오른쪽에는 설명변수를 두면 된다. R 모형 공식에 대해서는 다중회귀모형에서 더 자세하게 살펴보겠다. data는 회귀분석에 사용될 데이터 프레임을 지정하는 것으로써, 이 예제에서는 위에서 생성한 데이터 프레임 df1.1를 지정하면 된다. 모의자료가 있는 데이터 프레임 df1.1를 대상으로 단순회귀모형을 함수 lm()으로 적합시켜보자. 적합 결과는 \\(\\hat{y} = 4.94 + 1.911x\\) 임을 알 수 있다. fit1.1 &lt;- lm(y ~ x, data = df1.1) fit1.1 ## ## Call: ## lm(formula = y ~ x, data = df1.1) ## ## Coefficients: ## (Intercept) x ## 4.940 1.911 함수 lm()으로 생성된 객체 fit1.1을 단순하게 출력시키면 추정된 회귀계수만 나타나지만, 사실 객체 fit1.1은 다음과 같이 많은 양의 정보가 담겨 있는 리스트 객체이다. names(fit1.1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 사용자마다 필요한 정보가 서로 다를 수 있기 때문에 SAS나 SPSS에서와 같이 모든 결과물을 한 번에 출력하는 것은 좋은 방법이 아닐 수 있다. 객체 fit1.1에 담겨 있는 필요한 정보를 획득하기 위해서는 해당하는 함수를 사용해야 하며, 앞으로 차근차근 살펴보겠다. 모집단 회귀직선 및 생성된 모의자료, 그리고 추정된 회귀직선을 함께 나타낸 그래프가 그림 1.1이다. 추정된 회귀직선은 모집단 회귀직선과 매우 비숫하지만 완벽하게 일치하지 않음을 알 수 있다. 그림 1.1: 예제에서 사용된 모의자료와 회귀직선 \\(\\bullet\\) 예제 : 매출액에 대한 광고효과 분석 피자 전문 체인점 영업부서에서는 매출액에 대한 광고효과를 분석하기 위하여 유사한 인구분포를 갖는 20개 판매지역의 매출액 규모와 광고비 지출에 대한 자료를 수집하였다. 수집된 자료는 파일 ex1-2.csv에 입력되었고, 자료의 단위는 100만원이다. 매출액 규모와 광고비 지출 사이의 산점도를 작성하고, 두 변수 사이의 관계를 살펴보자. 최소제곱법에 의한 단순회귀직선을 추정하고, 추정된 \\(\\hat{\\beta}_{1}\\)의 의미를 해석해 보자. 자료가 콤마로 구분된 CSV 파일을 함수 readr::read_csv()로 불러와서 두 변수의 산점도를 작성해 보자. df1.2 &lt;- readr::read_csv(&quot;Data/ex1-2.csv&quot;) df1.2 ## # A tibble: 20 × 2 ## sales advertisement ## &lt;dbl&gt; &lt;dbl&gt; ## 1 425 23 ## 2 370 21 ## 3 200 16 ## 4 580 34 ## 5 620 32 ## 6 650 36 ## 7 700 40 ## 8 490 37 ## 9 610 35 ## 10 290 20 ## 11 320 20 ## 12 350 21 ## 13 400 23 ## 14 517 21 ## 15 545 30 ## 16 590 32 ## 17 711 39 ## 18 650 37 ## 19 740 41 ## 20 660 38 ggplot(df1.2, aes(x = advertisement, y = sales)) + geom_point(size = 2) + geom_smooth(se = FALSE, aes(color = &quot;비모수적 회귀곡선&quot;)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(color = &quot;회귀직선&quot;)) + labs(color = NULL) 그림 1.2: 예제 자료의 산점도와 비모수적 회귀곡선 비모수적 회귀곡선은 두 변수의 관계를 가장 잘 나타내는 곡선을 추정하는 기능을 가지고 있다. 그림 1.2에 작성된 비모수적 회귀곡선은 대체로 직선의 형태를 취하고 있음을 알 수 있으며, 함께 표시된 회귀직선과 큰 차이가 없는 것을 알 수 있다. 이것으로 두 변수의 관계를 직선으로 설정하는 데에 큰 무리가 없음을 알 수 있다. 함수 lm()으로 단순회귀직선을 추정해 보자. fit1.2 &lt;- lm(sales ~ advertisement, data = df1.2) fit1.2 ## ## Call: ## lm(formula = sales ~ advertisement, data = df1.2) ## ## Coefficients: ## (Intercept) advertisement ## -6.944 17.713 적합 결과는 \\(\\hat{y} = -6.994 + 17.713x\\) 임을 알 수 있다. 추정된 직선의 기울기 \\(\\hat{\\beta}_{1}=17.713\\) 의 의미는 광고비 지출을 한 단위인 100만원 증가시키면 평균 매출액의 규모가 17.713백만원 증가한다는 것이 된다. 절편 \\(\\hat{\\beta}_{0}=-6.994\\) 의 의미는 광고비 지출이 \\(0\\) 일 때 평균 매출액은 -6.994백만원이라는 것이 된다. 즉, 광고를 하지 않으면 적자를 본다는 것인데, 이러한 해석은 조사된 광고비 자료의 범위에 \\(0\\) 이 포함되어 있지 않기 때문에 적절하지 않다고 하겠다. 1.1.2 다중선형회귀모형 반응변수의 변동을 하나의 설명변수만으로 충분하게 설명하는 것은 대부분의 경우 불가능할 것이다. 따라서 반응변수와 관련이 있을 것으로 보이는 여러 개의 설명변수를 모형에 포함시키는 다중회귀모형이 실제 상황에서 많이 사용되는 모형이 된다. 기본 가정 및 추론 방법에서는 단순회귀모형과 큰 차이가 있는 것은 아니지만, 모형에 여러 개의 설명변수가 포함되기 때문에 단순회귀모형에서는 없었던 문제들이 발생할 수 있다. 다중선형회귀모형에서는 반응변수 \\(Y\\) 와 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 사이에 다음의 관계가 존재한다고 가정한다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} + \\varepsilon \\tag{1.3} \\end{equation}\\] 모수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 는 \\((k+1)\\) 차원의 공간에서 정의되는 회귀평면을 구성하고 있으며, 오차항 \\(\\varepsilon\\) 은 단순회귀모형에서와 동일한 의미를 갖고 있는 확률변수이다. \\(\\bullet\\) 회귀계수의 추정 반응변수 \\(Y\\) 와 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 에 대해 관측된 \\(n\\) 개의 자료는 식 (1.3)에 의해 다음과 같은 관계가 있다. \\[\\begin{align*} Y_{1} &amp;= \\beta_{0} + \\beta_{1}X_{11} + \\cdots + \\beta_{k}X_{k1} + \\varepsilon_{1} \\\\ Y_{2} &amp;= \\beta_{0} + \\beta_{1}X_{12} + \\cdots + \\beta_{k}X_{k2} + \\varepsilon_{2} \\\\ \\vdots \\\\ Y_{n} &amp;= \\beta_{0} + \\beta_{1}X_{1n} + \\cdots + \\beta_{k}X_{kn} + \\varepsilon_{n} \\end{align*}\\] 관측된 자료의 관계를 다음과 같이 행렬 형태로 정리할 수 있다. \\[ \\begin{pmatrix} Y_{1} \\\\ Y_{2} \\\\ \\vdots \\\\ Y_{n} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; X_{11} &amp; \\cdots &amp; X_{k1} \\\\ 1 &amp; X_{12} &amp; \\cdots &amp; X_{k2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{1n} &amp; \\cdots &amp; X_{kn} \\end{pmatrix} \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k} \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{pmatrix} \\] 또는 다음의 행렬로 표현할 수 있다. \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\end{equation}\\] \\(\\mathbf{Y}\\) 는 반응변수의 관찰값 벡터이고, \\(\\mathbf{X}\\) 는 설명변수의 관찰값 행렬로써 design matrix라고 하며, \\(\\boldsymbol{\\beta}\\) 는 모수 벡터, \\(\\boldsymbol{\\varepsilon}\\) 은 오차항 벡터이다. 회귀계수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 의 추정은 단순회귀모형의 경우와 동일한 방식으로 이루어진다. 즉, 다음의 RSS를 최소화하는 \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{k})\\) 를 선택한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left(y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1i} - \\ldots - \\hat{\\beta}_{k}x_{ki} \\right)^{2} \\tag{1.4} \\end{equation}\\] 식 (1.4)를 최소화시키는 \\(\\hat{\\boldsymbol{\\beta}}\\) 은 최소추정량이라고 불리며, 다음과 같이 주어진다. \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\mathbf{X}^{T} \\mathbf{Y} \\tag{1.5} \\end{equation}\\] \\(\\bullet\\) 예제 : 다중회귀모형의 회귀계수 추정 데이터 프레임 mtcars는 1974년 \\(\\textit{Motor Trend US}\\) 에 실린 32 종류 자동차 모델의 연비와 관련된 자료가 입력되어 있다. str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 연비를 나타내는 mpg를 반응변수로 하고, 설명변수에는 차량의 무게를 나타내는 wt와 0.25 마일까지 도달하는 데 걸리는 시간을 나타내는 qsec을 선택해서 다중회귀모형을 설정하고 회귀계수를 추정해 보자. 단순회귀모형의 경우와 동일하게 다중회귀모형에서도 함수 lm()을 사용해서 모형적합을 실시할 수 있다. 하지만 단순회귀모형의 경우와 다르게 설명변수의 개수가 2개 이상이 되며, 복잡한 형태의 모형이 설정되는 경우도 많이 있기 때문에 다중회귀모형의 경우에는 함수 lm()에 지정되는 R 모형공식에 반응변수와 설명변수를 구분하는 ~ 기호 외에도 많은 기호가 사용된다. R 모형공식에서 사용되는 기호 기호 사용법 물결표 (~) 반응변수와 설명변수의 구분. 물결표의 왼쪽에는 반응변수, 오른쪽에는 설명변수를 둔다. 플러스 (+) 모형에 포함된 설명변수의 구분. 반응변수 y와 설명변수 x1, x2, x3의 회귀모형은 y ~ x1 + x2 + x3로 표현된다. 콜론 (:) 설명변수 사이의 상호작용 표현. 반응변수 y와 설명변수 x1, x2 그리고 x1과 x2의 상호작용이 포함된 모형은 y ~ x1 + x2 + x1:x2로 표현된다. 별표 (*) 주효과와 상호작용 효과를 포함한 모든 효과 표현. y ~ x1 * x2 * x3는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3를 의미한다. 윗꺽쇠 (^) 지정된 차수까지의 상호작용 표현. y ~ (x1 + x2 + x3)^2는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3을 의미한다. 점 (.) 반응변수를 제외한 데이터 프레임에 있는 모든 변수. 만일 데이터 프레임에 y, x1, x2, x3가 있다면 y ~ . 은 y ~ x1 + x2 + x3을 의미한다. 마이너스(-) 회귀모형에서 제외되는 변수. y ~ (x1 + x2 + x3)^2 – x2:x3는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3을 의미한다. - 1, + 0 절편 제거. y ~ x – 1 혹은 y ~ x + 0은 원점을 지나는 회귀모형을 의미한다. I() 괄호 안의 연산자를 수학 연산자로 인식. y ~ x1 + I(x2+x3)는 \\(y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}(x_{2}+x_{3})+\\varepsilon\\) 모형을 의미한다. 이제 mpg를 반응변수로, wt와 qsec을 설명변수로 하는 다중회귀모형을 적합해 보자. fit1.3 &lt;- lm(mpg ~ wt + qsec, data = mtcars) fit1.3 ## ## Call: ## lm(formula = mpg ~ wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) wt qsec ## 19.7462 -5.0480 0.9292 적합된 모형식은 \\(\\widehat{\\mbox{mpg}} = 19.74 - 5.047\\mbox{ wt} + 0.929\\mbox{ qsec}\\) 임을 알 수 있다. 추정된 회귀계수에 대한 해석은 단순회귀모형의 경우와 비슷하지만 제한 사항이 추가된다. 단순회귀모형에서는 하나의 설명변수만 있기 때문에 추정된 모형은 직선으로 표현되고, 따라서 해당 설명변수가 한 단위 증가했을 때 반응변수의 평균 변화량으로 회귀계수를 해석할 수 있었다. 하지만 다중회귀모형에서는 추정된 회귀모형이 직선이 아닌 2차원 이상에서 정의되는 평면이 되기 때문에 조금 더 복잡한 상황이 되는데, 그것은 어느 한 설명변수의 효과를 측정하기 위해서는 회귀평면을 구성하고 있는 다른 설명변수의 값이 고정되어야 하기 때문이다. 그림 1.3에서는 mpg를 반응변수로, wt와 qsec을 설명변수로 하는 다중회귀모형으로 적합된 회귀평면이 작성되어 있다. 변수 wt의 추정된 회귀계수 \\(-5.047\\) 은 qsec을 일정한 수준으로 고정한 상태에서 wt가 한 단위 증가했을 때 mpg의 평균 변화량을 나타내는 것인데, 그림에서 회귀평면이 wt 축의 양의 방향으로 급격하게 기우는 모습을 볼 수 있다. 또한 변수 qsec에 대한 추정된 회귀계수 \\(0.929\\) 은 wt를 일정한 수준으로 고정한 상태에서 qsec이 한 단위 증가했을 때 mpg의 평균 변화량을 나타내는 것이며, 그림에서 회귀평면이 qsec의 양의 방향으로 완만하게 증가하는 모습을 볼 수 있다. 그림 1.3: 설명변수가 2개인 다중회귀모형에서 추정된 회귀평면 \\(\\bullet\\) 예제: 행렬 state.x77 행렬 state.x77은 미국 50개 주와 관련된 8개 변수로 구성되었다. 변수 Life Exp와 HS Grad는 이름 중간에 빈칸이 있으며, 각 주의 이름이 행렬의 row name으로 입력되어 있음을 알 수 있다. state.x77[1:5,] ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 변수 Murder를 반응변수로, 나머지 7개 변수를 설명변수로 하는 다중회귀모형을 설정해 보자. 회귀분석을 원활하게 수행하기 위해서는 자료가 행렬이 아닌 데이터 프레임의 형태로 입력되어 있어야 한다. 행렬 state.x77을 데이터 프레임으로 변환하면서, 빈칸이 있는 변수 이름을 수정하고 반응변수를 마지막 변수로 이동시키자. states &lt;- as_tibble(state.x77) |&gt; rename(Life_Exp = `Life Exp`, HS_Grad = `HS Grad`) |&gt; relocate(Murder, .after = last_col()) |&gt; print(n = 3) ## # A tibble: 50 × 8 ## Population Income Illiteracy Life_Exp HS_Grad Frost Area Murder ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 3615 3624 2.1 69.0 41.3 20 50708 15.1 ## 2 365 6315 1.5 69.3 66.7 152 566432 11.3 ## 3 2212 4530 1.8 70.6 58.1 15 113417 7.8 ## # ℹ 47 more rows 행렬을 tibble로 전환시켰기 때문에 각 주의 이름이 입력된 row name이 삭제되었는데, 만일 row name를 그대로 유지하고자 한다면 함수 as_tibble() 대신 as.data.frame()을 사용하면 된다. 반응변수의 위치를 마지막으로 이동시킨 이유는 산점도 행렬을 작성하고 결과를 해석할 때 더 편리하기 때문이다. 다중회귀모형에 의한 분석을 진행하기 전에 반드시 거쳐야 할 단계가 있는데, 그것은 모형에 포함될 변수들의 관계를 탐색하는 것이다. 변수들 사이의 관계 탐색에 많이 사용되는 방법은 상관계수와 산점도행렬이다. 산점도행렬과 같은 그래프에 의한 탐색은 필수적이라 할 수 있으며, 연속형 변수의 경우에는 두 변수끼리 짝을 지어 상관계수를 살펴보는 것도 필요하다. 상관계수는 변수들 사이의 선형관계 정도를 확인할 때 사용할 수 있는데, 함수 cor()로 계산할 수 있다. 사용법은 cor(x, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\"))이다. x와 y 모두 벡터, 행렬 혹은 데이터 프레임이 가능한데, x만 주어지면 x에 포함된 모든 변수들 사이의 상관계수를 계산하게 되고, y도 주어지면 x에 속한 변수와 y에 속한 변수들을 하나씩 짝을 지어 상관계수를 계산하게 된다. use는 결측값의 처리방식에 대한 것으로 선택할 수 있는 것은 디폴트인 \"everything\"과 \"all.obs\", \"complete.obs\", \"pairwise.complete.obs\"이 있으며, 해당 문자의 약칭으로도 사용이 가능하다. 결측값이 존재하면 use = \"everything\"인 경우에는 NA가 계산결과로 출력되고, use = \"all\"인 경우에는 오류가 발생한다. 또한 use = \"complete\"의 경우에는 NA가 있는 행은 모두 제거된 상태에서 상관계수가 계산되고, use = \"pairwise\"의 경우에는 상관계수가 계산되는 변수들만을 대상으로 NA가 있는 행을 제거하고 상관계수를 계산한다. method는 계산하는 상관계수의 종류를 선택한다. 디폴트인 \"pearson\"은 Pearson의 상관계수를 지정하는 것으로 두 변수 사이의 선형관계 정도를 표현하는 가장 일반적으로 많이 사용되는 상관계수를 계산한다. 두 번째 방법인 \"kendall\"은 Kendall의 순위상관계수 혹은 Kendall의 \\(\\tau\\) 를 지정하는 것으로서 concordant pair와 discordant pair를 이용하여 정의되는 비모수 상관계수를 계산한다. 순서형 자료에 주로 적용되는 방법이다. 세 번째 방법인 \"spearman\"은 Spearman의 순위상관계수 혹은 Spearman의 \\(\\rho\\) 를 지정하는 것으로서 두 변수 사이의 관계가 단조증가 혹은 단조감소 함수로 얼마나 잘 설명될 수 있는지를 표현하는 비모수 상관계수를 계산한다. 정규성 가정이 어긋나는 경우에 사용할 수 있다. 데이터 프레임 states를 구성하고 있는 변수들의 상관계수를 구해보자. cor(states) ## Population Income Illiteracy Life_Exp HS_Grad ## Population 1.00000000 0.2082276 0.10762237 -0.06805195 -0.09848975 ## Income 0.20822756 1.0000000 -0.43707519 0.34025534 0.61993232 ## Illiteracy 0.10762237 -0.4370752 1.00000000 -0.58847793 -0.65718861 ## Life_Exp -0.06805195 0.3402553 -0.58847793 1.00000000 0.58221620 ## HS_Grad -0.09848975 0.6199323 -0.65718861 0.58221620 1.00000000 ## Frost -0.33215245 0.2262822 -0.67194697 0.26206801 0.36677970 ## Area 0.02254384 0.3633154 0.07726113 -0.10733194 0.33354187 ## Murder 0.34364275 -0.2300776 0.70297520 -0.78084575 -0.48797102 ## Frost Area Murder ## Population -0.3321525 0.02254384 0.3436428 ## Income 0.2262822 0.36331544 -0.2300776 ## Illiteracy -0.6719470 0.07726113 0.7029752 ## Life_Exp 0.2620680 -0.10733194 -0.7808458 ## HS_Grad 0.3667797 0.33354187 -0.4879710 ## Frost 1.0000000 0.05922910 -0.5388834 ## Area 0.0592291 1.00000000 0.2283902 ## Murder -0.5388834 0.22839021 1.0000000 함수 cor()에 데이터 프레임을 입력하면 모든 변수들 사이의 상관계수가 행렬 형태로 계산되어 출력된다. 상관계수행렬은 변수의 개수가 많아지면 변수 사이의 관계 파악이 어려워지는 문제가 있다. 이런 경우에는 그래프로 표현하는 것이 더 효과적인 방법이 될 것이다. 패키지 GGally의 함수 ggcorr()은 상관계수행렬을 그래프로 표현할 때 많이 사용되는 함수이다. 사용법은 ggcorr(data, method = c(\"pairwise\", \"pearson\"), label = FALSE, label_round = 1, ...)이다. method는 결측값 처리 방식과 계산되는 상관계수의 종류를 차례로 지정하는데, 디폴트는 \"pairwise\"와 \"pearson\"이다. label은 그래프에 상관계수를 표시할 것인지 여부를 결정하는 것이고, label_round는 표시되는 상관계수의 반올림 자릿수를 지정한다. 데이터 프레임 states를 구성하고 하는 변수들의 상관계수를 그래프로 나타내 보자. 숫자로만 구성되어 있는 상관계수행렬보다 훨씬 간편하게 변수 사이의 상관관계를 파악할 수 있다. library(GGally) ggcorr(states, label = TRUE, label_round = 2) 그림 1.4: 함수 ggcorr()에 의한 상관계수 그래프 반응변수를 마지막 변수로 이동시켰기 때문에 반응변수와 다른 설명변수 사이의 상관계수를 편하게 확인할 수 있다. 반응변수 Murder가 설명변수 Illiteracy와는 비교적 높은 양의 상관관계를, Life_Exp와는 비교적 높은 음의 상관관계를 보이고 있다. 상관계수는 두 변수 사이의 선형관계 정도만을 측정하는 측도이다. 변수 사이에 존재하는 ’있는 그대로’의 관계를 확인하는 가장 좋은 방법은 산점도행렬이다. 함수 GGally::ggpairs()로 작성해 보자. ggpairs(states, lower = list(continuous = &quot;smooth&quot;)) 그림 1.5: 함수 ggpairs()에 의한 산점도행렬 반응변수를 마지막 변수로 위치를 이동시켰기 때문에 산점도행렬의 마지막 행을 이루고 있는 모든 패널에서 Murder가 Y축 변수로 배치되고, 설명변수들이 각각 X축 변수로 배치되었다. 이제 states 자료에 대한 회귀모형을 함수 lm()으로 적합해 보자. fit1.4 &lt;- lm(Murder ~ ., data = states) fit1.4 ## ## Call: ## lm(formula = Murder ~ ., data = states) ## ## Coefficients: ## (Intercept) Population Income Illiteracy Life_Exp HS_Grad ## 1.222e+02 1.880e-04 -1.592e-04 1.373e+00 -1.655e+00 3.234e-02 ## Frost Area ## -1.288e-02 5.967e-06 함수 lm()으로 생성된 객체 fit1.4을 단순하게 출력시키면 추정된 회귀계수만 나타나지만, 사실 객체 fit1.4은 다음과 같이 많은 양의 정보가 담겨 있는 리스트이다. names(fit1.4) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 사용자마다 필요한 정보가 서로 다를 수 있기 때문에 SAS나 SPSS에서와 같이 모든 결과물을 한 번에 출력하는 것은 좋은 방법이 아닐 수 있다. 객체 fit1.4에 담겨 있는 다양한 정보를 획득하기 위해서는 몇 가지 함수를 사용해야 하는데, 그 함수의 목록이 아래 표에 정리되어 있다. 표에 정리되어 있는 함수를 이용하여 함수 lm()으로 생성된 객체에서 필요한 결과를 확인하는 방법 및 해석에 대해서는 1.2절에서 살펴보겠다. 함수 lm() 으로 생성된 객체에서 필요한 결과를 얻기 위해 유용하게 사용되는 함수 함수 산출 결과 anova() 추정된 회귀모형의 분산분석표 혹은 두 개 이상의 추정된 모형을 비교하기 위한 분산분석표 coefficients() 추정된 회귀계수. coef()도 가능. confint() 회귀계수의 신뢰구간. 95% 신뢰구간이 디폴트. deviance() 잔차제곱합(residual sum of squares; RSS), \\(\\sum (y_{i}-\\hat{y}_{i})^{2}\\) fitted() 반응변수의 적합값, \\(\\hat{y}_{i}\\) residuals() 회귀모형의 잔차, \\(e_{i}=y_{i}-\\hat{y}_{i}\\) . resid()도 가능. summary() 회귀모형의 다양한 적합 결과 1.1.3 다항회귀모형 식 (1.3)에 정의된 다중회귀모형에서는 반응변수와 설명변수 사이의 관계가 선형이라고 가정하고 있다. 하지만 두 변수의 관계가 명확한 2차 함수 관계가 있을 경우에는 설명변수의 제곱항을 모형에 추가하는 다항회귀모형이 더 적절한 대안이 될 수 있다. 단순회귀모형에 대한 \\(p\\) 차 다항회귀모형은 다음과 같이 설정된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\cdots + \\beta_{p}X^{p} + \\varepsilon \\tag{1.6} \\end{equation}\\] 차수 \\(p\\) 는 가능한 낮게 잡는 것이 좋은데, 그것은 너무 높은 차수의 항을 모형에 포함시키면 \\(\\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1}\\) 가 불안정해질 수 있고, 따라서 회귀계수 추정에 문제가 생길 수 있기 때문이다. 이 문제는 ’다중공선성’을 소개할 때 다시 살펴보겠다. 일단 차수가 선택되면, 선택된 차수 이하의 모든 차수는 반드시 모형에 포함되어야 한다. 예를 들어 \\(Y=\\beta_{0}+\\beta_{1}X+\\beta_{2}X^{2}+\\varepsilon\\) 모형에서 어떠한 이유에서 \\(X\\) 의 1차항을 제거한다면, 모형은 \\(Y=\\beta_{0}+\\beta_{2}X^{2}+\\varepsilon\\) 가 되는데, 이 모형은 Y축을 중심으로 좌우대칭을 이루어야 하는 지나치게 강한 가정을 갖게 된다. 또한 만일 변수 \\(X\\) 의 값을 \\(a\\) 만큼 평행 이동시켜야 한다면, \\(X\\) 가 \\(X+a\\) 로 변경되어 다음과 같이 모형에 다시 1차항이 나타나게 된다. \\[\\begin{align*} Y &amp;= \\beta_{0} + \\beta_{2}(X+a)^{2} + \\varepsilon \\\\ &amp;= \\beta_{0} + \\beta_{2}(X^{2} + 2aX + a^{2}) + \\varepsilon \\end{align*}\\] 설명변수가 2개인 다중회귀모형에서 2차 다항회귀모형은 다음과 같은 형태가 될 수 있다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} + \\varepsilon \\tag{1.7} \\end{equation}\\] \\(\\bullet\\) 예제 : women 데이터 프레임 women에는 미국 30대 여성의 키(height)와 몸무게(weight)가 입력되어 있다. 두 변수 height와 weight의 회귀모형을 설정해 보자. 먼저 두 변수의 관계가 선형인지 여부를 그래프로 확인해 보자. ggplot(women, aes(x = height, y = weight)) + geom_point(size = 3) + geom_smooth(aes(color = &quot;loess&quot;), se = FALSE) + geom_smooth(aes(color = &quot;linear&quot;), method=&quot;lm&quot;, se = FALSE) + labs(color = &quot;&quot;) 그림 1.6: 데이터 프레임 women의 변수 height와 weight의 산점도와 선형회귀직선 및 국소회귀곡선 두 변수의 관계가 선형보다는 2차 곡선이 더 적합한 것으로 보인다. 다항회귀모형은 함수 poly()를 이용하거나 또는 함수 I()를 이용해서 적합할 수 있다. 함수 poly()는 함수 lm()과 함께 lm(y ~ poly(x, degree = 1, raw = FALSE), data)와 같이 사용할 수 있다. degree는 다항회귀모형의 차수를 지정하는 것으로 디폴트 값은 1차이고, raw는 직교다항회귀(orthogonal polynomial regression)의 사용 여부를 선택하는 것으로서 디폴트 값인 FALSE는 직교다항회귀에 의한 적합이 된다. 따라서 일반적인 다항회귀모형을 사용하고자 한다면 반드시 raw에 TRUE를 지정해야 한다. 차수가 높지 않은 일반적인 다항회귀모형을 적합하는 경우에는 함수 I()를 사용하는 것이 더 간편할 수 있다. 함수 lm() 안에서 lm(y ~ x + I(x^2), data)와 같이 입력하면 2차 다항회귀모형을 적합하게 된다. 반응변수 weight에 대한 설명변수 height의 2차 다항회귀모형을 적합해 보자. fit_w &lt;- lm(weight ~ height + I(height^2), women) fit_w ## ## Call: ## lm(formula = weight ~ height + I(height^2), data = women) ## ## Coefficients: ## (Intercept) height I(height^2) ## 261.87818 -7.34832 0.08306 적합된 회귀모형식은 \\(\\hat{y}_{i}=261.87 - 7.348x_{i} + 0.08x_{i}^{2}\\) 임을 알 수 있다. 1.1.4 가변수 회귀모형 선형회귀모형에서 반응변수는 유형이 반드시 연속형이어야 하며, 정규분포의 가정이 필요하다. 반면에 설명변수는 연속형 변수와 범주형 변수가 모두 가능한데, 연속형인 경우에는 정규분포의 가정은 필요 없으나 가능한 좌우대칭에 가까운 분포를 하는 것이 좋다. 범주형 변수 중 순서형 변수인 경우에는 연속형 변수처럼 변수의 값을 그대로 사용하는 것이 가능하지만, 명목형 변수의 경우에는 변수의 값을 그대로 사용하면 절대로 안 되고, 반드시 가변수(dummy variable)를 대신 사용해야 한다. 가변수를 설정하는 방식은 몇 가지가 있는데, 그 중 회귀계수 \\(\\beta_{0}\\) 를 유지하는 방식을 살펴보자. 이 경우, 가변수는 0 또는 1의 값을 갖게 되며, 범주의 개수보다 하나 작은 개수의 가변수를 사용하게 된다. 예를 들어, “Yes”, “No”와 같은 2개의 범주를 갖는 범주형 변수와 연속형 변수 \\(X\\) 를 설명변수로 하는 회귀모형은 다음과 같이 하나의 가변수를 갖게 된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\varepsilon \\tag{1.8} \\end{equation}\\] 가변수 \\(D\\) 가 “Yes” 범주이면 1, “No” 범주이면 0을 값으로 갖는다면, 모집단 회귀직선은 다음과 같이 주어진다. \\[ E(Y) = \\begin{cases} \\beta_{0} + \\beta_{1}X &amp; \\quad \\text{if } D = 0 \\\\ \\beta_{0} + \\beta_{2} + \\beta_{1}X &amp; \\quad \\text{if } D = 1 \\end{cases} \\] \\(D=0\\) 이 되는 범주를 ’기준범주’라고 하는데, 절편 \\(\\beta_{0}\\) 가 기준범주의 효과를 나타내고 있고, 가변수의 회귀계수 \\(\\beta_{2}\\) 는 기준범주와 “Yes” 범주의 효과 차이를 나타내고 있다. 식 (1.8)의 회귀모형에서는 반응변수 \\(Y\\) 와 설병변수 \\(X\\) 가 “Yes”와 “No” 범주에서 동일한 관계를 갖고 있다고 가정하고 있는데, 만일 각 범주에서 두 변수의 관계가 다를 수 있다면 기울기도 다르게 설정할 필요가 있다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\beta_{3}DX + \\varepsilon \\end{equation}\\] 범주형 변수가 3개의 범주를 갖는 경우에는 2개의 가변수가 필요하게 된다. 예를 들어 “high”, “medium”, “low”의 3가지 범주를 갖는 범주형 변수와 연속형 변수 \\(X\\) 를 설명변수로 하는 회귀모형은 다음과 같은 형태를 갖게 된다. 반응변수 \\(Y\\) 와 설병변수 \\(X\\) 가 세 범주에 동일한 관계를 갖고 있다고 가정하자. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D_{1} + \\beta_{3}D_{2} + \\varepsilon \\end{equation}\\] 단, \\(D_{1}\\) 은 범주가 “high”이면 1, 아니면 0이고, \\(D_{2}\\) 는 범주가 “medium”이면 1, 아니면 0이 되는 가변수이다. 이 경우, 기준범주는 “low”가 되며, \\(\\beta_{2}\\) 는 기준범주와 “high” 범주의 효과 차이를, \\(\\beta_{3}\\) 은 기준범주와 “medium” 범주의 효과 차이를 나타내고 있다. \\(\\bullet\\) 예제 : carData::Leinhardt carData::Leinhardt는 1970년대 105개 나라의 신생아 사망률, 소득, 지역 및 원유 수출 여부 자료가 입력되어 있다. 반응변수를 신생아 시망률(infant)로 하고, 나머지 변수인 소득(income), 원유 수출 여부(oil)과 지역(region)을 설명변수로 설정하자. 변수 oil은 'no'와 'yes'의 2개 범주를 갖고 있고, 변수 region은 'Africa', 'America', 'Asia', 'Europe'의 4개 범주를 갖고 있다. data(Leinhardt, package = &quot;carData&quot;) Leinhardt |&gt; head() ## income infant region oil ## Australia 3426 26.7 Asia no ## Austria 3350 23.7 Europe no ## Belgium 3346 17.0 Europe no ## Canada 4751 16.8 Americas no ## Denmark 5029 13.5 Europe no ## Finland 3312 10.1 Europe no 산점도행렬을 작성해 보자. 변수 income과 infant의 분포가 오른쪽 꼬리가 매우 긴 형태를 갖고 있으며, oil이 'yes'인 자료가 매우 드물다는 것을 알 수 있다. library(GGally) ggpairs(Leinhardt) 그림 1.7: Leinhardt 변수의 산점도 행렬 반응변수의 경우에 정규분포 가정이 있지만, 설명변수의 분포 형태에 대해서는 특별한 가정은 없다. 하지만 꼬리가 지나치게 긴 분포의 경우에는 대략 좌우대칭이 될 수 있도록 변환을 시키는 것이 좋다. 변수 income과 infant를 로그변환시키고 다시 산점도행렬을 작성해 보자. Leinhardt_ln &lt;- Leinhardt |&gt; mutate(ln_income = log(income), ln_infant = log(infant), .before = 1) |&gt; select(-income, -infant) Leinhardt_ln |&gt; ggpairs() 그림 1.8: Leinhardt_ln 변수의 산점도 행렬 변수 ln_income과 oil만을 설명변수로 하여 회귀모형을 적합해 보자. 함수 lm()은 요인을 설명변수로 입력하면 자동으로 필요한 개수의 가변수를 생성한다. lm(ln_infant ~ ln_income + oil, Leinhardt_ln) ## ## Call: ## lm(formula = ln_infant ~ ln_income + oil, data = Leinhardt_ln) ## ## Coefficients: ## (Intercept) ln_income oilyes ## 7.1396 -0.5211 0.7900 변수 oil의 수준은 'no'와 'yes'의 두 가지이지만 모형에 포함된 가변수는 한 개이다. 이것은 절편이 모형에 포함된 상태에서 요인의 수준 개수만큼 가변수를 포함시키면 회귀계수 전체를 추정할 수 없는 문제가 발생하기 때문이다. 생략된 가변수는 알파벳 순서에서 첫 번째 범주인 'no'에 대한 것이며, 이 범주가 기준 범주가 된다. 범주 'yes'에 대한 가변수의 회귀계수는 기준 범주와의 효과 차이를 나타낸 것이며, 양의 값으로 추정되었으므로 'yes' 범주의 신생아 사망률이 더 높다고 해석할 수 있다. 하지만 이러한 해석은 산유국의 신생아 사망률이 더 높다는 의미가 되는 것이기 때문에 정확한 이유를 확인할 필요가 있다고 보인다. 이 문제는 1.2절에서 예제로 다시 살펴보겠다. 설명변수를 모두 포함한 회귀모형을 적합해 보자. lm(ln_infant ~ ln_income + oil + region, Leinhardt_ln) ## ## Call: ## lm(formula = ln_infant ~ ln_income + oil + region, data = Leinhardt_ln) ## ## Coefficients: ## (Intercept) ln_income oilyes regionAmericas regionAsia ## 6.5521 -0.3398 0.6402 -0.5498 -0.7129 ## regionEurope ## -1.0338 변수 region의 기준범주는 'Africa'가 이며, region에 대한 모든 가변수의 회귀계수가 음수로 추정되었다는 것은 다른 모든 지역이 'Africa' 지역보다 신생아 사망률이 더 낮다는 것을 의미한다. 1.2 회귀모형의 추론 1.1절에서 우리는 다항회귀모형과 가변수 회귀모형 등 몇 가지 형태의 다중회귀모형을 살펴보았고, 최소제곱추정량에 의한 회귀계수의 추정 방법도 살펴보았다. 지금부터는 회귀계수의 추론에 대해 살펴보도록 하자. 반응변수와 설명변수에 대해 자료가 관측되면, 회귀계수에 대한 추정 뿐 아니라 다양한 추론도 진행할 수 있는데, 예를 들면, 회귀모형에 대한 적절한 해석을 하기 위한 가설 검정 및 개별 회귀계수의 신뢰구간 추정 등이 있다. 이러한 추론을 진행하기 위해서는 회귀계수의 추정량에 대한 표본분포를 알아야 한다. 식 (1.5)에 정의된 최소제곱추정량은 오차항 \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) 이 서로 독립이고 모두 \\(N(0, \\sigma^{2})\\) 의 분포를 한다는 가정을 한다면 다음의 분포를 따른다. \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} \\sim N \\left( \\boldsymbol{\\beta}, \\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1} \\sigma^{2} \\right) \\end{equation}\\] 최소제곱추정량을 사용한 회귀모형에 대한 추론의 정당성은 Gauss-Markov 정리에서 보장되는데, Gauss-Markov 정리란 오차항 \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\) 이 서로 독립이고 평균이 \\(0\\) , 분산이 \\(\\sigma^{2}\\) 이면, 최소제곱추정량 \\(\\hat{\\boldsymbol{\\beta}}\\) 은 최량선형불편추정량(Best Linear Unbiased Estimator)이 된다는 것이다. 여기에서 ’Best’의 의미는 모든 형태의 선형불편추정량 중에서 최소제곱추정량의 분산이 가장 작다는 것이다. 따라서 오차항의 가정이 심하게 위반이 된다면 최소제곱추정량에 의한 추론 결과에 대한 신뢰도가 떨어지게 된다. 1.2.1 회귀모형의 유의성 검정 식 (1.3)의 회귀모형에 포함된 설명변수 중 반응변수의 변동을 설명하는데 유의적인 변수가 적어도 하나라도 있는지 알아보는 검정이다. 귀무가설은 다음과 같고, \\[\\begin{equation} H_{0}:\\beta_{1}=\\beta_{2}=\\cdots=\\beta_{k}=0 \\tag{1.9} \\end{equation}\\] 대립가설은 다음과 같다. \\[\\begin{equation} H_{1}: \\text{at least one of }\\beta_{j} \\ne 0 \\end{equation}\\] 가설에 대한 검정통계량은 다음과 같다. \\[\\begin{equation} F = \\frac{(TSS-RSS)/k}{RSS/(n-k-1)} \\end{equation}\\] 귀무가설이 사실일 때 검정통게량의 분포는 \\(F_{k,~n-k-1}\\) 이다. 검정통계량의 구성을 살펴보자. \\(TSS=\\sum(y_{i}-\\overline{y})^{2}\\) 는 반응변수의 총변량으로써 설명변수와는 관련이 없는 변량이다. \\(RSS = \\sum(y_{i}-\\hat{y}_{i})^{2}\\) 는 잔차제곱합으로써 \\(y_{i}\\) 를 \\(\\hat{y}_{i}\\) 으로 예측한 오차이며, 회귀모형으로 설명이 안 된 변량이라고 할 수 있다. 따라서 총변량에서 설명이 안 된 변량을 제외한 \\(TSS-RSS\\) 는 회귀모형으로 설명된 변량이라고 할 수 있다. 검정통계량의 분모는 \\(RSS\\)를 자신의 자유도 \\((n-k-1)\\) 로 나눈 것인데, 이것은 회귀모형의 가정이 만족되는 경우에는 귀무가설의 사실 여부와 관계 없이 오차항의 분산인 \\(\\sigma^{2}\\)의 불편추정량이 된다. 즉, \\(E(RSS/(n-k-1))=\\sigma^{2}\\) 가 된다. 검정통계량의 분자인 \\((TSS-RSS)/k\\) 는 \\(H_{0}\\) 이 사실인 경우에는 \\(\\sigma^{2}\\) 의 불편추정량이 되지만, \\(H_{0}\\) 이 사실이 아닌 경우에는 \\(E((TSS-RSS)/k) &gt; \\sigma^{2}\\) 가 된다. 따라서 검정통계량이 1보다 상당히 큰 값이 되면 \\(H_{0}\\)이 사실이 아닐 가능성이 높게 되며, 기각역은 자유도가 \\((k, ~n-k-1)\\) 인 \\(F\\) 분포에서 결정된다. 가설 (1.9)이 기각되면 모형에 포함된 설명변수 중 유의한 변수가 있다는 것이므로, 개별 회귀계수에 대한 유의성 검정을 진행할 수 있다. 그러나 만일 가설을 기각할 수 없다면 식 (1.3)의 회귀모형으로는 반응변수의 변동을 제대로 설명할 수 없다는 것이 된다. 이런 결과가 나오면, 다른 설명변수의 조합을 찾아 보거나, 또는 다른 형태의 회귀모형을 시도해 봐야 한다. 1.2.2 개별회귀계수 유의성 검정 식 (1.3)에 정의된 회귀모형을 구성하고 있는 개별 설명변수의 유의성을 검정하는 절차이다. 가설은 다음과 같다. \\[\\begin{equation} H_{0}: \\beta_{j} = 0, \\quad H_{1}: \\beta_{j} \\ne 0, \\quad j = 1, \\ldots, k \\tag{1.10} \\end{equation}\\] 검정통계량은 개별 회귀계수의 추정량 \\(\\hat{\\beta}_{j}\\) 를 추정량의 표준오차로 나눈 것이다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{j}}{SE(\\hat{\\beta}_{j})} \\end{equation}\\] 귀무가설이 사실일 때 검정통계량의 분포는 \\(t_{n-k-1}\\) 이다. 개별 회귀계수에 대한 검정은 양측검정이 되며, 따라서 회귀계수의 신뢰구간과 밀접한 관련이 있다. 개별 회귀계수에 대한 95% 신뢰구간은 회귀계수와 추정량과 추정량의 표준오차를 이용하여 대략적으로 다음과 같이 표시된다. \\[\\begin{equation} \\hat{\\beta}_{j} \\pm 2 \\cdot SE(\\hat{\\beta}_{j}) \\end{equation}\\] 만일 주어진 자료를 근거로 계산된 \\(\\beta_{j}\\) 의 95% 신뢰구간에 0이 포함되어 있다면, 같은 자료로 계산한 검정통계량의 값은 5% 유의수준으로 구성된 기각역에 들어갈 수 없게 되어서 귀무가설을 기각할 수 없게 된다. 1.2.3 두 회귀모형의 비교 반응변수의 변동을 설명하는데 식 (1.3) 회귀모형의 설명변수를 모두 사용하는 것이 좋은지 아니면 일부분만을 사용하는 것이 더 좋은지 비교하는 절차이다. 비교하는 두 모형은 확장모형( \\(\\Omega\\) )인 모형 (1.3)과 특정 \\(q\\) 개의 회귀계수에 대한 다음의 가설이 사실인 축소모형( \\(\\omega\\) )이다. \\[\\begin{equation} H_{0}: \\beta_{k-q+1}=\\beta_{k-q+2}=\\cdots=\\beta_{k}=0 \\tag{1.11} \\end{equation}\\] 두 모형의 설명력 차이가 크지 않다면 ’모수절약의 원칙’에 의하여 축소모형을 선택하는 것이 더 좋을 것이다. 모형의 설명력 비교는 잔차제곱합을 이용할 수 있는데, 만일 확장모형의 잔차제곱합, \\(RSS_{\\Omega}\\) 와 축소모형의 잔차제곱합, \\(RSS_{\\omega}\\) 의 차이가 크지 않다면, 축소모형의 설명력이 확장모형 만큼 좋다는 의미가 된다. 검정통계량은 다음과 같이 구성된다. \\[\\begin{equation} F=\\frac{(RSS_{\\omega}-RSS_{\\Omega})/q}{RSS_{\\Omega}/(n-k-1)} \\end{equation}\\] 귀무가설이 사실일 때 검정통게량의 분포는 \\(F_{q,~n-k-1}\\) 이다. \\(\\bullet\\) 예제: state.x77 함수 lm()으로 생성된 객체를 대상으로 회귀모형의 추론을 위한 함수의 사용법을 예제와 함께 살펴보자. 1.1.2절에서 생성된 데이터 프레임 states를 예제로 회귀모형의 추론 단계에서 가장 빈번하게 사용되는 함수 summary()의 결과를 확인해 보자. states &lt;- as_tibble(state.x77) |&gt; rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) fit1 &lt;- lm(Murder ~ ., states) summary(fit1) ## ## Call: ## lm(formula = Murder ~ ., data = states) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4452 -1.1016 -0.0598 1.1758 3.2355 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.222e+02 1.789e+01 6.831 2.54e-08 *** ## Population 1.880e-04 6.474e-05 2.905 0.00584 ** ## Income -1.592e-04 5.725e-04 -0.278 0.78232 ## Illiteracy 1.373e+00 8.322e-01 1.650 0.10641 ## Life_Exp -1.655e+00 2.562e-01 -6.459 8.68e-08 *** ## HS_Grad 3.234e-02 5.725e-02 0.565 0.57519 ## Frost -1.288e-02 7.392e-03 -1.743 0.08867 . ## Area 5.967e-06 3.801e-06 1.570 0.12391 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.746 on 42 degrees of freedom ## Multiple R-squared: 0.8083, Adjusted R-squared: 0.7763 ## F-statistic: 25.29 on 7 and 42 DF, p-value: 3.872e-13 함수 lm()으로 생성된 객체에 함수 summary()를 적용시켜 얻은 결과물은 SAS나 SPSS에서 볼 수 있는 결과물과는 형식에서 차이가 있으나 많은 정보를 매우 효과적으로 보여주는 방식이라고 할 수 있다. 결과물을 하나씩 살펴보자. 먼저 Residuals:에는 잔차의 분포를 엿볼 수 있는 요약통계가 계산되어 있다. 가정이 만족된다면 잔차는 평균이 0인 정규분포를 보이게 되는데, 잔차의 요약 통계 결과 값으로 대략적인 판단을 할 수 있다. Coefficients:에는 회귀계수의 추정값과 표준오차가 계산되어 있다. 또한 개별 회귀계수의 유의성 검정인 \\(H_{0}:\\beta_{j} = 0, \\quad H_{1}: \\beta_{j} \\ne 0\\) 에 대한 검정통계량의 값과 p-값이 계산되어 있다 Residual standard error:는 회귀모형의 평가 측도 중 하나로 식 (1.14)에 정의되어 있다. Multiple R-squared:는 결정계수 \\(R^{2}\\) 이고, Adjusted R-squared:는 수정결정계수의 값이다. F-statistic:은 모든 회귀계수가 0이라는 가설, 즉 \\(H_{0}:\\beta_{1}=\\beta_{2}=\\cdots=\\beta_{k}=0\\) 에 대한 검정통계량의 값과 자유도, 그리고 p-값이 계산되어 있다. 함수 summary()로 얻어지는 결과물로 회귀모형에 대한 중요한 추론이 가능함을 알 수 있다. 한 가지 SAS나 SPSS에 익숙한 사용자들에게 아쉬울 수 있는 점은 아마도 회귀모형의 분산분석표가 출력되지 않는다는 것일 텐데, 이것은 함수 anova()를 사용함으로써 해결할 수 있다. anova(fit1) ## Analysis of Variance Table ## ## Response: Murder ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## Population 1 78.854 78.854 25.8674 8.049e-06 *** ## Income 1 63.507 63.507 20.8328 4.322e-05 *** ## Illiteracy 1 236.196 236.196 77.4817 4.380e-11 *** ## Life_Exp 1 139.466 139.466 45.7506 3.166e-08 *** ## HS_Grad 1 8.066 8.066 2.6460 0.1113 ## Frost 1 6.109 6.109 2.0039 0.1643 ## Area 1 7.514 7.514 2.4650 0.1239 ## Residuals 42 128.033 3.048 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 함수 anova()는 두 회귀모형을 비교할 때에도 사용되는 함수이다. 비교되는 두 모형은 확장모형과 확장모형의 부분집합인 축소모형이어야 한다. 확장모형의 부분집합이라는 것은 확장모형의 일부 회귀계수가 0이 되는 모형을 의미하는 것으로써, 확장모형에 없는 설명변수가 축소모형에 포함되면 정상적인 비교가 이루어질 수 없다. states 자료에서 모든 설명변수가 포함된 확장모형 fit1과 변수 Income, Illiteracy, HS_Grad의 회귀계수가 0인 축소모형 fit2를 비교해 보자. 함수 anova()에 의한 비교 방법은 anova(fit2, fit1)과 같이 축소모형이 먼저 지정되어야 한다. fit2 &lt;- lm(Murder ~ Population + Life_Exp + Frost + Area, states) anova(fit2, fit1) ## Analysis of Variance Table ## ## Model 1: Murder ~ Population + Life_Exp + Frost + Area ## Model 2: Murder ~ Population + Income + Illiteracy + Life_Exp + HS_Grad + ## Frost + Area ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 45 137.75 ## 2 42 128.03 3 9.7212 1.063 0.375 함수 anova()로 검정이 실시된 귀무가설은 변수 Income, Illiteracy, HS_Grad의 회귀계수가 0이라는 것인데, p-값이 0.375으로 계산되어 귀무가설을 기각할 수 없게 되었다. 두 모형의 설명력에는 차이가 없다는 것이므로, 모수 절약의 원칙에 따라 축소모형을 선택하는 것이 더 좋다고 하겠다. 회귀계수의 신뢰구간은 함수 confint()로 계산할 수 있다. 95% 신뢰구간이 디폴트로 계산되며, 만일 신뢰수준을 변경하고자 한다면 옵션 level에 원하는 신뢰수준을 입력하면 된다. 객체 fit1에 포함되어 있는 회귀계수의 95% 신뢰구간은 다음과 같이 계산된다. confint(fit1) ## 2.5 % 97.5 % ## (Intercept) 8.608453e+01 1.582763e+02 ## Population 5.739093e-05 3.186812e-04 ## Income -1.314619e-03 9.962053e-04 ## Illiteracy -3.063433e-01 3.052562e+00 ## Life_Exp -2.171926e+00 -1.137814e+00 ## HS_Grad -8.320224e-02 1.478789e-01 ## Frost -2.780257e-02 2.034427e-03 ## Area -1.702987e-06 1.363763e-05 회귀계수의 95% 신뢰구간에 0이 포함됐다는 것은 5% 유의수준에서 가설 \\(H_{0}:\\beta_{j}=0, \\quad H_{1}:\\beta_{j} \\ne 0\\) 의 귀무가설을 기각할 수 없음을 의미한다. 따라서 95% 신뢰구간에 0이 포함된 변수 Income, HS_Grad, Frost, Area는 5% 유의수준에서 비유적인 변수인 것이다. \\(\\bullet\\) 예제 : carData::Leinhardt 1.1.4절에서 살펴본 carData::Leinhardt의 예제에서 infant와 income을 로그 변환하여 Leinhardt_ln을 생성하였는데, Leinhardt_ln에는 범주형 변수인 oil과 region이 있으며, 두 변수를 설명변수로 회귀모형에 포함시킨다면 가변수를 사용해야 한다. 가변수를 사용하는 경우에 식 (1.8)에서와 같이 반응변수와 다른 연속형 설명변수의 관계, 즉 기울기가 동일하다고 가정할 수 있다. 그러나 범주형 변수로 구성되는 각 그룹별로 설명변수의 기울기가 다르다고 가정하는 것이 더 일반적인 접근이 될 수 있다. 만일 기울기가 그룹별로 다르다고 가정한다면 가변수와 다른 설명변수의 상호작용 효과를 추가해야 한다. 예를 들어 연속형 변수 \\(X\\) 와 범주의 개수가 2개인 범주형 변수를 설명변수로 하는 회귀모형에서 가변수와 연속형 변수의 상호작용 효과가 추가된 모형은 다음과 같이 표현된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\beta_{3}XD + \\varepsilon \\tag{1.12} \\end{equation}\\] 이 경우에 모집단 회귀직선은 다음과 같다. \\[ E(Y) = \\begin{cases} \\beta_{0} + \\beta_{1}X &amp; \\quad \\text{if } D = 0 \\\\ (\\beta_{0} + \\beta_{2}) + (\\beta_{1}+\\beta_{3})X &amp; \\quad \\text{if } D = 1 \\end{cases} \\] 연속형 설명변수와 가변수의 상호작용 효과를 모형에 추가하는 것이 더 포괄적인 모형을 구축하는 방법이지만, 설명변수가 많은 경우에는 지나치게 많은 변수가 모형에 추가될 수 있어서 분석에 큰 부담이 될 수 있다. 반드시 필요한 상호작용 효과를 선별해서 추가하는 것이 바람직하다. Leinhardt_ln 자료에서 반응변수 ln_infant에 대해 설명변수ln_income과oil의 회귀모형을 설정하되oil이\"no\"인 그룹과\"yes\"인 그룹에서ln_infant와 ln_income의 관계가 같지 않다고 가정해 보자. data(Leinhardt, package = &quot;carData&quot;) Leinhardt_ln &lt;- Leinhardt |&gt; mutate(ln_income = log(income), ln_infant = log(infant), .before = 1) |&gt; select(-income, -infant) 회귀모형에 두 설명변수 변수의 상호작용 효과를 추가해 보자. fit_L &lt;- lm(ln_infant ~ ln_income*oil, Leinhardt_ln) summary(fit_L) ## ## Call: ## lm(formula = ln_infant ~ ln_income * oil, data = Leinhardt_ln) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.59564 -0.31166 0.02369 0.35132 1.40499 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.42765 0.27987 26.540 &lt; 2e-16 *** ## ln_income -0.56905 0.04542 -12.529 &lt; 2e-16 *** ## oilyes -5.37649 1.31025 -4.103 8.48e-05 *** ## ln_income:oilyes 0.98100 0.20551 4.773 6.41e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5929 on 97 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.6364, Adjusted R-squared: 0.6252 ## F-statistic: 56.59 on 3 and 97 DF, p-value: &lt; 2.2e-16 모든 회귀계수가 유의적임을 알 수 있는데, 상호작용 효과항이 유의하다는 것은 산유국 여부가 절편 및 기울기 차이에 유의한 영향을 주고 있다는 의미가 된다. 세 변수의 관계를 그래프를 이용해서 조금 더 깊게 탐색해 보자. Leinhardt_ln |&gt; ggplot(aes(x = ln_income, y = ln_infant)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + facet_wrap(vars(oil)) 그림 1.9: 변수 oil의 두 그룹에서 ln_infant와 ln_income의 산점도 oil이 \"no\"인 그룹에서 ln_infant'와ln_income는 음의 관계를 가지고 있다. 소득이 증가하면 신생아 사망률이 감소하는 것은 당연한 것으로 보인다. 그러나oil이“yes”`인 그룹에서는 양의 관계를 보여주고 있는데, 이것은 소득 수준이 상당히 높은 두 나라의 신생아 사망률이 지나치게 높기 때문에 발생한 현상으로 보인다. 즉, 두 나라의 자료는 전혀 다른 얘기를 하고 있는 것으로 보인다. 두 점이 어떤 나라의 자료인지 확인해 보자. Leinhardt_ln |&gt; filter(oil == &quot;yes&quot;) |&gt; slice_max(ln_infant, n = 2) ## ln_income ln_infant region oil ## Saudi.Arabia 7.333023 6.476972 Asia yes ## Libya 8.009695 5.703782 Africa yes 소득이 상당히 높지만 다른 나라의 경우와는 너무도 다르게 신생아 사망률이 지나치게 높은 두 나라의 자료는 제외되어야 할 것 같다. 두 자료를 제외하고 다시 회귀모형을 적합해 보자. Leinhardt_ln |&gt; rownames_to_column(var = &quot;country&quot;) |&gt; filter(!(country %in% c(&quot;Saudi.Arabia&quot;, &quot;Libya&quot;))) |&gt; lm(ln_infant ~ ln_income*oil, data = _) |&gt; summary() ## ## Call: ## lm(formula = ln_infant ~ ln_income * oil, data = filter(rownames_to_column(Leinhardt_ln, ## var = &quot;country&quot;), !(country %in% c(&quot;Saudi.Arabia&quot;, &quot;Libya&quot;)))) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.59564 -0.29881 0.02503 0.34251 1.39357 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 7.42765 0.26149 28.405 &lt;2e-16 *** ## ln_income -0.56905 0.04244 -13.410 &lt;2e-16 *** ## oilyes -0.90561 1.76281 -0.514 0.609 ## ln_income:oilyes 0.16568 0.29887 0.554 0.581 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.5539 on 95 degrees of freedom ## (4 observations deleted due to missingness) ## Multiple R-squared: 0.6572, Adjusted R-squared: 0.6464 ## F-statistic: 60.71 on 3 and 95 DF, p-value: &lt; 2.2e-16 두 자료를 제외한 상태에서는 산유국 여부가 더 이상 유의한 변수가 아닌 것으로 나타났다. \\(\\bullet\\) 예제 : modeldata::crickets 패키지 modeldata의 데이터 프레임 crickets에는 기온과 귀뚜라미 울음소리 횟수의 관계를 탐색하기 위해 관측한 자료가 입력되어 있다. 변수 temp는 기온, rate는 귀뚜라미의 분당 울음소리 횟수이며, species는 귀뚜라미 종류이다. data(crickets, package = &quot;modeldata&quot;) str(crickets) ## tibble [31 × 3] (S3: tbl_df/tbl/data.frame) ## $ species: Factor w/ 2 levels &quot;O. exclamationis&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ temp : num [1:31] 20.8 20.8 24 24 24 24 26.2 26.2 26.2 26.2 ... ## $ rate : num [1:31] 67.9 65.1 77.3 78.7 79.4 80.4 85.8 86.6 87.5 89.1 ... 세 변수의 관계 탐색을 위한 그래프를 작성해 보자. 변수 temp와 rate의 단순 산점도와 species의 효과를 살펴볼 수 있는 산점도를 모두 작성해 보자. library(patchwork) p1 &lt;- ggplot(crickets, aes(x = temp, y = rate)) + geom_point() p2 &lt;- ggplot(crickets, aes(x = temp, y = rate, color = species)) + geom_point() p1 + p2 그림 1.10: crickets 자료의 세 변수 관계 탐색 변수 temp와 rate 사이에는 명확한 양의 관계가 있는데, species의 두 범주 사이에는 효과 차이가 있는 것으로 보인다. 가변수를 사용한 선형회귀모형을 사용하는 것이 적절한 것으로 보이는데, 두 범주에서 변수 temp와 rate의 관계가 동일한지 여부를 확인하기 위해서 그림 1.10의 오른쪽 그래프에 회귀직선을 추가해 보자. ggplot(crickets, aes(x = temp, y = rate, color = species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Temperature (C)&quot;, y = &quot;Chirp Rate (per minute)&quot;) 그림 1.11: crickets 자료의 세 변수 관계 탐색 그림 1.11에서 두 직선의 기울기는 큰 차이가 없는 것으로 보인다. 따라서 temp와 species의 상호작용 효과는 필요 없는 것으로 보이는데, 이것을 검정을 통해 확인해 보자. 식 (1.8)의 모형으로 fit.main을 적합하고, 식 (1.12)의 모형으로 fit.inter을 적합해 보자. fit.main &lt;- lm(rate ~ temp + species, data = crickets) fit.inter &lt;- lm(rate ~ temp * species, data = crickets) 모형 fit.main은 fit.inter에서 상호작용 효과만 제외된 모형으므로 fit.inter의 축소모형이 된다. 따라서 상호작용 효과의 유의성 여부는 함수 anova()를 사용해서 두 모형을 비교해서 획인할 수 있다. anova(fit.main, fit.inter) ## Analysis of Variance Table ## ## Model 1: rate ~ temp + species ## Model 2: rate ~ temp * species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 89.350 ## 2 27 85.074 1 4.2758 1.357 0.2542 검정 결과 p-값이 0.2542로 계산되어서, 두 모형의 설명력에는 차이가 없는 것으로 볼 수 있다. 따라서 축소모형인 fit.main을 선택하게 되고, 상호작용 효과는 비유의적이라고 결론내릴 수 있다. 물론 모형 fit.inter의 개별회귀계수 검정으로도 동일한 가설을 검정할 수 있다. summary(fit.inter)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.0408481 4.1514800 -2.6594969 1.300079e-02 ## temp 3.7514472 0.1601220 23.4286850 1.780831e-19 ## speciesO. niveus -4.3484072 4.9616805 -0.8763981 3.885447e-01 ## temp:speciesO. niveus -0.2339856 0.2008622 -1.1649059 2.542464e-01 모형 fit.main의 적합 결과를 확인해 보자. summary(fit.main) ## ## Call: ## lm(formula = rate ~ temp + species, data = crickets) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0128 -1.1296 -0.3912 0.9650 3.7800 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.21091 2.55094 -2.827 0.00858 ** ## temp 3.60275 0.09729 37.032 &lt; 2e-16 *** ## speciesO. niveus -10.06529 0.73526 -13.689 6.27e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.786 on 28 degrees of freedom ## Multiple R-squared: 0.9896, Adjusted R-squared: 0.9888 ## F-statistic: 1331 on 2 and 28 DF, p-value: &lt; 2.2e-16 1.3 변수선택 반응변수 \\(Y\\) 에 대한 다음의 회귀모형에는 모든 가능한 설명변수가 다 포함되어 있다고 가정하고, 그런 의미에서 full model이라고 하자. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} + \\varepsilon \\tag{1.13} \\end{equation}\\] 모든 가능한 설명변수가 다 포함된 모형 (1.13)을 근거로 실시한 예측 결과는 일반적으로 최상의 결과를 보여주지 못하는 것으로 알려져 있다. 많은 설명변수 중 모형 설명력을 실질적으로 향상시킬 수 있는 일부 변수만을 선택해서 회귀모형을 설정하는 것이 더 향상된 예측력을 기대할 수 있으며, 모형의 의미를 더 쉽게 해석할 수 있게 된다. 반응변수의 변동을 설명할 수 있는 많은 설명변수들 중에서 ’최적’의 설명변수를 선택하는 절차를 변수선택이라고 한다. 변수선택 방법은 크게 세 가지로 구분되는데, 검정에 의하여 단계적으로 모형을 찾아가는 방법과 다양한 평가측도를 이용하여 모형을 찾아가는 방법, 그리고 shrinkage 방법이 있다. 검정에 의한 변수선택은 SAS나 SPSS 등에서 일반적으로 이루어지는 방법으로서 후진소거법, 전진선택법과 단계별 선택법이 있다. 변수 선택 과정에서 요구되는 계산이 방대하지 않기 때문에 대규모의 설명변수가 있는 경우 손쉽게 중요 변수를 선택할 수 있다는 장점이 있는 방법이지만 변수의 선택과 제거가 ‘한 번에 하나씩’ 이루어지기 때문에 이른바 ‘최적’ 모형을 놓치는 경우가 발생할 수도 있다. 또한 각 단계마다 여러 번의 검정이 동시에 이루어지기 때문에 일종 오류를 범할 확률이 증가하는 검정의 정당성 문제가 발생할 수도 있다. 그리고 모형의 수립 목적이 예측인 상황에서는 변수선택 과정이 목적과 어울리지 않는다는 문제도 지니고 있는 등 많은 문제점이 있기 때문에 최근에는 거의 사용되지 않는 방법이다. 1.3.1 회귀모형의 평가 측도 회귀모형을 적합하는 목적은 반응변수 변량의 단순 설명 뿐 아니라 반응변수 값의 예측도 있다. 반응변수의 변량을 충분히 설명하는 것이 주된 목적이라면 결정계수를 모형의 평가 측도로 사용하는 것이 좋을 것이다. 그러나 정확한 예측을 목적으로 한다면 결정계수가 아닌 다른 평가 측도를 사용해야 한다. 결정계수(\\(R^{2}\\) ) 결정계수는 반응변수의 총변량 중 회귀모형으로 설명된 변량의 비율을 의미한다. \\[\\begin{equation} R^{2} = \\frac{TSS-RSS}{TSS} = 1-\\frac{RSS}{TSS} \\end{equation}\\] 단순회귀모형에서 결정계수는 \\(Y\\) 와 \\(X\\) 의 상관계수인 \\(r=Cor(X,Y)\\)의 제곱과 같고, 다중회귀모형에서는 \\(Y\\) 와 \\(\\hat{Y}\\) 의 상관계수 제곱, \\(Cor(Y, \\hat{Y})^{2}\\) 과 같다. 결정계수는 회귀모형에 설명변수가 추가되면 무조건 증가하는 특성이 있기 때문에 설명변수의 개수가 다른 회귀모형의 평가 측도로는 적절하지 않다고 할 수 있다. 수정결정계수(\\(adj~R^{2}\\) ) 회귀모형에 설명변수가 추가되면 무조건 값이 증가하는 결정계수의 문제를 보완한 측도이다. 추가된 설명변수가 모형의 설명력 향상에 도움이 되는 경우에만 값이 증가되도록 결정계수를 수정한 측도이다. \\[\\begin{equation} adj~R^{2}=1-\\frac{RSS/(n-k-1)}{TSS/(n-1)} \\end{equation}\\] Residual standard error(RSE) 잔차제곱합, \\(RSS\\) 는 반응변수의 총변량 중에 회귀모형으로 설명이 되지 않는 변량으로써, 결정계수의 값을 결정하는 요소이며, 회귀모형에 설명변수가 추가되면 무조건 감소하는 특성을 가지고 있다. 이러한 특성은 다음과 같이 잔차제곱합을 자신의 자유도로 나누면 수정이 된다. \\[\\begin{equation} RSE = \\sqrt{\\frac{RSS}{n-k-1}} \\tag{1.14} \\end{equation}\\] 수정결정계수와 실질적으로 동일한 특성을 가지고 있는 측도로써, 추가된 설명변수가 모형의 설명력 향상에 도움이 되는 경우에만 값이 감소하는 특성을 가지고 있다. Information criteria 회귀모형의 오차항에 대한 분포를 가정하고 있기 때문에 Likelihood function은 어렵지 않게 구성할 수 있는데, 회귀모형에 설명변수를 추가함으로써 모수의 개수가 증가하게 되면 Maximum likelihood의 값도 자연스럽게 증가하게 된다. 이것은 결정계수의 특성과 유사한 것이며, 따라서 설명변수 증가에 대한 penalty를 부과하는 측도를 생각해 볼 수 있다. AIC와 BIC는 다음과 같이 정의된다. \\[\\begin{align*} AIC &amp;= -2\\log \\hat{L} + 2K \\\\ BIC &amp;= -2\\log \\hat{L} + k\\log n \\end{align*}\\] 단, \\(\\hat L\\) 은 maximum likelihood function이고, \\(n\\) 은 자료의 크기, \\(k\\) 는 설명변수의 개수이다. 회귀모형에 설명변수가 추가되면, \\(\\hat L\\) 의 값은 증가하게 되고, 따라서 \\(-2\\log \\hat L\\) 은 감소한다. 설명변수 증가에 대한 penalty로써 AIC는 \\(2k\\)를, BIC는 \\(k \\log n\\)를 사용하고 있는데, 설명변수를 추가함으로써 증가된 penalty의 값보다 모형의 적합도 향상으로 감소된 \\(-2 \\log \\hat L\\) 의 값이 더 크다면 AIC나 BIC는 감소하게 된다. 따라서 추가된 설명변수가 모형의 적합도 향상에 도움이 되는 경우에만 값이 감소하는 특성을 가지고 있다. Cross-validation(CV) CV는 회귀모형의 예측력을 평가할 수 있는 resampling 기법이다. 예측 목적으로 설정된 회귀모형의 경우에는 모형 적합에 사용된 자료의 변동 설명보다 적합에 사용되지 않은 자료에 대한 예측 오차가 더 중요한 평가 요소라고 할 수 있다. 통계모형 적합에 사용되지 않은 새로운 자료에 대한 예측 오차를 test error라고 하는데, 회귀모형의 예측 결과에 대한 정당성을 확보하기 위해서는 낮은 test error가 필수적이다. CV는 모형 적합에 사용되는 자료인 training data를 이용해서 test error를 추정하기 위한 resampling 기법이다. 몇 가지 조금 다른 방법이 있는데, 여기에서는 k-fold CV라는 방법에 대해서만 살펴보겠다. k-fold CV는 training data를 비슷한 크기의 k개 그룹(fold)으로 분리하는 것으로 시작한다. 분리된 k개의 그룹 중 첫 번째 그룹의 자료를 제외하고, 나머지 (k-1)개 그룹의 자료를 사용해서 적합한 회귀모형으로 적합 과정에서 제외된 첫 번째 그룹에 대한 예측을 실시하고 예측 오차를 계산한다. 이어서 각 그룹의 자료를 하나씩 차례로 제외하고 나머지 자료로 적합 및 예측하는 과정을 반복하여 k번의 예측을 실시한다. Test error는 k번의 예측에서 각각 발생된 예측 오차의 평균으로 추정한다. 1.3.2 평가 측도에 의한 변수선택 평가 측도에 의한 방법은 1.3.1절에서 살펴본 \\(AIC\\), \\(BIC\\), \\(adj~R^{2}\\) 및 \\(C_{p}\\) 통계량 등을 기반으로 ‘최적’ 변수를 선택하는 방법이다. 모형 (1.13)의 full model에서 \\(d\\) 개의 설명변수를 선택하여 설정된 회귀모형에 대한 \\(C_{p}\\) 통계량은 다음과 같이 정의된다. \\[\\begin{equation} C_{p} = \\frac{1}{n} \\left( RSS + 2d\\hat{\\sigma}^{2} \\right) \\tag{1.15} \\end{equation}\\] 단, \\(\\hat{\\sigma}^{2}\\) 는 모형 (1.13)으로 추정한 오차항의 분산이며, \\(RSS\\) 는 잔차제곱합이다. 모형의 평가에서 \\(AIC\\), \\(BIC\\), \\(C_{p}\\) 값은 작을수록 좋은 모형이고, \\(adj~R^{2}\\)는 값이 클수록 좋은 모형이라 할 수 있다. 평가 측도를 사용한 방법은 세 가지로 구분된다. forward stepwise selection과 backward stepwise elimination, 그리고 best subset selection이 있다. 1. Forward stepwise selection 절편만 있는 모형 \\(M_{0}~(Y=\\beta_{0}+\\varepsilon)\\) 에서 시작하여 단계적으로 변수를 하나씩 모형에 추가하는 방법이다. 추가할 변수를 선택하는 절차는 다음과 같이 두 가지 방식이 있다. \\(\\bullet\\) 방식 1 \\(AIC\\) 등을 기준으로 모형의 성능을 최대로 개선시킬 수 있는 변수를 하나씩 모형에 추가 더 이상 모형의 성능을 개선시킬 변수가 없으면, 절차 종료 \\(\\bullet\\) 방식 2 \\(i=0, \\ldots, k-1\\) 에 대하여 \\(M_{i}\\) 에 하나의 설명변수를 추가한 \\((k-i)\\) 개 모형 중 결정계수 값이 가장 큰 모형을 \\(M_{i+1}\\) 으로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형을 하나 선택 Forward stepwise selection에서는 일단 모형에 포함된 변수에 대해서는 추가적인 확인 과정이 없다. 그러나 이러한 특성은 개별 설명변수의 영향력은 모형에 추가적으로 포함되는 다른 설명변수에 의해 변할 수 있기 때문에 문제가 될 수 있다. 또한 ‘한 번에 하나씩’ 이루어지는 선택 과정으로 인하여 ‘최적’ 변수의 조합을 찾지 못할 수 있다는 문제도 있다. 2. Backward stepwise elimination 모든 설명변수가 포함된 full model, \\(M_{k}\\) 에서 시작하여 단계적으로 변수를 하나씩 제거하는 방법이다. 제거할 변수를 선택하는 절차는 다음과 같이 두 가지 방식이 있다. \\(\\bullet\\) 방식 1 제거되면 모형의 성능이 최대로 개선되는 변수를 하나씩 제거 모형에 포함된 변수 중 어떤 변수라도 제거되면 모형의 성능이 더 나빠질 때 절차 종료 \\(\\bullet\\) 방식 2 \\(i=k, k-1, \\ldots, 1\\) 에 대하여 \\(M_{i}\\) 에서 하나씩 변수가 제거된 모구 \\(i\\) 개 모형 중 결정계수의 값이 가장 큰 모형을 \\(M_{i-1}\\) 으로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형을 하나 선택 Backward stepwise elimination에서는 일단 제거된 변수는 다시 모형에 포함될 수 없는 방식이다. 그러나 이러한 특성은 개별 설명변수의 영향력은 모형에서 다른 설명변수가 제거되면 변할 수 있기 때문에 문제가 될 수 있다. 또한 Forward stepwise selection의 경우처럼 ‘한 번에 하나씩’ 이루어지는 선택 과정으로 인하여 ‘최적’ 변수의 조합을 찾지 못할 수 있다는 문제도 있다. 3. Hybrid stepwise selection Forward stepwise selection과 Backward stepwise elimination는 일단 모형에 포함되거나 혹은 모형에서 제거된 변수에 대한 추가적인 고려가 불가능한 방식이다. 이러한 특성으로 인하여 ‘최적’ 모형을 찾지 못할 가능성이 있는데, 이 문제는 두 방식의 특성을 혼합시킨 방식을 적용하면 해결할 수 있다. Hybrid forward selection : 각 단계별로 변수를 추가한 후에 모형에 포함되어 있는 변수를 대상으로 backward elimination 기법을 적용해서 변수 제거 Hybrid backward elimination : 각 단계별로 변수를 제거한 후에 이미 제거된 변수를 대상으로 forward selection 기법으로 모형에 추가될 변수 선택 4. Best subset selection \\(k\\) 개 설명변수의 모든 가능한 조합 중 모형의 성능을 최대로 할 수 있는 조합을 찾는 방법이다. \\(k\\) 개 설명변수에 대한 모든 가능한 조합은 \\(2^{k}\\) 가 되기 때문에, 설명변수가 많은 경우에는 현실적으로 적용하기 어려운 방법이다. 일반적으로 적용되는 방식은 다음과 같다. \\(i=1,2,\\ldots,k\\) 에 대하여 \\(i\\) 개 설명변수가 있는 \\(\\binom{~k~}{i}\\) 개 모형을 적합하고, 그 중 결정계수의 값이 가장 큰 모형을 \\(M_{i}\\) 로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형을 하나 선택 \\(\\bullet\\) 변수선택을 위한 함수 Best subset selection은 함수 leaps::regsubsets()로 할 수 있다. 기본적인 사용법은 regsubsets(formula, data, nbest = 1, nvmax = 8)이다. formula와 data는 함수 lm()에서 사용한 방식과 동일하다. 설명변수의 개수가 동일한 모형 중 결정계수의 값이 가장 큰 nbest개 모형을 선택하는데, 이렇게 선택된 모형을 대상으로 다양한 평가 측도를 기준으로 비교할 수 있다. 또한 최대 nvmax개의 설명변수가 포함된 모형을 대상으로 비교를 하기 때문에, nvmax에는 설명변수의 개수를 지정할 필요가 있다. Stepwise selection은 함수 MASS::stepAIC()로 할 수 있다. 기본적인 사용법은 stepAIC(object, scope, direction, trace = 1, k = 2)이다. object는 stepwise selection을 시작하는 회귀모형을 나타내는 lm 객체를 지정한다. Forward selection의 경우에는 절편만이 있는 모형인 lm(y ~ 1, data)가 될 것이고, Backward elimination의 경우에는 모든 설명변수가 포함된 모형인 lm(y ~ ., data)가 될 것이다. scope는 탐색 범위를 나타내는 upper와 lower를 요소가 갖고 있는 리스트를 지정한다. direction은 탐색 방식을 지정하는데, \"forward\"는 모형에 포함된 변수는 계속 유지하면서 forward selection 방법을 진행하며, \"backward\"는 일단 모형에서 제거된 변수는 최종 모형에 포함시키지 않는 방식으로 backward elimination을 진행한다. 또한 \"both\"는 hybrid stepwise selection 방법을 진행한다. direction의 디폴트는 scope를 지정하면 \"both\"가 되지만, scope가 생략되면 \"backward\"가 된다. trace는 탐색 과정의 출력 여부를 지정하는 것으로 출력되는 것이 디폴트이며, FALSE 또는 0을 지정하면 출력되지 않는다. k는 AIC나 BIC의 계산 과정에서 모형에 포함된 변수의 개수만큼 불이익을 주기 위한 상수로서, 디폴트는 AIC 계산을 위한 k = 2이다. 만일 BIC에 의한 단계별 선택법을 적용하고자 한다면 k = log(n)으로 수정해야 한다. 단, n은 데이터 프레임의 행 개수이다. \\(\\bullet\\) 예제: states 1.1.2절에서 생성된 데이터 프레임 states를 대상으로 변수선택을 진행해 보자. library(tidyverse) states &lt;- as_tibble(state.x77) %&gt;% rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) Best subset selection 방법을 함수 leaps::regsubsets()로 진행해 보자. library(leaps) fits &lt;- regsubsets(Murder ~ ., states) 설명변수의 개수가 \\(i\\) 개 (\\(i=1, \\ldots, 7\\)) 인 모형 중 결정계수가 가장 높은 nbest개의 모형은 다음과 같이 함수 summary()로 출력할 수 있다. 모형에 포함되는 변수에는 \"*\"가 표시되어 있다. summary(fits) ## Subset selection object ## Call: regsubsets.formula(Murder ~ ., states) ## 7 Variables (and intercept) ## Forced in Forced out ## Population FALSE FALSE ## Income FALSE FALSE ## Illiteracy FALSE FALSE ## Life_Exp FALSE FALSE ## HS_Grad FALSE FALSE ## Frost FALSE FALSE ## Area FALSE FALSE ## 1 subsets of each size up to 7 ## Selection Algorithm: exhaustive ## Population Income Illiteracy Life_Exp HS_Grad Frost Area ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; ## 3 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 설명변수가 하나인 경우에는 Life_Exp, 두 개인 경우에는 Life_Exp와 Frost, 세 개인 경우에는 Population, Illiteracy, Life_Exp가 선택된 것을 알 수 있다. 이제 선택된 7개 모형을 대상으로 평가 측도를 비교해 보자. 평가 측도의 비교는 객체 fits를 함수 plot()에 입력하면 된다. 평가 측도는 scale에 지정할 수 있는데, 가능한 키워드는 \"bic\", \"Cp\", \"adjr2\", \"r2\"이며, 디폴트는 \"bic\"이다. plot(fits) 그림 1.12: BIC에 의한 비교 결과 그림 1.12의 X축에는 설명변수가 표시되어 있고, Y축에는 비교 대상이 되는 각 모형의 \\(BIC\\) 의 값이 표시되어 있다. 즉, 그래프의 각 행은 비교 대상이 되는 각 모형을 나타내는 것이며, X축에 표시된 설명변수가 해당 모형에 포함이 된 것이면 직사각형에 색이 채워진다. 위에서 첫 번째 모형이 \\(BIC\\) 의 값이 가장 작은 모형이며, 변수 Population, Life_Exp, Frost, Area가 포함된 모형이다. 수정결정계수를 평가 측도로 변경하여 변수선택을 진행해 보자. 변수 Population, Illiteracy, Life_Exp, Frost, Area가 선택되었다. plot(fits, scale = &quot;adjr2&quot;) 그림 1.13: 수정결정계수에 의한 비교 결과 Stepwise selection 방법을 함수 MASS::stepAIC()로 진행해 보자. 먼저 모든 설명변수가 포함된 모형과 절편만 있는 모형에 대한 lm 객체를 생성시키자. fit_full &lt;- lm(Murder ~ ., states) fit_null &lt;- lm(Murder ~ 1, states) AIC에 의한 forward selection으로 변수선택을 진행해 보자. fit_null로 시작하고 scope의 upper가 fit_full로 지정하였기 때문에 hybrid forward selection이 수행된다. 변수 Life_Exp, Frost, Population, Area, Illiteracy가 선택되었다. library(MASS) stepAIC(fit_null, scope = list(lower = fit_null, upper = fit_full), trace = FALSE) ## ## Call: ## lm(formula = Murder ~ Life_Exp + Frost + Population + Area + ## Illiteracy, data = states) ## ## Coefficients: ## (Intercept) Life_Exp Frost Population Area Illiteracy ## 1.202e+02 -1.608e+00 -1.373e-02 1.780e-04 6.804e-06 1.173e+00 AIC에 의한 backward elimination으로 변수선택을 진행해 보자. Backward elimination은 full model을 시작 모형으로 지정하면 실행된다. AIC에 의한 forward selection과 같은 결과를 보이고 있다. stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) ## ## Call: ## lm(formula = Murder ~ Population + Illiteracy + Life_Exp + Frost + ## Area, data = states) ## ## Coefficients: ## (Intercept) Population Illiteracy Life_Exp Frost Area ## 1.202e+02 1.780e-04 1.173e+00 -1.608e+00 -1.373e-02 6.804e-06 BIC에 의한 forward selection으로 변수선택을 진행해 보자. 변수 Illiteracy가 제외되어 AIC에 의한 방법과는 다른 결과가 나왔다. stepAIC(fit_null, scope = list(lower = fit_null, upper = fit_full), k = log(nrow(states)), trace = FALSE) ## ## Call: ## lm(formula = Murder ~ Life_Exp + Frost + Population + Area, data = states) ## ## Coefficients: ## (Intercept) Life_Exp Frost Population Area ## 1.387e+02 -1.837e+00 -2.204e-02 1.581e-04 7.387e-06 BIC에 의한 backward elimination으로 변수선택을 진행해 보자. BIC에 의한 forward selection과 같은 결과를 보이고 있다. stepAIC(fit_full, direction = &quot;both&quot;, k = log(nrow(states)), trace = FALSE) ## ## Call: ## lm(formula = Murder ~ Population + Life_Exp + Frost + Area, data = states) ## ## Coefficients: ## (Intercept) Population Life_Exp Frost Area ## 1.387e+02 1.581e-04 -1.837e+00 -2.204e-02 7.387e-06 1.3.3 Shrinkage 방법 Shrinkage 방법이란 모든 설명변수를 포함한 full model을 대상으로 회귀계수의 추정 결과에 제약 조건을 부과함으로써 회귀계수의 추정값을 0으로 수축시키는 방법이다. 회귀계수의 추정값이 정확하게 0으로 수축되면, 해당 변수는 실질적으로 모형에서 제외된 것이기 때문에 변수선택과 동일한 효과를 볼 수 있다. 가장 많이 사용되는 shrinkage 방법에는 ridge regression과 lasso가 있다. 회귀계수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 를 최소제곱추정법으로 추정하기 위해서는 다음의 \\(RSS\\) 를 최소화시켜야 한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{k}\\beta_{j}x_{j} \\right)^{2} \\end{equation}\\] Ridge regression의 회귀계수 추정량 \\(\\hat{\\beta}^{R}\\) 의 추정 결과는 다음의 변량을 최소화시켜야 한다. \\[\\begin{equation} RSS + \\lambda \\sum_{j=1}^{k}\\beta_{j}^{2} \\end{equation}\\] 단, \\(\\lambda\\) 는 회귀계수의 수축 정도를 조절하는 모수로써 클수록 강하게 수축시키는 역할을 한다. Ridge regression의 회귀계수 추정값은 영향력이 작은 변수의 경우에는 상대적으로 0에 더 근접한 값으로 수축이 되지만, 정확하게 0의 값을 갖는 것은 아니다. 따라서 변수선택의 목적으로는 사용할 수 없는 방법이 된다. Lasso(Least Absolute Shrinkage and Selection Operator)는 ridge regression과는 다르게 영향력이 작은 변수의 회귀계수를 0으로 만들 수 있기 때문에 변수선택의 효과가 있는 방법이 된다. Lasso의 회귀계수 추정량 \\(\\hat{\\beta}^{L}\\)의 추정 결과는 다음의 변량을 최소화시켜야 한다. \\[\\begin{equation} RSS + \\lambda \\sum_{j=1}^{k}|\\beta_{j}| \\end{equation}\\] \\(\\lambda\\) 는 ridge regression과 같은 역할을 하는 모수이다. \\(\\bullet\\) Lasso 모형 적합을 위한 함수 Lasso는 패키지 glmnet의 함수를 사용해서 적합할 수 있다. 기본적인 적합 함수는 glmnet(x, y)가 되는데, x는 설명변수의 행렬이고, y는 반응변수의 벡터를 지정해야 한다. 최적 lasso 모형을 적합하기 위해서는 최적 \\(\\lambda\\) 값을 선택해서 사용해야 한다. 함수 cv.glmnet(x, y)는 cross-validation으로 최적 \\(\\lambda\\) 값을 추정해 준다. 예제로 앞절에서 사용한 states 자료를 대상으로 lasso 모형을 적합시켜보자. 먼저 설명변수로 이루어진 행렬과 반응변수의 벡터를 생성해 보자. X &lt;- model.matrix(Murder ~ ., states)[,-1] Y &lt;- states$Murder 설명변수의 행렬은 함수 cbind()로 만들 수 있지만, 변수으 개수가 많은 경우에는 불편한 방법이 된다. 함수 model.matrix()는 회귀모형의 design matrix를 생성하는 기능이 있는 함수로써, model.matrix(formula, data)의 형태처럼 lm()과 동일한 방식으로 사용할 수 있다. Design matrix의 첫 번째 열은 모두 1이기 때문에 설명변수의 행렬을 생성하기 위해서는 제외해야 한다. Lasso 모형을 함수 glmnet()으로 적합하고, 회귀계수 추정값의 변화 그래프를 함수 plot()으로 작성해 보자. library(glmnet) fit_L &lt;- glmnet(X, Y) plot(fit_L) 그림 1.14: lambda의 변화에 따른 lasso 회귀계수 추정값의 변화 그림 1.14은 X축에 표시된 L1 Norm인 \\(\\sum_{j=1}^{k}|\\beta_{j}|\\) 의 값 변화에 따른 회귀계수 추정결과의 변화를 선으로 나타낸 그래프이다. L1 Norm의 값이 커진다는 것은 \\(\\lambda\\) 의 값이 작아진다는 것을 의미하는 것이며, \\(\\lambda\\) 의 값이 작아지면 회귀계수의 추정값에 대한 수축 강도가 감소하기 때문에 추정값이 커지게 된다. 그래프 위쪽의 눈금은 0이 아닌 회귀계수의 개수를 표시하는 것이다. 최적 \\(\\lambda\\) 의 값을 함수 cv.glmnet()으로 추정해 보자. set.seed(123) cvfit_L &lt;- cv.glmnet(X, Y) cvfit_L ## ## Call: cv.glmnet(x = X, y = Y) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.1325 34 3.593 0.5818 5 ## 1se 0.4439 21 4.123 0.7045 5 10-fold CV로 mean squared error를 최소화시키는 \\(\\lambda\\) 값을 구하는 함수이다. min에 해당하는 Lambda는 CV MSE를 최소화시키는 \\(\\lambda\\) 값이며, 이 경우 0이 아닌 회귀계수는 5개가 되었다. 1se에 해당하는 Lambda는 최소 CV MSE의 \\(\\pm ~1 \\cdot SE\\) 범위 안에서의 최대 \\(\\lambda\\) 값이 된다. 예측 오차가 최소 CV MSE와는 큰 차이가 없으면서도 가능한 더 많은 회귀계수를 0으로 수축시킬 수 있는 모형에 해당하는 \\(\\lambda\\) 값이 된다. CV 결과를 그래프로 확인해 보자. plot(cvfit_L) 그림 1.15: lambda의 변화에 따른 CV MSE의 변화 왼쪽에서 첫 번째 수직점선이 최소 CV MSE에 해당하는 \\(\\lambda\\) (lambda.min)이고, 두 번째 수직점선이 1SE에 해당하는 \\(\\lambda\\) (lambda.1se)를 표시하고 있다. 최적 \\(\\lambda\\) 값으로 lasso 모형의 회귀계수 추정 결과는 함수 coef()로 확인할 수 있다. s에는 \\(\\lambda\\) 값을 지정하거나, \"lambda.1se\" 또는 \"lambda.min\"을 지정할 수 있다. 디폴트는 \"lambda.1se\"이다. coef(cvfit_L) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.047138e+02 ## Population 1.049949e-04 ## Income . ## Illiteracy 1.134404e+00 ## Life_Exp -1.387603e+00 ## HS_Grad . ## Frost -8.497434e-03 ## Area 1.839126e-06 coef(cvfit_L, s = &quot;lambda.min&quot;) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.155400e+02 ## Population 1.562245e-04 ## Income . ## Illiteracy 1.162400e+00 ## Life_Exp -1.541952e+00 ## HS_Grad . ## Frost -1.216227e-02 ## Area 5.322211e-06 Lasso 모형의 회귀계수 추정 결과는 최소제곱추정에 의한 추정 결과와는 다르며, 회귀계수 추정에 대한 SE도 계산되지 않는다. 적합된 lasso 모형을 활용하는 방식은 두 가지가 될 수 있다. 회귀계수의 추정 결과가 0인 변수는 다중회귀모형에서 제외하는 변수선택의 일환으로 활용하거나, lasso 모형을 그대로 예측모형으로 활용하는 것이다. 1.4 회귀진단 지금까지 우리는 변수선택 과정을 통해 최적모형을 찾는 방법을 살펴보았다. 이제 선택된 모형을 이용하여 반응변수에 대한 예측을 실시하거나 혹은 변수들에 대한 추론 등을 실시할 수 있게 되었다. 그러나 만일 선택된 모형이 회귀모형의 가정을 전혀 만족시키지 못하고 있다면 이러한 예측이나 추론 등은 통계적 신빙성이 전혀 없는 결과가 될 수도 있다. 따라서 선택된 회귀모형이 가정사항을 만족하고 있는지를 확인하는 과정이 필요한데, 이것을 회귀진단이라고 한다. 회귀진단은 일반적으로 회귀모형에 대한 진단과 관찰값에 대한 진단으로 구분해서 진행된다. 1.4.1 회귀모형의 가정 만족 여부 확인 다중회귀모형 \\(y_{i}=\\beta_{0}+\\beta_{1}x_{1i}+\\cdots+\\beta_{ki}x_{ki}+\\varepsilon_{i}\\), \\(i=1,\\ldots,n\\) 에 대한 가정은 다음과 같다. 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 의 평균은 0, 분산은 \\(\\sigma^{2}\\) 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 의 분포는 정규분포 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 은 서로 독립 반응변수와 설명변수의 관계는 선형 오차항에 대한 가정 사항은 잔차 (\\(e_{i}=y_{i}-\\hat{y}_{i}\\))를 이용하여 확인하게 되는데, 그것은 오차항이 관측할 수 없는 대상이기 때문이다. 1.3절에서 예제 자료인 데이터 프레임 states에 대한 변수선택을 시행하였는데, 선택된 모형에 대한 가정만족 여부를 확인해 보자. library(tidyverse) states &lt;- as_tibble(state.x77) |&gt; rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) fit_full &lt;- lm(Murder ~ ., states) fit_s &lt;- MASS::stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) 추정된 회귀모형의 가정 만족 여부를 확인하는 가장 기본적인 절차는 함수 lm()으로 생성된 객체를 함수 plot()에 적용시키는 것이다. 여섯 개의 그래프가 작성되지만 디폴트로 네 개의 그래프가 차례로 그려진다. 하나의 Plots 창에 네 개의 그래프를 한꺼번에 그리기 위해서는 par(mfrow = c(2,2))를 먼저 실행시켜서 그래프 영역을 두 개의 행과 두 개의 열로 분할해야 한다. 회귀모형 fit_s의 가정의 만족 여부를 함수 plot()으로 확인해보자. par(mfrow = c(2, 2)) plot(fit_s) 그림 1.16: 함수 plot()에 의한 회귀진단 그림 1.16의 각 그래프에 3개의 숫자가 표시되어 있는 것을 볼 수 있는데, 이것은 각 그래프에서 가장 극단적인 세 점의 행 번호이다. 원래는 자료로 사용된 데이터 프레임의 행 이름이 표시되는 것인데, 데이터 프레임 states의 경우에는 tibble이어서 행 이름이 없기 때문에 행 번호가 표시된 것이다. 행 이름을 표시하기 위해서는 states를 생성할 때 as_tibble()이 아닌 as.data.frame()을 사용하거나, 함수 plot()의 옵션 labels.id에 행렬 state.x77의 행 이름을 지정하면 된다. 또한 점 모양도 조금 더 명확하게 인식될 수 있는 것으로 바꾸어 보자. Base 그래픽스 함수인 plot()의 경우에 점 모양은 입력 요소 pch로 조절한다. par(mfrow = c(2, 2)) plot(fit_s, pch = 20, labels.id = rownames(state.x77)) 그림 1.17: 함수 plot()에 의한 회귀진단 그림 1.17의 왼쪽 위 패널에 있는 Residuals vs Fitted라는 제목의 그래프는 일반적으로 가장 많이 사용되는 잔차 산점도 그래프로서, 잔차 \\(e_{i}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도이다. 오른쪽 위 패널에 있는 Normal Q-Q라는 제목의 그래프는 표준화잔차 \\(r_{i}\\) 의 정규 분위수-분위수 그래프이다. 왼쪽 아래 패널에 있는 Scale-Location이라는 제목의 그래프는 \\(\\sqrt{|r_{i}|}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도로서 동일 분산 가정의 만족 여부를 확인하는 그래프이다. 마지막으로 오른쪽 아래 패널에 있는 Residuals vs Leverage라는 제목의 그래프는 관찰값의 진단에 사용되는 그래프로, 자세한 설명은 1.4.2절에서 하겠다. Normal Q-Q 그래프와 Scale-Location 그래프에서 사용된 표준화 잔차는 일반 잔차의 퍼짐 정도를 조정한 잔차로써 다음과 같이 정의된다. \\[\\begin{equation} r_{i} = \\frac{e_{i}}{RSE \\sqrt{1-h_{i}}} \\end{equation}\\] 표준화 잔차를 정의하는데 사용된 \\(h_{i}\\)는 leverage라고 불리는 통계량으로써, 설명변수들의 \\(k\\) 차원 공간에서 설명변수의 \\(i\\) 번째 관찰값이 자료의 중심으로부터 떨어져 있는 거리를 표현하는 것으로 볼 수 있다. Leverage 값이 큰 관찰값은 회귀계수의 추정 결과에 큰 영향을 줄 가능성이 높다고 할 수 있다. 그림 1.18는 leverage가 큰 관찰값이 회귀계수 추정 결과에 어떻게 영향을 줄 수 있는지를 보여주는 모의자료를 이용한 예가 된다. 두 그래프에서 빨간 점은 leverage가 높은 관찰값인데, 실선은 빨간 점을 포함하고 추정한 회귀직선이고, 점선은 빨간 점을 제외한 상테에서 추정한 회귀직선이다. 첫 번째 그래프에서만 큰 영향력을 확인할 수 있다. 그림 1.18: leverage가 높은 관찰값의 영향력을 보여주는 모의자료 이제 그림 1.17의 그래프를 중심으로 회귀모형 fit_s의 가정 만족 여부를 확인해 보자. \\(\\bullet\\) 오차항의 동일분산 가정 동일분산 가정의 만족 여부를 확인하는 기본적인 방법은 함수 plot()으로 생성되는 잔차 \\(e_{i}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도와 \\(\\sqrt{|r_{i}|}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도를 확인하는 것이다. 모형 fit_s에 대한 두 종류의 잔차 산점도를 다시 살펴보자. 그림 1.19: 동일분산 가정 확인을 위한 잔차 산점도 그림 1.19의 첫 번째 그래프에서는 Y축이 0인 수평선을 중심으로 점들이 거의 일정한 폭을 유지하며 분포하고 있는지 확인해야 한다. 그림 1.19의 두 번째 그래프에서는 점들이 전체적으로 증가하거나 감소하는 패턴이 있는지 확인해야 한다. 그래프에 추가된 빨간 곡선을 참조하여 판단하는 것이 좋은데, 이것은 로버스트 국소다항회귀에 의해 추정된 비모수 회귀곡선이다. 극단값에 영향을 덜 받으면서 두 변수의 관계를 가장 잘 나타내는 곡선이라고 하겠다. 그림 1.19에서는 분산이 일정하지 않다는 증거를 확인하기 어려워 보인다. \\(\\bullet\\) 오차항의 정규분포 가정 회귀분석에서 이루어지는 검정 및 신뢰구간은 오차항이 정규분포를 한다는 가정에 근거를 두고 이루어진다. 그러나 오차항의 분포가 정규분포의 형태에서 약간 벗어나는 것은 큰 문제를 유발하지 않으며, 표본 크기가 커지면 중심극한정리를 적용할 수 있어서 비정규성은 큰 문제가 되지 않는다. 그러나 오차항의 분포가 Cauchy 분포와 같이 꼬리가 긴 형태의 분포임이 확인된다면 최소제곱법에 의한 회귀계수의 추정보다는 로버스트 선형회귀를 이용하는 것이 더 효과적일 것이다. 정규성의 확인에 사용되는 그래프로는 함수 plot()에서 생성되는 표준화잔차에 대한 정규 분위수-분위수 그래프가 있다. 회귀모형 fit_s의 정규성 가정을 확인해보자. 그림 1.20: 정규분포 가정 확인을 위한 그래프 잔차의 정규 분위수-분위수 그래프에서는 크기순으로 재배열한 잔차가 표본 분위수가 되는데, 표본 분위수 사이의 간격 패턴이 정규분포 이론 분위수 사이의 간격 패턴과 유사하면 점들이 기준선 위에 분포하게 된다. 정규분포 가정에는 문제가 없는 것으로 보인다. \\(\\bullet\\) 오차항의 독립성 가정 수집된 자료가 시간적 혹은 공간적으로 서로 연관되어 있는 경우에는 오차항의 독립성 가정이 만족되지 않을 수 있다. 시간에 흐름에 따라 관측된 시계열 자료나 공간에 따라 관측된 공간 자료를 대상으로 회귀분석을 하는 경우에는 반드시 확인해야 할 가정이 된다. 독립성 가정은 여러 행태로 위반될 수 있는데, 우선 \\(\\varepsilon_{i}\\) 가 \\(\\varepsilon_{i-1}\\) 와 서로 연관되어 있는지 여부, 즉 1차 자기상관관계만을 확인하려면 Durbin-Watson 검정을 실시하면 된다. Durbin-Watson 검정은 패키지 car의 함수 durbinWatsonTest()로 할 수 있으며, 귀무가설은 오차항의 1차 자기상관계수가 0이라는 것이다. Durbin-Watson 검정에서 귀무가설을 기각하지 못했다고 해서 오차항이 독립이라고 바로 결론을 내릴 수는 없는데, 그것은 일반적인 형태의 위반 여부를 확인해야 하기 때문이다. 일반적인 형태의 독립성 위반이란 오차항이 자기회귀이동평균모형, 즉 ARMA(p,q)모형인지 여부를 확인하는 것이다. 이것을 위한 첫 번째 단계는 오차항의 1차 자기상관계수부터 K차 자기상관계수가 모두 0이라는 귀무가설을 검정하는 Breusch-Godfrey 검정을 실시하는 것이다. 이 작업은 패키지 forecast의 함수 checkresiduals()로 할 수 있다. 데이터 프레임 states는 시간적 혹은 공간적으로 연관을 갖기 어려운 방식으로 수집되어 있기 때문에 오차항의 독립성 가정에는 큰 문제가 없는 경우라고 할 수 있다. 대신 살펴볼 자료는 패키지 carData에 있는 데이터 프레임 Hartnagel이다. 이 데이터는 1931년부터 1968년까지의 캐나다 범죄율에 관한 연도별 자료이다. 여성 범죄율(fconvict)을 반응변수로 하고 출산율(tfr)과 여성 고용률(partic)을 설명변수로 하는 회귀모형 fit_H을 적합해 보자. data(Hartnagel, package = &quot;carData&quot;) fit_H &lt;- lm(fconvict ~ tfr + partic, Hartnagel) summary(fit_H) ## ## Call: ## lm(formula = fconvict ~ tfr + partic, data = Hartnagel) ## ## Residuals: ## Min 1Q Median 3Q Max ## -32.944 -12.771 -3.219 5.308 50.514 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 207.092195 36.432869 5.684 2.01e-06 *** ## tfr -0.052305 0.006995 -7.478 9.34e-09 *** ## partic 0.182202 0.090496 2.013 0.0518 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 19.6 on 35 degrees of freedom ## Multiple R-squared: 0.6622, Adjusted R-squared: 0.6429 ## F-statistic: 34.3 on 2 and 35 DF, p-value: 5.654e-09 모형 fit_H에 대한 Durbin-Watson 검정을 실시해 보자. car::durbinWatsonTest(fit_H) ## lag Autocorrelation D-W Statistic p-value ## 1 0.714117 0.5453534 0 ## Alternative hypothesis: rho != 0 1차 자기상관계수는 0.714로 추정되었고, 검정 결과 p-값은 0으로 계산되어 1차 자기상관계수가 0이라는 귀무가설을 기각할 수 있다. 이어서 오차항의 1차 자기상관계수부터 K차 자기상관계수가 모두 0이라는 가설을 검정해 보자. forecast::checkresiduals(fit_H) 그림 1.21: 함수 checkresiduals()로 작성된 그래프 ## ## Breusch-Godfrey test for serial correlation of order up to 8 ## ## data: Residuals ## LM test = 25.188, df = 8, p-value = 0.001444 매우 작은 p-값이 계산되어서 오차항의 1차 자기상관계수부터 8차 자기상관계수가 모두 0이라는 귀무가설을 기각할 수 있다. 함수 checkresiduals()는 잔차에 대한 몇 가지 그래프도 함께 작성한다. 그림 1.21의 위 패널에 작성된 것은 잔차의 시계열 그래프이고, 왼쪽 아래 패널에는 각 시차(lag)별 잔차의 표본 ACF가 작성되어 있으며 오른쪽 아래 패널에는 잔차의 히스토그램이 작성되어 있다. 독립성 가정이 만족되지 않는 경우에 사용할 수 있는 대안으로는 잔차를 대상으로 적절한 ARMA(p,q)모형을 적합시켜 회귀모형에 포함시키는 것을 생각할 수 있다. \\(\\bullet\\) 선형관계 가정 단순회귀모형의 경우 반응변수와 설명변수의 선형관계는 두 변수의 산점도로 충분히 확인할 수 있다. 그러나 다중회귀모형의 경우에는 \\(X_{i}\\) 와 \\(Y\\) 의 산점도 혹은 \\(X_{i}\\) 와 잔차 \\(e\\) 산점도가 큰 의미를 갖지 못하게 되는데, 이것은 회귀모형에 포함된 다른 변수의 영향력을 확인할 수 없기 때문이다. 이러한 경우 부분잔차(partial residual)가 매우 유용하게 사용될 수 있다. 변수 \\(X_{i}\\) 의 부분잔차란 반응변수 \\(Y\\) 에서 모형에 포함된 다른 설명변수의 영향력이 제거된 잔차를 의미하는 것으로써, \\(Y-\\sum_{j \\ne i}\\hat{\\beta}_{j}X_{j}\\) 로 정의할 수 있다. 그런데 \\(Y=\\hat{Y} + e\\) 가 되기 때문에, 부분잔차는 \\(\\hat{Y}+e-\\sum_{j \\ne i}\\hat{\\beta}_{j}X_{j}\\) 로 표현되고, 이어서 \\(e + \\hat{\\beta}_{i}X_{i}\\) 로 정리할 수 있다. 따라서 다중선형회귀모형에서 \\(X_{i}\\) 와 \\(Y\\) 의 선형관계는 \\(X_{i}\\) 와 \\(e + \\hat{\\beta}_{i}X_{i}\\) 의 산점도로 확인할 수 있다. 데이터 프레임 states의 회귀모형 fit_s에 포함된 설명변수와 반응변수의 선형관계를 부분잔차 산점도를 작성하여 확인해 보자. 부분 잔차 산점도는 패키지 car의 crPlots()로 작성할 수 있는데, 이 그래프는 Component + Residual Plots라고도 불린다. car::crPlots(fit_s) 그림 1.22: 선형 가정 확인을 위한 부분잔차 산점도 각 변수의 부분잔차 산점도에는 두 변수의 회귀직선을 나타내는 파란 색의 대시(dashed line)와 국소다항회귀곡선을 나타내는 마젠타(magenta) 색의 실선이 추가되어 있다. 국소다항회귀곡선이 회귀직선과 큰 차이를 보인다면 비선형 관계를 의심할 수 있을 것이다. 그런 경우에는 해당 변수의 제곱항을 모형에 포함시키거나 또는 해당 변수의 적절한 변환이 이루어져야 할 것이다. 회귀모형 fit_s에서는 선형관계에 큰 문제가 없는 것으로 확인된다. \\(\\bullet\\) 다중공선성 다중공선성은 회귀모형의 가정과 직접적인 연관이 있는 것이 아니지만 회귀모형의 추정결과를 해석하는 과정에 큰 영향을 미칠 수 있는 문제가 된다. 즉, 설명변수들 사이에 강한 선형관계가 존재하는 다중공선성의 문제가 생기면 회귀계수 추정량의 분산이 크게 증가하게 되어 결과적으로 회귀계수의 신뢰구간 추정 및 검정에 큰 영향을 미치게 된다. 다중공선성은 분산팽창계수(variance inflation factor)를 계산해 보면 확인할 수 있다. 변수 \\(X_{j}\\) 의 분산팽창계수는 \\(1/(1-R_{j}^{2})\\) 로 계산되는데, 여기에서 \\(R_{j}^{2}\\) 는 \\(X_{j}\\) 를 종속변수로 하고 나머지 설명변수를 독립변수로 하는 회귀모형의 결정계수이다. 따라서 분산팽창계수의 값이 10 이상이 된다는 것은 \\(R_{j}^{2}\\) 의 값이 0.9 이상이라는 것이므로 강한 다중공선성의 존재를 의미한다. 분산팽창계수의 계산은 패키지 car의 함수 vif()로 할 수 있다. 모형 fit_s의 다중공선성 존재 여부를 확인해 보자. car::vif(fit_s) ## Population Illiteracy Life_Exp Frost Area ## 1.171232 2.871577 1.625921 2.262943 1.036358 회귀모형 fit_s에 있는 다섯 설명변수의 분산팽창계수가 모두 큰 값이 아닌 것으로 계산되었고, 따라서 다중공선성의 문제는 없는 것으로 보인다. 1.4.2 특이한 관찰값 탐지 회귀모형의 가정사항 만족 여부를 확인하는 것과 더불어 특이한 관찰값의 존재유무를 확인하는 것도 중요한 회귀진단 항목이 된다. 특이한 관찰값이란 회귀계수의 추정에 과도하게 큰 영향을 미치는 관찰값이나, 추정된 회귀모형으로는 설명이 잘 안 되는 이상값을 의미한다. 영향력이 큰 관찰값을 발견하는 데 필요한 통계량으로는 DFBETAS, DFFITS, Covariance ratio, Cook’s distance와 Leverage 등이 있으며, 이러한 통계량을 근거로 하여 특이한 관찰값 탐지에 사용되는 많은 R 함수가 있다. 그 중 패키지 car의 함수 influencePlot()에 대해 살펴보도록 하자. 예제로써 1.4.1절에서 살펴본 states를 대상으로 적합된 회귀모형 fit_s를 대상으로 특이한 관찰값을 탐지해 보자. 그런데 함수 influencePlot()가 작성하는 그래프에는 가장 극단적인 관찰값의 라벨이 표시되는데, 회귀분석에 사용된 데이터 프레임의 행 이름이 관찰값의 라벨이 된다. 그런데 1.4.1절에서 적합된 회귀모형 fit_s에 사용된 데이터 프레임 states는 tibble이기 때문에 행 이름이 없어서 행 번호가 라벨로 표시가 되며, 따라서 그래프에서 특이한 관찰값을 인식하는 데 불편하게 된다. 이 절에서는 states를 전통적 데이터 프레임으로 전환하여 다시 모형 fit_s를 적합하고, 이어서 특이한 관찰값 탐지 함수에 적용하도록 하겠다. library(tidyverse) states &lt;- as.data.frame(state.x77) |&gt; rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) fit_full &lt;- lm(Murder ~ ., states) fit_s &lt;- MASS::stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) 이상값이란 추정된 회귀모형으로는 설명이 잘 안 되는 관찰값이라고 할 수 있는데, 대부분의 경우 이상값은 큰 잔차를 갖게 된다. 그러나 만일 이상값에 해당되는 관찰값이 영향력도 크다면 그림 1.18의 첫 번째 그래프에서 볼 수 있듯이 회귀계수의 추정을 왜곡시켜 그렇게 크지 않은 잔차를 갖게 될 수도 있다. 따라서 이상값을 판단하고자 한다면 일반적인 잔차보다는 스튜던트화 잔차를 이용하는 것이 더 효과적이라고 하겠다. 함수 influencePlot()은 스튜던트화 잔차와 leverage, 그리고 Cook’s distance를 하나의 그래프에서 같이 보여줌으로써 특이한 관찰값을 분류하는 데 큰 도움이 되는 함수이다. Cook’s distance는 \\(i\\) 번째 관찰값을 포함한 상태에서 추정한 회귀계수 벡터 \\(\\hat{\\boldsymbol{\\beta}}\\) 과 \\(i\\) 번째 관찰값을 제외하고 추정한 회귀계수 벡터 \\(\\hat{\\boldsymbol{\\beta}}_{(i)}\\) 의 통합된 차이를 보여주는 통계량이다. \\[\\begin{equation} COOKD = \\frac{\\left(\\hat{\\boldsymbol{\\beta}}-\\hat{\\boldsymbol{\\beta}}_{(i)} \\right)^{T}\\mathbf{X}^{T}\\mathbf{X}\\left(\\hat{\\boldsymbol{\\beta}}-\\hat{\\boldsymbol{\\beta}}_{(i)} \\right) }{(k+1)RSE} \\end{equation}\\] \\(\\hat{y}_{(i)}\\) 를 \\(i\\) 번째 관찰값을 제외한 나머지 \\((n-1)\\) 개의 자료로 추정된 회귀모형으로 제외된 \\(y_{i}\\) 를 예측한 결과라고 하면, 스튜던트화 잔차 \\(t_{i}\\) 는 \\(y_{i}\\) 와 \\(\\hat{y}_{(i)}\\) 의 표준화된 차이라고 할 수 있다. \\[\\begin{equation} t_{i} = \\frac{y_{i}-\\hat{y}_{(i)}}{SE\\left(y_{i}-\\hat{y}_{(i)}\\right)} \\end{equation}\\] 스튜던트화 잔차의 값이 크다는 것은 해당 관찰값이 이상값으로 분류될 가능성이 높다는 의미가 된다. 이제 회귀모형 fit_s를 함수 influencePlot()에 입력해서 결과를 확인해 보자. car::influencePlot(fit_s) 그림 1.23: 함수 influencePlot()에 의한 회귀진단 그래프 ## StudRes Hat CookD ## Alaska -1.1711577 0.74103486 0.648670758 ## California -0.3054018 0.37300431 0.009442443 ## Maine -2.0697239 0.06978331 0.049840421 ## Nevada 2.3610559 0.18825163 0.195174098 함수 influencePlot()으로 작성된 그래프는 각 관찰값의 leverage를 X축 좌표로, 스튜던트화 잔차를 Y축 좌표로 하는 산점도이며, 점의 크기는 Cook’s distance에 비례하여 결정된다. 관찰값들의 평균 leverage의 두 배와 세 배가 되는 지점에 수직 점선이 추가되며, 스튜던트화 잔차가 \\(\\pm 2\\) 인 지점에 수평 점선이 추가된다. 또한 X축과 Y축에서 가장 극단적인 두 점에는 라벨이 표시된다. 그래프에서 확인할 수 있는 것으로는 우선 Alaska의 경우에 leverage 값과 Cook’s distance의 값이 제일 커서 영향력이 큰 관찰값이라 할 수 있지만, 스튜던트화 잔차는 비교적 크지 않다는 점이다. Nevada의 경우에는 Cook’s distance와 스튜던트화 잔차가 비교적 크지만, leverage는 작은 값을 갖고 있다. 전체적으로, 회귀모형 fit_s에는 문제가 되는 관찰값이 없다고 할 수 있다. 1.5 예측 변수선택과 모형진단 과정 등을 통해 회귀모형이 완성되면 분석의 마지막 단계인 예측을 실행할 수 있게 된다. 예측은 새롭게 주어진 설명변수의 값 \\(\\mathbf{x}_{o}=\\left(1, x_{1o}, \\ldots, x_{ko} \\right)^{T}\\) 에 대하여 반응변수의 값을 예측하는 것인데, 두 가지 방식으로 구분된다. 하나는 반응변수의 평균값에 대한 예측이고 다른 하나는 반응변수의 개별 관찰값에 대한 예측이다. 예를 들어 주택의 매매 가격(\\(Y\\) )와 주택의 특성과 관련된 k개의 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 로 이루어진 회귀모형을 추정하였다고 하자. 새롭게 설명변수의 값이 \\(\\mathbf{x}_{o}\\) 로 주어졌을 경우, 반응변수의 평균 값에 대한 예측이란 주택의 특성이 \\(\\mathbf{x}_{o}\\) 에 해당되는 모든 주택의 평균 매매 가격을 예측하는 것이고, 반응변수의 개별 관찰값에 대한 예측이란 주택의 특성이 \\(\\mathbf{x}_{o}\\) 인 어느 특정 주택의 매매 가격을 예측하는 것이다. 두 가지 상황에서 예측 결과는 \\(\\hat{y}_{o} = \\mathbf{x}_{o}^{T} \\hat{\\boldsymbol{\\beta}}\\) 으로 동일하지만 예측 오차는 개별 관찰값에 대한 예측의 경우가 더 큰 값을 갖게 된다. \\(\\bullet\\) 함수 predict()에 의한 예측 반응변수의 예측은 함수 predict()로 할 수 있다. 기본적인 사용법은 predict(object, newdata, interval = c(\"none\", \"confidence\", \"prediction\"), level = 0.95)이다. object는 예측에 사용될 회귀모형의 lm 객체를 의미하며, newdata는 새롭게 주어지는 설명변수의 값으로 반드시 데이터 프레임으로 입력되어야 한다. 옵션 interval은 예측에 대한 신뢰구간을 계산하는 것으로 반응변수의 평균값에 대한 예측인 경우에는 \"confidence\", 반응변수의 개별 관찰값에 대한 예측인 경우에는 \"prediction\"을 선택해야 하며, 디폴트는 \"none\"이다. \\(\\bullet\\) 데이터 분리 회귀모형의 예측 결과는 반드시 평가를 받아야 한다. 모형의 예측력을 평가할 수 있는 방법으로 전체 데이터를 training data와 test data로 분리하는 것을 생각할 수 있다. Training data를 대상으로 변수선택 및 진단과정을 거쳐 최종모형을 선택하고, 이어서 선택된 모형을 사용하여 test data를 대상으로 예측을 실시하는 것이다. Test data에는 반응변수의 값이 포함되어 있으므로 예측 오차를 계산할 수 있다. 데이터를 분리하는 작업은 dplyr::slice_sample()로도 할 수 있으나, 패키지 caret의 함수 createDataPartition()을 사용하도록 하겠다. 패키지 caret에는 Machine Learning 작업을 위한 다양한 함수가 마련되어 있다. 패키지에 대한 자세한 소개는 생략하겠으나 Machine Learning 분야에 관심이 있는 사용자는 눈여겨봐야 할 패키지이다. 비록 Machine Learning의 중심이 최근에 출시되고 있는 tidymodels로 점차 옮겨가고 있으나 아직 중요한 역할을 담당하고 있다. 함수 createDataPartition()의 기본적인 사용법은 createDataPartition(y, p = 0.5, list = TRUE)이다. y에는 반응변수를 벡터 형태로 지정해야 하고, p에는 training data로 분리될 자료의 비율을 지정한다. list는 training data로 선택된 행 번호를 리스트 형태로 출력할 것인지를 지정하는 것이며, FALSE를 지정하면 행렬 형태로 출력된다. \\(\\bullet\\) 예제: state.x77에 대한 예측 모형 설정 행렬 state.x77의 변수 Murder을 반응변수로, 나머지 변수를 설명변수로 하는 회귀모형을 설정하고 예측을 실시해 보자. 자료분리는 training data로 80%, test data로는 20%가 되도록 하자. 먼저 데이터 프레임 states를 생성해 보자. library(tidyverse) states &lt;- as.data.frame(state.x77) |&gt; rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) 자료분리를 실시해 보자. 난수 추출에서 seed를 지정한 이유는 같은 결과를 보기 위함이다. library(caret) set.seed(1234) x.id &lt;- createDataPartition(states$Murder, p = 0.8, list = FALSE)[,1] train_s &lt;- states |&gt; slice(x.id) test_s &lt;- states |&gt; slice(-x.id) Training data를 대상으로 최적모형을 선택하고, 그 결과를 확인해 보자. fit_f &lt;- lm(Murder ~ ., train_s) fit_s &lt;- MASS::stepAIC(fit_f, direction = &quot;both&quot;, trace = FALSE) summary(fit_s) ## ## Call: ## lm(formula = Murder ~ Population + Life_Exp + Frost + Area, data = train_s) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.4060 -1.1697 0.1592 1.0078 3.2016 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.408e+02 1.439e+01 9.781 8.36e-12 *** ## Population 1.445e-04 6.192e-05 2.333 0.025186 * ## Life_Exp -1.862e+00 2.037e-01 -9.141 5.01e-11 *** ## Frost -2.492e-02 5.940e-03 -4.195 0.000164 *** ## Area 6.905e-06 3.068e-06 2.251 0.030426 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.754 on 37 degrees of freedom ## Multiple R-squared: 0.8032, Adjusted R-squared: 0.7819 ## F-statistic: 37.75 on 4 and 37 DF, p-value: 1.383e-12 모형진단을 실시해 보자. par(mfrow = c(2,2)) plot(fit_s, pch = 20) 그림 1.24: 모형 fit_s의 모형진단 그래프 car::vif(fit_s) ## Population Life_Exp Frost Area ## 1.155550 1.051388 1.199224 1.029173 예측모형으로 사용하는 데 큰 문제는 없는 것으로 보인다. 이제 test data에 대한 예측을 실시해 보자. pred_s &lt;- predict(fit_s, newdata = test_s) 예측에 대한 평가는 패키지 caret의 함수 defaultSummary()로 할 수 있다. 이 함수에는 test data의 반응변수와 예측 결과를 각각 obs와 pred라는 이름의 열로 구성한 데이터 프레임을 만들어서 입력하면 된다. defaultSummary(data.frame(obs = test_s$Murder, pred = pred_s)) ## RMSE Rsquared MAE ## 1.7758976 0.7475613 1.4721779 예측오차를 \\(e_{i}\\) 라고 하면, RMSE는 \\(\\sqrt{mean\\left( e_{i}^{2} \\right)}\\), MAE는 \\(mean(|e_{i}|)\\), Rsquared는 반응변수와 예측 결과의 상관계수의 제곱으로 계산된다. 예측오차에 대한 요약통계량 값을 보면 전반적으로 무난한 예측이 이루어진 것으로 보인다. 또한 summary(fit_s)에서 확인할 수 있는 training data에 대한 RMSE와 Rsquared의 값과 비교해서도 크게 떨어지지 않는 것으로 보아 overfitting의 문제도 없는 것으로 보인다. Test data의 반응변수와 모형 fit_s의 예측 결과를 산점도로 나타내어 보자. data.frame(obs = test_s$Murder, pred = pred_s) |&gt; rownames_to_column(var = &quot;state&quot;) |&gt; ggplot(aes(x = obs, y = pred)) + geom_point() + geom_abline(intercept = 0, slope = 1) + geom_text(aes(label = state), nudge_x = 0.3, nudge_y = 0.2) 그림 1.25: state.x77 자료의 예측 결과 그래프 1.6 실습 예제 "],["chapter-Logistic.html", "2 장 로지스틱 회귀모형 2.1 이항반응변수에 대한 선형회귀모형의 한계 2.2 로지스틱 회귀모형 2.3 변수선택 2.4 회귀진단 2.5 분류성능 평가 2.6 희귀사건의 분류 2.7 실습 예제", " 2 장 로지스틱 회귀모형 선형회귀모형에서는 반응변수가 연속형 변수임을 가정하고 있다. 그러나 범주형 변수를 반응변수로 하여 회귀모형을 설정해야 하는 경우도 많이 있는데, 이런 상황에서 선형회귀모형을 그대로 사용해도 괜찮은지 여부를 확인할 필요가 있으며, 만일 문제가 있다면 대안으로 사용할 수 있는 모형이 무엇인지 알아보아야 한다. 범주형 변수가 “성공”, “실패”와 같이 2개의 범주만을 갖는 경우에는 이항변수, 범주의 개수가 3개 이상인 경우에는 다항변수라고 부른다. 이 장에서는 이항변수를 반응변수로 설정해야 하는 경우에 적용할 수 있는 로지스틱 회귀모형에 대하여 살펴보겠다. 2.1 이항반응변수에 대한 선형회귀모형의 한계 반응변수 \\(Y\\) 와 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 로 구성된 선형회귀모형은 다음과 같이 표현된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1} + \\cdots + \\beta_{k}X_{k} + \\varepsilon \\tag{2.1} \\end{equation}\\] 단, \\(\\varepsilon \\sim N(0, \\sigma^{2})\\). 반응변수의 조건부 기대값은 다음과 같다. \\[\\begin{equation} E(Y|X)=\\beta_{0}+\\beta_{1}X_{1}+\\cdots+\\beta_{k}X_{k} \\tag{2.2} \\end{equation}\\] 이항반응변수 \\(Y\\) 는 “성공” 범주에 속하면 1, “실패” 범주에 속하면 0을 값으로 갖는 것이 일반적으로 적용되는 방식이다. 두 개의 값만을 가질 수 있는 이항반응변수를 모형 (2.1)으로 설명할 때 발생할 수 있는 첫 번째 문제는 오차항 \\(\\varepsilon\\) 이 \\(N(0,\\sigma^{2})\\) 의 분포를 한다는 가정을 만족시킬 수 없다는 것이다. 두 번째 문제는 반응변수의 조건부 기대값 \\(E(Y|X)\\) 와 설명변수의 선형결합인 \\(\\beta_{0}+\\beta_{1}X_{1}+\\cdots+\\beta_{k}X_{k}\\) 의 범위가 일치하지 않는다는 것이다. 이항반응변수 \\(Y\\) 는 베르누이 분포를 따른다고 할 수 있는데, 이 경우 조건부 기대값은 다음과 같이 계산된다. \\[\\begin{align*} E(Y|X) &amp;= 1 \\cdot P(Y=1|X) + 0 \\cdot P(Y=0|X) \\\\ &amp;= P(Y=1|X) \\end{align*}\\] 즉, 이항반응변수의 조건부 기대값은 반응변수가 1이 될 확률이 되며, 따라서 범위는 \\((0,1)\\) 이 된다. 반면에 설명변수가 취할 수 있는 값에 특별한 제약조건이 없다면 설명변수의 선형결합 \\(\\beta_{0}+\\beta_{1}X_{1}+\\cdots+\\beta_{k}X_{k}\\) 의 범위는 \\((-\\infty, \\infty)\\) 가 되기 때문에, \\(E(Y|X)\\) 와 설명변수의 선형결합이 취하는 값의 범위는 일치하지 않게 된다. 선형회귀모형을 사용하여 이항반응변수가 “성공” 범주에 속할 확률을 추정하게 되면, 어떠한 문제가 발생할 수 있는지 예제를 통해서 살펴보도록 하자. \\(\\bullet\\) 예제 : ISLR::Default 패키지 ISLR의 데이터 프레임 Default는 신용카드 사용 금액을 매달 갚아가는 상황에 대한 모의자료이다. 변수 default는 연체 여부를 나타내는 요인으로 \"No\"와 \"Yes\"의 두 범주를 갖고 있는 이항반응변수이다. student는 학생 여부에 대한 요인이고, balance는 카드 사용 금액 중 갚고 남은 잔액, income은 소득을 나타낸다. data(Default, package = &quot;ISLR&quot;) str(Default) ## &#39;data.frame&#39;: 10000 obs. of 4 variables: ## $ default: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ student: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 2 1 1 1 2 1 2 1 1 ... ## $ balance: num 730 817 1074 529 786 ... ## $ income : num 44362 12106 31767 35704 38463 ... 변수 default의 값을 \"No\"는 0으로, \"Yes\"는 1로 바꾸고, default를 반응변수, balance를 설명변수로 하는 선형회귀모형을 적합하여 그 결과를 두 변수의 산점도에 함께 표시해 보자. 요인 default를 함수 as.numeric()을 사용하여 숫자형으로 변환시키면 첫 번째 범주인 \"No\"가 1로, 두 번째 범주인 \"Yes\"가 2로 변하게 된다. 여기에 1을 더 빼서 \"No\"를 0으로, \"Yes\"를 1로 변환하였다. library(tidyverse) Default |&gt; ggplot(aes(x = balance, y = as.numeric(default)-1)) + geom_jitter(height = 0.005, width = 100) + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(y = &quot;Probability of Default&quot;) 그림 2.1: 이항반응변수를 선형회귀모형으로 적합한 결과 그림 2.1에 표시된 파란색 회귀직선은 주어진 balance의 값에서 default가 1이 될 확률을 선형회귀모형으로 추정한 결과이다. balance가 매우 작은 값을 갖는 영역에서 default가 1이 될 확률이 음으로 추정된 것을 볼 수 있다. 또한 회귀직선이 점들을 거의 설명하지 못하고 있음도 확인할 수 있다. 이항반응변수에 대해서는 적절하지 않은 모형으로 보인다. 그림 2.2는 로지스틱 회귀모형으로 default가 1이 될 확률을 추정한 결과이다. 선형회귀모형이 갖고 있던 두 가지 문제가 모두 해결된 것을 볼 수 있다. Default |&gt; ggplot(aes(x = balance, y = as.numeric(default)-1)) + geom_jitter(height = 0.005, width = 100) + geom_smooth(method = &quot;glm&quot;, method.args = list(family = binomial), se = FALSE) + labs(y = &quot;Probability of Default&quot;) 그림 2.2: 이항반응변수를 로지스틱 회귀모형에 적합한 결과 함수 geom_smooth()에서 사용된 method = \"glm\"과 family = binomial에 대한 설명은 로지스틱 회귀모형의 적합에 사용되는 함수 glm()에 대한 소개에서 진행하겠다. 2.2 로지스틱 회귀모형 선형회귀모형을 사용하여 변수 default가 1이 될 확률을 추정한 결과에서 음의 값이 나온 이유는 2.1절에서 살펴본 데로 설명변수가 취할 수 있는 값의 범위가 반응변수의 기댓값이 취할 수 있는 값의 범위와 다르기 때문이다. 즉, 선형회귀모형에서는 \\(E(Y|X) = P(Y=1|X) = \\beta_{0} + \\beta_{1}X\\) 로 설정되는데, \\(0 \\leq P(Y=1|X) \\leq 1\\) 이고, \\(-\\infty &lt; \\beta_{0}+\\beta_{1}X &lt; \\infty\\) 이기 때문에 서로 범위가 맞지 않는다. 이 문제는 \\(P(Y=1|X)\\) 에 적절한 변환을 실시함으로써 해결을 할 수 있다. 먼저 반응변수 \\(Y\\) 가 1이 될 사건에 대한 odds를 생각해 보자. Odds의 범위는 0에서 무한대가 된다. \\[\\begin{equation} 0 \\leq \\frac{P(Y=1|X)}{1-P(Y=1|X)} &lt; \\infty \\end{equation}\\] 이어서 \\(Y\\) 가 1이 될 사건에 대한 log odds 또는 logit 변환을 생각해 보자. 설명변수의 선형결합과의 범위 불일치 문제가 해결되었음을 알 수 있다. \\[\\begin{equation} -\\infty &lt; \\log \\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) &lt; \\infty \\end{equation}\\] \\(Y\\) 가 1이 될 사건에 대한 logit 변환을 적용한 로지스틱 회귀모형은 다음과 같이 정의된다. \\[\\begin{equation} \\log \\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} \\tag{2.3} \\end{equation}\\] 반응변수가 1이 될 확률은 모형 (2.3)의 inverse logit 변환으로 다음과 같이 표현된다. \\[\\begin{equation} P(Y=1|X) = \\frac{e^{\\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}}}{1+e^{\\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}}} \\end{equation}\\] \\(\\bullet\\) 로지스틱 회귀식의 특징 설명변수가 한 개인 로지스틱 회귀모형을 대상으로 절편과 기울기를 나타내는 회귀계수의 역할을 살펴보자. 예를 들어, 반응변수가 1이 될 확률이 \\(P(Y=1|X)=e^{b0+b1X}/(1+e^{b0+b1X})\\) 로 표현된다고 하자. 그림 2.3의 첫 번째 그래프는 \\(b1=1\\) 로 고정하고, \\(b0\\) 의 값을 \\(-4, ~0, ~4\\) 로 변화시켰을때 \\(P(Y=1|X)\\) 의 값을 나타낸 것이고, 두 번째 그래프는 \\(b0 = 0\\) 으로 고정하고, \\(b1\\)의 값을 \\(0.25, ~ 0.5, ~1\\) 로 변화시켰을때 \\(P(Y=1|X)\\) 의 값을 나타낸 것이다. 그림 2.3: 로지스틱 회귀계수의 역할 로지스틱 회귀곡선은 절편이 증가함에 따라 왼쪽으로 이동되는 것을 알 수 있는데, 이것은 고정된 설명변수의 수준에서 절편이 증가함에 따라 반응변수가 1이 될 확률이 증가하는 것을 보여준다. 또한 기울기가 증가함에 따라 반응변수가 1이 될 확률이 급격하게 증가하는 것을 볼 수 있다. 2.2.1 모형 적합 회귀모형의 적합이란 모형 (2.3)에 포함된 회귀계수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 의 추정을 의미한다. 반응변수 \\(Y\\) 가 0 또는 1을 값으로 갖는 베르누이 분포를 하고 있기 때문에 회귀계수는 maximum likelihood estimation으로 추정을 할 수 있는데, 다만 우도함수의 형태가 방정식을 유도하여 회귀계수의 값을 구할 수 있는 형태가 아니기 때문에 수치적인 반복 계산으로 추정하게 된다. 설정된 모형에 큰 문제가 없다면 대부분의 경우 몇 번의 반복으로 모수를 추정할 수 있다. 그러나 여러 번의 반복 계산을 수행하여도 수렴 기준을 충족하지 못해 결국 추정에 실패하는 경우도 있는데, 그 이유 중에 하나가 complete separation이다. Complete separation은 특정 설명변수의 선형결합으로 반응변수의 값이 완벽하게 분리되는 상태를 의미하는 것인데, 예를 들어 설명변수 \\(X\\) 가 10 미만의 값을 가지면 반응변수 \\(Y\\) 는 항상 0이 되고, \\(X\\) 가 10 이상의 값을 가지면 \\(Y\\) 는 항상 1이 된다면, 설명변수 \\(X\\) 로 반응변수의 두 그룹이 완벽하게 분리되는 것이다. Complete separation이 발생할 수 있는 몇 가지 상황이 있는데, 먼저 범주형 설명변수의 특정 범주에 대해 반응변수가 모두 0 또는 1의 값을 갖는 경우이다. 예를 들어, 반응변수가 나이와 밀접하게 연관된 질환의 유무이고 설명변수에 나이를 그룹으로 구분한 범주형 변수가 포함되면, 특정 나이 그룹에서는 모두 질환이 있는 것으로 나타나지만, 질환이 전혀 없는 나이 그룹이 나타날 수 있는 것이다. 또한 희귀 사건을 반응변수로 다루고 있는 경우와 표본의 크기가 작은 경우에도 complete separation이 발생할 수 있다. 로지스틱 회귀모형의 적합은 일반화선형모형(generalize linear model) 적합을 위한 함수 glm()으로 할 수 있다. 기본적인 사용법은 glm(formula, family = binomial, data)이다. formula에는 ‘반응변수 ~ 설명변수’ 형식의 공식이 지정되는데, 반응변수의 유형은 0 또는 1을 값으로 갖는 숫자형 벡터이거나 요인이어야 한다. 요인인 경우에는 첫 번째 범주가 ‘실패’, 두 번째 범주가 ‘성공’으로 분류되어 ‘성공’ 확률을 추정하게 된다. family는 일반화선형모형에서 반응변수의 분포 및 link function을 지정하는 것으로서 로지스틱 회귀모형의 경우에는 binomial을 지정해야 한다. \\(\\bullet\\) 예제: ISLR::Default 2.1절에서 살펴본 Default 자료에서 default를 반응변수, balance와 student, 그리고 income을 설명변수로 하며, default가 두 번째 범주인 \"Yes\"과 되 홧률에 대한 로지스틱 회귀모형을 적합해 보자. 단, income은 $1,000 단위로 조절한다. 먼저 산점도행렬을 작성해서 변수들 사이의 관계를 살펴보자. 변수 default가 \"Yes\"인 경우가 매우 드물다는 것을 알 수 있다. 또한 default가 \"Yes\"와 \"No\"인 그룹에서 balance의 분포는 큰 차이를 보이는 반면에 income은 거의 비슷한 분포를 갖고 있다. data(Default, package = &quot;ISLR&quot;) Default &lt;- Default |&gt; mutate(income = income/1000) GGally::ggpairs(Default, aes(fill = default), showStrips = TRUE) 그림 2.4: Default 자료의 산점도행렬 이제 로지스틱 회귀모형을 적합해 보자. 모형의 주요한 적합 결과는 함수 summary()로 확인할 수 있다. fit &lt;- glm(default ~ ., family = binomial, Default) summary(fit) ## ## Call: ## glm(formula = default ~ ., family = binomial, data = Default) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.087e+01 4.923e-01 -22.080 &lt; 2e-16 *** ## studentYes -6.468e-01 2.363e-01 -2.738 0.00619 ** ## balance 5.737e-03 2.319e-04 24.738 &lt; 2e-16 *** ## income 3.034e-03 8.203e-03 0.370 0.71152 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 2920.6 on 9999 degrees of freedom ## Residual deviance: 1571.5 on 9996 degrees of freedom ## AIC: 1579.5 ## ## Number of Fisher Scoring iterations: 8 개별 회귀계수에 대한 유의성 검정 결과를 볼 수 있다. 가변수가 사용된 요인 student에 대해서는 \"No\" 그룹과 \"Yes\" 그룹 사이에 유의적인 차이가 있는 것으로 나타났고, balance도 유의적인 변수로 나타났다. 마지막 부분에 deviance가 계산되어 있는데, deviance는 일반화선형모형의 적합도를 측정할 때 사용되는 통계량으로서 추정된 모형의 적합값과 실제 관찰값의 일치 정도를 측정한다고 할 수 있다. Null deviance는 절편만 있는 모형과 주어진 자료를 완전하게 설명하는 완전모형(saturated model)과의 적합도 차이이고, residual deviance는 현재의 모형과 완전모형의 적합도 차이이다. 따라서 두 deviance의 차이는 현재 모형에 포함된 설명변수가 모형의 적합도 향상에 어느 정도 기여했는지를 측정할 수 있는 도구가 된다. 즉, 만일 현재 모형에 \\(k\\) 개의 설명변수가 포함되어 있다면, 현재 모형의 유의성 검정인 \\(H_{0}:\\beta_{1} = \\cdots = \\beta_{k} = 0\\) 에 대한 검정통계량으로 사용될 수 있다는 것이다. 두 deviance의 차이는 귀무가설이 사실일 때 \\(\\chi^{2}\\)분포를 하고, 자유도는 두 모형을 구성하는 모수 개수의 차이이다. 현재 모형의 유의성 검정은 함수 anova()를 사용하여 다음의 방식으로도 실시할 수 있다. 귀무가설은 기각되며, 따라서 3개의 설명변수 중 적어도 하나는 유의한 변수라는 의미가 된다. fit_null &lt;- glm(default ~ 1, family = binomial, Default) fit &lt;- glm(default ~ ., family = binomial, Default) anova(fit_null, fit, test = &quot;Chisq&quot;) ## Analysis of Deviance Table ## ## Model 1: default ~ 1 ## Model 2: default ~ student + balance + income ## Resid. Df Resid. Dev Df Deviance Pr(&gt;Chi) ## 1 9999 2920.7 ## 2 9996 1571.5 3 1349.1 &lt; 2.2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 \\(\\bullet\\) 설명변수의 효과 분석 개별 설명변수의 효과 분석은 로지스틱 회귀모형에서도 중요한 의미를 가지고 있으나, 선형회귀모형의 경우와는 조금 다른 방식으로 이루어져야 한다. 선형회귀모형은 다른 변수가 고정된 상태에서 \\(X_{j}\\) 를 한 단위 변화시키면 \\(E(Y|X)\\) 가 \\(\\beta_{j}\\) 만큼 변동되는 구조를 가지고 있다. 반면에 로지스틱 회귀모형은 개별 설명변수의 효과를 odds ratio로 나타내야 하는 구조를 가지고 있다. 즉, 로지스틱 회귀모형은 log odds에 대한 모형이다. \\[\\begin{align*} \\log \\left(\\frac{P(Y=1|X)}{1-P(Y=1|X)}\\right) &amp;= \\log \\Omega (X) \\\\ &amp;= \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} \\end{align*}\\] 이것을 odds에 대한 모형으로 변환시켜 보자. \\[\\begin{align*} \\Omega(X) &amp;= e^{\\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k}} \\\\ &amp;= e^{\\beta_{0}} e^{\\beta_{1}X_{1}} \\cdots e^{\\beta_{k}X_{k}} \\end{align*}\\] 다른 변수는 모두 고정시키고 \\(X_{j}\\) 만 \\(\\delta\\) 만큼 변화시킬 때의 odds는 다음과 같이 표현된다. \\[\\begin{equation} \\Omega(X; X_{j}+\\delta) = e^{\\beta_{0}} e^{\\beta_{1}X_{1}} \\cdots e^{\\beta_{j}(X_{j}+\\delta)} \\cdots e^{\\beta_{k}X_{k}} \\end{equation}\\] 다른 변수는 모두 고정시키고 \\(X_{j}\\) 만 \\(\\delta\\) 만큼 변화시킬 때의 odds의 변화 정도는 \\(\\Omega(X)\\) 와 \\(\\Omega(X; X_{j}+\\delta)\\) 의 비율인 odds ratio로 나타낼 수 있다. \\[\\begin{equation} \\frac{\\Omega(X; X_{j}+\\delta)}{\\Omega(X)} = e^{\\beta_{j}\\delta} \\end{equation}\\] Odds ratio가 1보다 작은 값을 갖는 경우에는 해당 설명변수의 값을 증가시키면 반응변수가 ‘성공’ 범주가 될 odds가 감소하는 것이고, 1보다 큰 값을 갖는 경우에는 ‘성공’ 범주가 될 odds가 증가하는 것이 된다. Default 자료에서 적합된 로지스틱 회귀모형 fit에 포함된 각 설명변수의 odds ratio를 구해보자. fit을 함수 coef()에 입력해서 회귀계수를 추출하고, 이어서 exp()로 변환시키면 odds ratio가 계산된다. coef(fit) |&gt; exp() |&gt; round(6) ## (Intercept) studentYes balance income ## 0.000019 0.523732 1.005753 1.003038 변수 balance의 값이 $1 증가하면 default가 \"Yes\"가 될 odds가 0.57% 증가하는 것으로 추정되며, student의 경우에는 \"Yes\" 그룹이 \"No\" 그룹에 비해 default가 \"Yes\"가 될 odds를 (1-0.5237)*100%, 즉 47.63% 감소시키는 것으로 나타났다. Odds ratio에 대한 신뢰구간은 fit을 함수 confint()에 입력해서 회귀계수의 신뢰구간을 계산하고, 이어서 exp()로 변환시키면 odds ratio에 대한 신뢰구간이 계산된다. 함수 confint()의 디폴트 level은 0.95이다. confint(fit) |&gt; exp() |&gt; round(6) ## 2.5 % 97.5 % ## (Intercept) 0.000007 0.000049 ## studentYes 0.329883 0.833422 ## balance 1.005309 1.006224 ## income 0.987038 1.019309 odds ratio(\\(e^{\\beta_{j}}\\))의 신뢰구간에 1이 포함된다는 것은 회귀계수(\\(\\beta_{j}\\))의 신뢰구간에 0이 포함되는 것과 동일한 것이다. 따라서 변수 income은 5% 유의수준에서 비유의적이다. 2.2.2 확률 예측 로지스틱 회귀모형은 반응변수가 ‘성공’ 범주에 속할 확률을 추정하기 위한 회귀모형이다. 회귀계수가 추정되면 \\(\\pi(\\mathbf{x}) = P(Y=1|\\mathbf{x})\\) 을 다음과 같이 추정할 수 있다. \\[\\begin{equation} \\hat{\\pi}(\\mathbf{x}) = \\frac{e^{\\hat{\\beta}_{0} + \\cdots + \\hat{\\beta}_{k}x_{k}}}{1+e^{\\hat{\\beta}_{0} + \\cdots + \\hat{\\beta}_{k}x_{k}}} \\end{equation}\\] 확률 예측은 함수 predict()로 할 수 있으며, 기본적인 사용법은 predict(object, newdata, type = \"response\")이다. object에는 적합한 로지스틱 모형에 대한 glm 객체를 지정하고, newdata에는 예측에 사용될 설명변수의 새로운 자료를 데이터 프레임의 형태로 지정한다. type은 예측의 유형을 지정하는 것으로서 반응변수가 ‘성공’ 범주에 속할 확률을 예측하고자 하는 경우에는 반드시 \"response\"를 지정해야 한다. \\(\\bullet\\) 예제: ISLR::Default 2.2.1절의 예제에서 적합한 모형 fit을 대상으로 balance의 값이 $1,500이고, income이 $40,000, 그리고 student가 \"Yes\"와 \"No\"인 경우에 대하여 각각 default가 \"Yes\"일 확률을 추정해 보자. 우선 예측에 사용될 설명변수의 새로운 데이터 프레임 new_df를 다음과 같이 설정한다. new_df &lt;- tibble(student = c(&quot;Yes&quot;,&quot;No&quot;), balance = 1500, income = 40) 이제 모형 fit을 대상으로 new_df에 대한 예측을 실시해 보자. predict(fit, newdata = new_df, type = &quot;response&quot;) ## 1 2 ## 0.05788194 0.10499192 이번에는 모형 추정에 사용된 기존의 자료를 대상으로 default가 Yes가 될 확률을 추정해서 그래프로 나타내 보자. 함수 predict()에서 newdata를 생략하면 기존 자료에 대한 확률이 예측된다. Default |&gt; mutate(pred = predict(fit, type = &quot;response&quot;)) |&gt; ggplot(aes(x = balance, y = as.numeric(default)-1)) + geom_jitter(height = 0.005, width = 100) + geom_line(aes(x = balance, y = pred, color = student), linewidth = 1) + labs(y = &quot;Probability of default&quot;) 그림 2.5: student와 balance에 따른 default 확률 변수 balance의 값이 증가하면 default가 \"Yes\"가 될 확률이 증가하고 있다. 또한 주어진 balance 값에 대하여 student가 \"Yes\"인 그룹이 \"No\"인 그룹에 비하여 default가 \"Yes\"가 될 확률이 낮음을 알 수 있다. 2.3 변수선택 선형회귀모형의 경우와 마찬가지로 로지스틱 회귀모형에서도 ‘최적’ 변수로 이루어진 모형을 찾는 것은 매우 중요한 작업이다. 1.3절에서 살펴본 선형회귀모형의 평가측도에 의한 best subset selection과 stepwise selection, 그리고 lasso 모형의 적용 과정 등이 로지스틱 회귀모형에도 큰 차이 없이 적용된다. 다만 모형의 차이로 인하여 사용할 수 있는 평가 측도에 차이가 있는데, AIC, BIC와 2.5절에서 살펴볼 accuracy, F1 score, AUC 등의 분류성능 평가에 관련된 측도가 로지스틱 회귀모형에 적용할 수 있는 평가 측도가 된다. 변수선택에 사용될 함수는 stepwise selection과 lasso 모형의 경우에는 선형회귀모형의 경우와 동일하지만, best subset selection의 경우에는 bestglm::bestglm()를 사용해야 한다. 기본적인 사용법은 bestglm(Xy, family = binomial, IC = c(\"BIC\", \"AIC\"))이다. Xy는 반응변수와 설명변수로 구성된 데이터 프레임인데, 반응변수가 마지막 열에 위치해야 한다. IC는 변수선택 기준으로 사용되는 통계량으로서 \"BIC\"가 디폴트로 사용된다. \\(\\bullet\\) 예제 carData::Mroz Mroz는 미국 753명 여성을 대상으로 직업 참여율과 관련된 8개 항목을 조사한 자료이다. data(Mroz, package = &quot;carData&quot;) str(Mroz) ## &#39;data.frame&#39;: 753 obs. of 8 variables: ## $ lfp : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ k5 : int 1 0 1 0 1 0 0 0 0 0 ... ## $ k618: int 0 2 3 3 2 0 2 0 2 2 ... ## $ age : int 32 30 35 34 31 54 37 54 48 39 ... ## $ wc : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 2 1 2 1 1 1 ... ## $ hc : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ lwg : num 1.2102 0.3285 1.5141 0.0921 1.5243 ... ## $ inc : num 10.9 19.5 12 6.8 20.1 ... 반응변수 lfp는 직업 유무를 나타내는 요인이고, 설명변수 k5는 5세 이하 자녀의 수, k618은 6세에서 18세 사이 자녀의 수, age는 여성의 나이, wc는 여성의 대학과정 등록 여부, hc는 남편의 대학과정 등록 여부, lwg는 여성의 기대 수입의 로그값, inc는 여성을 제외한 가구원의 총소득이다. Best subset selection 함수 bestglm::bestglm()을 사용하여 설명변수의 모든 가능한 조합에 대하여 AIC 또는 BIC를 기준으로 모형을 선택해 보자. Mroz에 적용하기 위해서 먼저 반응변수 lfp를 마지막 열로 이동한 데이터 프레임 Xy를 다음과 같이 생성하자. Xy &lt;- Mroz |&gt; relocate(lfp, .after = last_col()) 이제 AIC와 BIC를 기준으로 각각 변수선택을 실시해 보자. library(bestglm) fit_bic &lt;- bestglm(Xy, family = binomial) fit_aic &lt;- bestglm(Xy, family = binomial, IC = &quot;AIC&quot;) BIC에 의한 선택 결과를 확인해 보자. fit_bic의 요소 Subsets에는 설명변수의 개수가 \\(i=0, \\ldots, k\\) 인 모형 중 최적 모형의 적합결과가 입력되어 있으며, 그 중 BIC가 가장 작은 ‘최적’ 모형은 변수 개수에 별표가 추가되어 있다. fit_bic$Subsets ## Intercept k5 k618 age wc hc lwg inc logLikelihood BIC ## 0 TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE -514.8732 1029.7464 ## 1 TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE -497.3750 1001.3741 ## 2 TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE -482.2418 977.7317 ## 3 TRUE TRUE FALSE TRUE FALSE FALSE TRUE FALSE -468.8080 957.4882 ## 4 TRUE TRUE FALSE TRUE TRUE FALSE FALSE TRUE -462.1527 950.8016 ## 5* TRUE TRUE FALSE TRUE TRUE FALSE TRUE TRUE -453.2277 939.5758 ## 6 TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE -452.7801 945.3046 ## 7 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE -452.6330 951.6344 요소 BestModel에는 lm 객체 형태로 최적 모형의 적합 결과가 입력되어 있다. fit_bic$BestModel ## ## Call: glm(formula = y ~ ., family = family, data = Xi, weights = weights) ## ## Coefficients: ## (Intercept) k5 age wcyes lwg inc ## 2.90193 -1.43180 -0.05853 0.87237 0.61568 -0.03368 ## ## Degrees of Freedom: 752 Total (i.e. Null); 747 Residual ## Null Deviance: 1030 ## Residual Deviance: 906.5 AIC: 918.5 AIC에 의한 선택 결과도 확인해 보자. 객체 fit_aic에도 동일한 방식으로 결과가 입력되어 있다. fit_aic$Subsets ## Intercept k5 k618 age wc hc lwg inc logLikelihood AIC ## 0 TRUE FALSE FALSE FALSE FALSE FALSE FALSE FALSE -514.8732 1029.7464 ## 1 TRUE TRUE FALSE FALSE FALSE FALSE FALSE FALSE -497.3750 996.7500 ## 2 TRUE TRUE FALSE TRUE FALSE FALSE FALSE FALSE -482.2418 968.4836 ## 3 TRUE TRUE FALSE TRUE FALSE FALSE TRUE FALSE -468.8080 943.6160 ## 4 TRUE TRUE FALSE TRUE TRUE FALSE FALSE TRUE -462.1527 932.3053 ## 5* TRUE TRUE FALSE TRUE TRUE FALSE TRUE TRUE -453.2277 916.4554 ## 6 TRUE TRUE TRUE TRUE TRUE FALSE TRUE TRUE -452.7801 917.5602 ## 7 TRUE TRUE TRUE TRUE TRUE TRUE TRUE TRUE -452.6330 919.2659 fit_aic$BestModel ## ## Call: glm(formula = y ~ ., family = family, data = Xi, weights = weights) ## ## Coefficients: ## (Intercept) k5 age wcyes lwg inc ## 2.90193 -1.43180 -0.05853 0.87237 0.61568 -0.03368 ## ## Degrees of Freedom: 752 Total (i.e. Null); 747 Residual ## Null Deviance: 1030 ## Residual Deviance: 906.5 AIC: 918.5 두 방법으로 동일한 변수가 선택된 것을 알 수 있다. 만일 설명변수가 많은 경우에는 BestModel를 구성하고 있는 설명변수의 이름을 추출해서 비교하는 것이 더 편리할 수 있다. fit_aic$BestModel$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit_bic$BestModel$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; Stepwise selection 함수 MASS::stepAIC()를 사용하여 stepwise selection에 의한 변수선택을 진행해 보자. 진행 과정은 선형회귀모형의 경우와 동일하다. library(MASS) fit_null &lt;- glm(lfp ~ 1, family = binomial, Mroz) fit_full &lt;- glm(lfp ~ ., family = binomial, Mroz) AIC에 의한 단계적 선택을 진행해 보자. fit1 &lt;- stepAIC(fit_null, scope = list(upper = fit_full, lower = fit_null), trace = FALSE) fit2 &lt;- stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) BIC에 의한 단계적 선택을 진행해 보자. fit3 &lt;- stepAIC(fit_null, scope = list(upper = fit_full, lower = fit_null), trace = FALSE, k = log(nrow(Mroz))) fit4 &lt;- stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE, k = log(nrow(Mroz))) 네 가지 방법이 모두 동일한 결과를 보여주고 있다. fit1$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit2$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit3$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit4$term |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; Lasso 모형 적합 함수 glmnet::glmnet()를 사용하여 lasso 모형을 적합해 보자. 설명변수로 이루어진 행렬을 함수 model.matrix()를 이용하여 생성하자. X_Mroz &lt;- model.matrix(lfp ~ ., Mroz)[,-1] 최적 \\(\\lambda\\) 값에 의한 로지스틱 lasso 모형을 함수 cv.glmnet()으로 적합해 보자. 로지스틱 모형의 설정은 family = \"binomial\"을 지정함으로써 가능하며, type.measure로 CV error measure를 지정할 수 있다. 로지스틱 모형에서 디폴트 measure는 \"deviance\"이며, 2.5절에서 살펴볼 분류 성능을 평가하는 측도 중 misclassification error는 \"class\", ROC curve의 면적은 \"auc\"를 지정하면 사용할 수 있다. library(glmnet) set.seed(123) cvfit_Mroz &lt;- cv.glmnet(X_Mroz, Mroz$lfp, family = &quot;binomial&quot;, type.measure = &quot;deviance&quot;) CV error에 대한 결과를 모형 cvfit_Mroz의 내용과 그래프로 확인해 보자. cvfit_Mroz ## ## Call: cv.glmnet(x = X_Mroz, y = Mroz$lfp, type.measure = &quot;deviance&quot;, family = &quot;binomial&quot;) ## ## Measure: Binomial Deviance ## ## Lambda Index Measure SE Nonzero ## min 0.001766 45 1.221 0.02598 7 ## 1se 0.026225 16 1.243 0.02073 5 plot(cvfit_Mroz) 그림 2.6: lambda의 변화에 따른 CV error의 변화 최적 \\(\\lambda\\) 값으로 lasso 모형의 회귀계수 추정 결과를 확인해 보자. coef(cvfit_Mroz) ## 8 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 1.54013397 ## k5 -0.88490744 ## k618 . ## age -0.02985041 ## wcyes 0.46525473 ## hcyes . ## lwg 0.40095168 ## inc -0.01694822 Best subset selection과 stepwise selection, 그리고 lasso 모형 모두 동일한 변수를 선택했다. 선택된 변수로 이루어진 모형은 다음과 같다. fit1 ## ## Call: glm(formula = lfp ~ k5 + age + lwg + inc + wc, family = binomial, ## data = Mroz) ## ## Coefficients: ## (Intercept) k5 age lwg inc wcyes ## 2.90193 -1.43180 -0.05853 0.61568 -0.03368 0.87237 ## ## Degrees of Freedom: 752 Total (i.e. Null); 747 Residual ## Null Deviance: 1030 ## Residual Deviance: 906.5 AIC: 918.5 로지스틱 회귀모형을 구성하고 있는 개별 설명변수들의 효과는 odds ratio로 설명할 수 있지만, 그래프로 표현할 수 있다면 훨씬 효과적일 것이다. 모형 fit1에서 설명변수 k5와 wc가 반응변수 lfp가 \"yes\"가 될 확률에 미치는 영향을 그래프로 표현해 보자. 다른 설명변수 age, lwg와 inc는 각 변수의 평균값으로 고정하고, wc가 각각 \"yes\"와 \"no\"일 때, k5의 값이 0에서 4로 증가함에 따라 lfp가 \"yes\"가 될 확률이 어떻게 변화하는지 그래프로 나타내 보자. 먼저 확률 예측에 사용될 새로운 데이터를 구성해 보자. df1 &lt;- Mroz |&gt; summarise(across(c(age,lwg,inc), mean)) |&gt; expand_grid(k5 = 0:4, wc = c(&quot;yes&quot;, &quot;no&quot;) ) df1 ## # A tibble: 10 × 5 ## age lwg inc k5 wc ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;chr&gt; ## 1 42.5 1.10 20.1 0 yes ## 2 42.5 1.10 20.1 0 no ## 3 42.5 1.10 20.1 1 yes ## 4 42.5 1.10 20.1 1 no ## 5 42.5 1.10 20.1 2 yes ## 6 42.5 1.10 20.1 2 no ## 7 42.5 1.10 20.1 3 yes ## 8 42.5 1.10 20.1 3 no ## 9 42.5 1.10 20.1 4 yes ## 10 42.5 1.10 20.1 4 no 이제 df1에 주어진 설명변수의 값에 대하여 lfp가 \"yes\"가 될 확률을 예측하고, 그 결과를 막대그래프로 나타내 보자. df1 |&gt; mutate(p1 = predict(fit1, newdata = df1, type = &quot;response&quot;)) |&gt; ggplot() + geom_col(aes(x = k5, y = p1, fill = wc), position = &quot;dodge&quot;) + labs(y = &quot;Prob&quot;) 그림 2.7: k5와 wc의 값에 따른 lfp가 'yes'가 될 확률의 변화 k5의 값이 증가함에 따라 직업을 가질 확률이 매우 급격하게 감소하는 것을 볼 수 있다. 또한 여성의 대학과정 등록한 여부가 직업을 가질 확률이 큰 영향을 주고 있음도 확인할 수 있다. 이번에는 모형 fit1에서 설명변수 k5와 wc, 그리고 lwg의 영향력을 그래프로 표현해 보자. 설명변수 age와 inc는 평균으로 고정하고, wc가 각각 \"yes\"와 \"no\"이고, k5의 값이 0~3이며, lwg가 (-2,3)의 값을 갖을 때, lfp가 \"yes\"가 될 확률이 어떻게 변화하는지 그래프로 나타내 보자. 먼저 확률 예측에 사용될 새로운 데이터를 구성해 보자. df2 &lt;- Mroz |&gt; summarise(across(c(age, inc), mean)) |&gt; expand_grid(k5 = 0:3, wc = c(&quot;yes&quot;, &quot;no&quot;), lwg = seq(-2, 3, length = 50) ) 이제 df2에 주어진 설명변수의 값에 대하여 lfp가 \"yes\"가 될 확률을 예측하고, 그 결과를 선그래프로 나타내 보자. df2 |&gt; mutate(p2 = predict(fit1, newdata = df2, type = &quot;response&quot;)) |&gt; ggplot() + geom_line(aes(x = lwg, y = p2, color = factor(k5)), linewidth = 1) + facet_wrap(vars(wc)) + labs(color = &quot;Number of Kids&quot;, y = NULL) + ylim(0,1) 그림 2.8: k5, wc와 lwg의 값에 따른 lfp가 'yes'가 될 확률의 변화 변수 lwg의 값에 따른 lfp가 \"yes\"가 될 확률을 선그래프로 표시하되, k5의 값에 따라 다른 색으로 표시했으며, 변수 wc는 faceting으로 효과를 비교했다. 그림 2.8와 같은 facet 그래프는 faceting 변수의 각 범주 내에서의 효과 비교는 수월하게 이루지지만, 범주 간의 효과 비교는 조금 어려운 측면이 있다. 예를 들어, wc가 \"no\"인 그룹과 \"yes\"인 그룹에서 k5가 0에 해당하는 두 곡선의 직접적인 비교가 어렵다는 것이다. 따라서 변수 wc의 효과를 다르게 나타낼 수 있는 그래프를 함께 작성하는 것이 필요할 것이다. wc를 시각적 요소 linetype에 매핑한 그래프를 작성해 보자. df2 |&gt; mutate(p2 = predict(fit1, newdata = df2, type = &quot;response&quot;)) |&gt; ggplot() + geom_line(aes(x = lwg, y = p2, color = factor(k5), linetype = wc), linewidth = 1) + labs(color = &quot;Number of Kids&quot;, linetype = &quot;WC&quot;, y = NULL) + ylim(0,1) 그림 2.9: k5, wc와 lwg의 값에 따른 lfp가 'yes'가 될 확률의 변화 이번에는 모형 fit1에서 설명변수 k5와 inc, 그리고 lwg의 영향력을 그래프로 표현해 보자. 설명변수 age는 평균으로, wc는 \"yes\"인 경우로 고정하자. k5의 값이 0, 1, 2, 3이며, lwg가 (-2, 3)의 값을 갖고, inc가 (0, 30)의 값을 갖을 때, lfp가 \"yes\"가 될 확률이 어떻게 변화하는지 그래프로 나타내 보자. 먼저 확률 예측에 사용될 새로운 데이터를 구성해 보자. df3 &lt;- Mroz |&gt; summarise(age = mean(age), wc = &quot;yes&quot;) |&gt; expand_grid(k5 = 0:3, inc = seq(0, 30, length = 50), lwg = seq(-2, 3, length = 50) ) 연속형 변수 inc와 lwg의 값에 따라 lfp가 \"yes\"가 될 확률의 변화를 함께 나타낼 수 있는 그래프에는 Heatmap이 있으며, 함수 geom_raster()로 작성할 수 있다. 변수 k5의 효과는 faceting으로 나타내 보자. df3 |&gt; mutate(p3 = predict(fit1, newdata = df3, type = &quot;response&quot;)) |&gt; ggplot() + geom_raster(aes(x = lwg, y = inc, fill = p3)) + scale_fill_viridis_c() + facet_wrap(vars(k5), labeller = &quot;label_both&quot;) + labs(fill = &quot;P(Y=1)&quot;) 그림 2.10: k5, inc와 lwg의 값에 따른 lfp가 'yes'가 될 확률의 변화 변수 inc의 값이 낮아지고 lwg의 값이 증가함에 따라 lfp가 \"yes\"가 될 확률이 높아지고 있지만, k5의 값이 상승함에 따라 확률은 전반적으로 크게 떨어지는 것을 볼 수 있다. 2.4 회귀진단 선형회귀모형의 경우와 같이 로지스틱 회귀모형에서도 적합한 모형에 대한 회귀진단을 실시해야 한다. 선택한 모형이 주어진 자료를 잘 설명하고 있는지에 대한 확인이 필요한 것이다. 다만 선형회귀모형의 회귀진단에서 주로 사용됐던 잔차와는 조금 다른 형태의 잔차가 로지스틱 회귀모형에서 사용된다는 차이가 있다. 선형회귀모형에서 잔차 \\(e_{i}=\\hat{y}_{i}-y_{i}\\) 는 회귀모형의 오차 \\(\\varepsilon_{i}\\) 를 대신하는 역할을 하고 있지만, 로지스틱 회귀모형에서는 선형회귀모형처럼 가법모형 방식의 오차를 정의할 수 없다는 문제가 있다. 따라서 로지스틱 회귀모형에서 잔차의 의미는 조금 다를 수밖에 없다. 로지스틱 회귀모형에서 사용되는 잔차에는 Pearson 잔차와 deviance 잔차가 있다. 먼저 Pearson 잔차 \\(r_{i}^{P}\\) 는 다음과 같이 정의된다. \\[\\begin{equation} r_{i}^{P}=\\frac{y_{i}-\\hat{\\pi}_{i}}{\\sqrt{\\hat{\\pi}_{i}(1-\\hat{\\pi}_{i})}} \\end{equation}\\] 단, \\(\\hat{\\pi}_{i}\\) 는 \\(\\pi_{i} = P(Y_{i}=1|X)\\) 의 추정값이다. Deviance 잔차 \\(r_{i}^{D}\\) 는 다음과 같이 정의된다. \\[\\begin{equation} r_{i}^{D}=s_{i}\\sqrt{-2 \\left(y_{i}\\log \\hat{\\pi}_{i} + (1-y_{i}) \\log (1-\\hat{\\pi}_{i}) \\right)} \\end{equation}\\] 단, \\(s_{i}\\) 는 \\(y_{i}=1\\) 이면 1이고, \\(y_{i}=0\\) 이면 0이다. 두 잔차의 제곱합은 모두 모형의 적합도를 나타내는 통계량이 된다. 회귀진단 과정에서는 잔차의 분포를 조금 더 좌우대칭에 가깝게 변형을 하기 위해 표준화를 시킨 잔차를 사용하기도 한다. 표준화 Pearson 잔차는 \\(r_{i}^{P}/\\sqrt{1-h_{i}}\\) 로 정의되고, 표준화 deviance 잔차는 \\(r_{i}^{D}/\\sqrt{1-h_{i}}\\) 로 정의된다. 단, \\(h_{i}\\) 는 \\(i\\) 번째 관찰값의 leverage이다. 영향력이 큰 관찰값의 탐지도 선형회귀모형의 경우와 유사한 방식으로 진행한다. Cook’s distance와 leverage가 중요한 통계량으로 사용된다. 다중공선성의 존재 여부도 선형회귀모형의 경우와 동일하게 확인할 필요가 있다. \\(\\bullet\\) 예제:carData::Mroz 2.3절에서 수행된 모든 변수선택은 동일한 모형을 결과로 보여주었다. 선정된 모형을 대상으로 회귀진단을 실시해 보자. fit_full &lt;- glm(lfp ~ ., family = binomial, Mroz) fit1 &lt;- MASS::stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) 먼저 모형 fit1의 잔차 산점도를 작성해 보자. 로지스틱 회귀모형에 대한 잔차 산점도는 패키지 car의 함수 residualPlots()로 작성할 수 있다. 디폴트 잔차는 Pearson 잔차이며, deviance 잔차를 원하는 경우에는 type = \"deviance\"를 추가하면 된다. 실행 결과로 각 설명변수와 Pearson 잔차 (또는 deviance 잔차)의 산점도가 작성되는데, 설명변수가 요인인 경우에는 상자그림이 작성된다. 또한 설명변수의 선형결합인 \\(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{1} + \\cdots + \\hat{\\beta}_{k}x_{k}\\) 와 잔차의 산점도가 마지막 그래프로 작성된다. library(car) residualPlots(fit1) 그림 2.11: 함수 residualPlots()에 의한 회귀진단 ## Test stat Pr(&gt;|Test stat|) ## k5 0.1102 0.73990 ## age 0.6284 0.42793 ## wc ## lwg 152.7522 &lt; 2e-16 *** ## inc 3.2493 0.07145 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 그림 2.11에서 볼 수 있는 잔차 산점도는 선형회귀모형의 경우처럼 0을 중심으로 일정한 폭의 영역에 무작위로 흩어져 있는 것과는 많이 다른 모습이다. 그것은 반응변수가 0이면 잔차가 음수가 되고, 반응변수가 1이면 양수가 되는 관계가 있기 때문이다. 각 패널에 추가된 마젠타(magenta) 색의 실선은 국소다항회귀를 나타내는 것으로서 0 근처에서 수평선을 유지해야 한다. 만일 명백한 곡선의 형태가 나타난다면 해당 변수의 적합에 문제가 있음을 보여주는 것이 된다. 만일 표준화 Pearson 잔차나 스튜던트화 Pearson 잔차를 대신 사용하고자 한다면 type = \"rstandard\" 또는 type = \"rstudent\"를 입력하면 된다. 그래프와 더불어 출력된 검정 결과는 숫자형 변수의 curvature test로서, 각 설명변수의 2차항을 추가한 각각의 모형에서 추가된 2차항 변수의 유의성을 검정한 것이다. 변수 lwg의 경우, 추가된 2차항이 유의한 것으로 나타났는데, 잔차 산점도에서도 곡선이 나타난 것으로 보아 2차항을 모형에 포함시켜 확인할 필요가 있는 것으로 보인다. 기존의 모형에 변수를 추가하거나 제외하는 작업은 함수 update()로 하는 것이 더 편리하다. 모형 fit1을 대상으로 기존의 모형(. ~ .)에 I(lwg^2)을 추가(. ~ . + I(lwg^2))하는 것이 된다. fit2 &lt;- update(fit1, . ~ . + I(lwg^2)) summary(fit2) ## ## Call: ## glm(formula = lfp ~ k5 + age + wc + lwg + inc + I(lwg^2), family = binomial, ## data = Mroz) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.518168 0.821533 7.934 2.12e-15 *** ## k5 -1.527347 0.223132 -6.845 7.65e-12 *** ## age -0.068245 0.012794 -5.334 9.61e-08 *** ## wcyes 0.139512 0.239424 0.583 0.560097 ## lwg -7.763915 1.094941 -7.091 1.33e-12 *** ## inc -0.033799 0.008864 -3.813 0.000137 *** ## I(lwg^2) 4.580429 0.566054 8.092 5.88e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 1029.7 on 752 degrees of freedom ## Residual deviance: 753.7 on 746 degrees of freedom ## AIC: 767.7 ## ## Number of Fisher Scoring iterations: 7 모형에 추가된 lwg의 2차항은 유의한 것으로 나타났는데, 변수를 추가함으로 해서 모형의 적합도가 얼마나 향상됐는지는 모형의 AIC와 BIC를 비교해 보면 알 수 있다. 또한 이 변수가 추가되면서 wc가 비유의적인 변수가 되었는데, 모형의 적합도에는 큰 차이가 있을 것으로 보이지는 않지만 비유의적인 변수를 제외한 모형도 비교해 보자. fit3 &lt;- update(fit2, . ~ . - wc) AIC(fit1, fit2, fit3) ## df AIC ## fit1 6 918.4554 ## fit2 7 767.7032 ## fit3 6 766.0427 BIC(fit1, fit2, fit3) ## df BIC ## fit1 6 946.1998 ## fit2 7 800.0717 ## fit3 6 793.7871 AIC와 BIC에서 fit2와 fit3는 차이가 없는 모형임을 알 수 있다. 영향력이 큰 관찰값의 탐지는 패키지 car의 함수 influencePlot()으로 할 수 있다. 사용법 및 해석 방식은 선형회귀모형의 경우와 동일하다. influencePlot(fit2) 그림 2.12: 함수 influencePlot()에 의한 회귀진단 그래프 ## StudRes Hat CookD ## 79 0.8256292 0.07649726 0.004919585 ## 104 0.9178673 0.05114926 0.004084955 ## 119 2.5033037 0.02013335 0.054106915 ## 586 -2.3107567 0.02148102 0.037489575 다중공선성의 문제도 패키지 car의 함수 vif()로 확인해 보자. 변수 lwg와 I(lwg^2)의 VIF 값이 크게 나왔는데, 이것은 다항회귀모형에서 나타나는 자연스러운 현상으로 볼 수 있다. vif(fit2) ## k5 age wc lwg inc I(lwg^2) ## 1.327656 1.319828 1.248518 17.993556 1.126411 18.628778 2.5 분류성능 평가 로지스틱 회귀모형은 분류 모형으로 매우 광범위하게 사용되고 있다. 이 절에서는 로지스틱 회귀모형의 분류성능을 평가하는 몇 가지 측도에 대해 살펴보도록 하자. 변수선택과 회귀진단 과정을 통해 최적 로지스틱 회귀모형이 선정되면, 반응변수가 “성공” 범주에 속할 확률 \\(\\pi(\\mathbf{x}) = P(Y=1|\\mathbf{x})\\) 을 추정할 수 있게 된다. 이어서 추정된 확률 \\(\\hat{\\pi}(\\mathbf{x})\\) 를 이용하여 각 관찰값을 다음과 같이 두 범주로 분류할 수 있게 된다. \\[\\begin{equation} \\hat{y} = \\begin{cases} 0, &amp; \\quad \\text{if}~~ \\hat{\\pi}(\\mathbf{x}) &lt; d \\\\ 1, &amp; \\quad \\text{if}~~ \\hat{\\pi}(\\mathbf{x}) \\geq d \\end{cases} \\end{equation}\\] 분류기준값 \\(d\\) 는 0.5로 놓고 분류를 시행하는 것이 일반적이다. 분류성능을 평가하기 위한 첫 번째 단계는 관찰값 \\(y\\) 와 예측값 \\(\\hat{y}\\) 의 2차원 분할표인 confusion matrix를 작성하는 것이다. \\(n\\) 개의 자료를 대상으로 분류 작업을 실행하여 \\(y=0\\) 인 자료 중에 \\(\\hat{y} = 0\\) 또는 \\(\\hat{y} = 1\\) 로 각각 분류된 자료의 개수와 \\(y=1\\) 인 자료 중에 \\(\\hat{y} = 0\\) 또는 \\(\\hat{y} = 1\\) 로 각각 분류된 자료의 개수를 2차원 분할표 형태로 작성한 것이 confusion matrix이다. 표 2.1은 Machine Learning 분야에서 사용되는 용어를 사용하여 작성된 confusion matrix이다. Machine Learning 분야에서는 일반적으로 ‘성공’ 범주를 Event로, ‘실패’ 범주를 NonEvent로 표시를 한다. 또한 ‘성공’ 범주로 분류하면 Positive, ‘실패’ 범주로 분류하면 Negative로 표현하며, 분류 결과가 맞으면 True, 틀리면 False로 표현한다. 따라서 \\(y=0\\) 인 자료를 \\(\\hat{y} = 0\\) 분류하면 True Negative(TN), \\(\\hat{y} = 1\\) 분류하면 False Positive(FP)로 표현하고, \\(y=1\\) 인 자료를 \\(\\hat{y} = 0\\) 로 분류하면 False Negative(FN), \\(\\hat{y} = 1\\) 로 분류하면 True Positive(TP)로 표현한다. 표 2.1: Confusion matrix Observation Prediction NonEvent Event NonEvent True Negative False Negative Event False Positive True Positive 2.5.1 분류성능 평가 측도 1) 정분류율(Accuracy rate) 정분류율은 예측값이 관찰값과 동일한 범주로 분류된 비율을 나타내는 것이다. 표 2.1의 confusion matrix에서는 \\((TN + TP)/n\\) 으로 표현된다. 정분류율은 모형의 분류성능에 대한 기본적인 평가 측도로 사용되고 있으나, 문제점이 있는 평가 측도이어서 조심해서 사용해야 한다. 첫 번째 문제로 지적되는 것은 더 많이 관측된 범주의 비율보다 정분류율이 더 높아야 의미가 있는 측도가 된다는 점이다. 예를 들어 Event로 100개의 자료가 관측되었고, NonEvent로 50개의 자료가 관측된 경우에, 총 150개 자료를 모두 Positive로 단순 분류해도 정분류율은 100/150 = 0.67이 된다. 따라서 모형에 의한 정분류율이 더 많이 관측된 범주의 비율(No information rate)보다 작다면 사용할 의미가 없어지는 것이다. 두 번째 문제는 분류에서 발생되는 두 가지 오류에 대한 정보를 전혀 얻을 수 없는 측도라는 점이다. 두 가지 분류 오류인 False Positive와 False Negative로 인한 cost가 다른 경우에 정분류율만으로 분류 결과를 평가하기 어렵다는 문제이다. 예를 들어 스팸 메일에 대한 분류 작업을 하는 경우, 중요한 메일이 스팸으로 분류되는 False Positive가 훨씬 중요한 오류가 될 것이다. 이런 경우 정분류율에서 발생되는 약간의 불이익을 감소하더라도 False Positive가 더 낮은 분류 모형을 사용하는 것이 바람직할 것이다. 2) 민감도(Sensitivity)와 특이도(Specificity) 민감도는 Event로 관측된 자료 중 Positive로 분류된 자료의 비율을 의미한다. \\[\\begin{equation} sensitivity = \\frac{TP}{FN+TP} \\end{equation}\\] 민감도를 True Positive rate라고 할 수 있는데, 그것은 Event 범주에서 Positive로 잘 분류된 비율이기 때문이다. 특이도는 NonEvent로 관측된 자료 중 Negative로 분류된 자료의 비율을 의미한다. \\[\\begin{equation} specificity = \\frac{TN}{TN+FP} \\end{equation}\\] NonEvent 범주에서 Positive로 잘못 분류되는 비율을 False Positive 비율이라고 한다면, 특이도는 1-False Positive rate가 된다. 민감도와 특이도는 한쪽이 증가하면 다른 쪽은 감소하는 trade-off의 관계를 가지고 있다. True Positive를 높이는 것이 더 중요한 상황이라면 민감도를 판단 기준으로 사용해야하지만, False Positive를 낮추는 것이 더 중요한 상황이라면 특이도를 판단 기준으로 사용해야 할 것이다. 따라서 True Positive와 False Positive의 상대적 중요도를 고려해서 분류기준점을 조절하는 것이 필요하다. 3) Precision과 F1 score Precision은 Positive로 분류한 자료 중 Event 자료의 비율을 나타낸다. \\[\\begin{equation} precision = \\frac{TP}{TP+FP} \\end{equation}\\] F1 score는 Precision과 Recall이라고도 불리는 Sensitivity의 harmonic mean으로 정의된다. \\[\\begin{equation} F1 = 2 \\times \\frac{precision \\times recall}{precision + recall} \\end{equation}\\] F1 score는 False Positive와 False Negative의 영향력이 모두 반영된 측도로 볼 수 있으며, 따라서 단순한 정분류율보다 더 유용하게 사용되고 있다. 4) ROC(Receiver Operating Characteristic) curve ROC curve는 모든 분류 기준값 에 대해서 계산된 특이도와 민감도를 \\((x,y)\\) 좌표로 하여 작성된 그래프이다. 그림 2.13에 예시된 ROC curve를 볼 수 있다. 그래프의 좌측 하단은 분류기준값 \\(d\\) 를 1로 설정한 지점으로서 모든 자료가 Negative로 분류되었고, 따라서 민감도가 0, 특이도가 1이 된 것이다. 이제 분류 기준값을 조금씩 낮추게 되면 특이도가 떨어지는 대신 민감도는 높아지게 되는데, 분류 정확도가 높은 모형이라면 곡선의 기울기가 매우 급격하게 상승하게 된다. 즉, 특이도를 조금만 희생시키면 민간도가 급격하게 좋아지게 되어서 전반적인 분류 정확도를 높일 수 있는 것이다. 그래프의 우측 상단은 분류기준값을 0으로 설정한 지점으로서 모든 자료가 Positive로 분류되었고, 따라서 민감도는 1, 특이도는 0이 된 것이다. 그림 2.13: ROC curve ROC curve의 전반적인 모습을 보고 모형의 분류성능을 판단하는 것이 정확한 결론을 내는 방법이지만, 많은 분석을 실시하는 상황에서는 조금 더 간단한 방법이 필요하기도 한다. 분류 정확도가 높은 모형의 ROC curve는 기울기가 매우 급격하게 상승을 하는데, 이렇게 되면 ROC curve 아래 부분의 면적이 증가하게 된다. 따라서 ROC curve 아래 부분의 면적을 기준으로 분류 정확도를 판단하는 것이 가능한 것이다. ROC curve 아래 부분의 면적을 AUC(Area Under the Curve)라고 하며, AUC 값이 더 큰 모형이 선호된다. ROC curve의 그래프에는 대부분 점선으로 대각선이 그려진다. 이 대각선에서는 민감도와 특이도 값을 더하면 1이 되는데, 이것은 True Positive rate이 False Positive rate과 같은 경우에 해당하는 상황이다. 즉, 분별력이 전혀 없는 모형이 되는 것이다. 만일 ROC curve가 대각선과 일치한다면 AUC는 0.5가 된다. \\(\\bullet\\) 분류성능 평가 측도를 위한 함수 지금까지 살펴본 분류성능 평가 측도 중 정분류율과 민감도, 특이도 및 F1 score 등은 패키지 caret의 함수 confusionMatrix()로 구할 수 있다. 함수 confusionMatrix()의 기본적인 사용법은 confusionMatix(data, reference, positive)이다. data에는 예측 결과를 요인으로 지정하고, reference에는 반응변수를 요인으로 지정해야 한다. positive에는 반응변수의 요인 수준 중에 Event에 해당하는 수준을 지정해야 한다. ROC curve는 패키지 pROC의 함수 roc()로 작성할 수 있다. 함수 roc()의 기본적인 사용법은 roc(response, predictor, plot = TRUE, ... )이다. response에는 반응변수의 벡터를 지정하는데, 유형은 요인, 숫자, 문자 모두 가능하다. predictor에는 ‘성공’ 범주에 속할 확률이 계산된 벡터를 지정해야 한다. ROC curve의 그래프를 작성하려면 옵션 plot에 TRUE를 지정해야 한다. AUC의 계산은 roc 객체를 함수 auc()에 입력하면 계산된다. \\(\\bullet\\) 예제 carData::Mroz 데이터 프레임 Mroz의 변수 lfp를 반응변수로 하고 나머지 변수를 설명변수로 하는 로지스틱 회귀모형을 적합해 보자. 2.3절과 2.4절의 예제에서는 Mroz 자료를 모두 사용하여 분석을 실시하였다. 그러나 예측모형을 만드는 것이 목적인 경우에는 전체 자료를 반드시 training data와 test data로 분리하고, training data만을 사용하여 ‘최적’ 모형을 적합해야 한다. Test data는 모형 적합 과정에서는 절대로 사용되면 안 되고, 선택된 모형에 대한 평가 목적으로만 사용되어야 한다. 예측모형 적합을 위한 첫 번째 단계인 자료 분리는 함수 caret::createDataPartition()으로 하겠다. 이 함수는 연속형 반응변수의 예측 상황인 1.5절에서 이미 효과적으로 사용한 함수이다. 이항반응변수의 경우에는 반응변수의 두 범주의 비율이 전체 자료와 분리된 자료에서 모두 동일하게 유지되는 것이 필요한데, 이것은 층화추출이 이루어지면 가능하다. 함수 createDataPartition()에 요인이 첫 번째 변수로 입력되면 요인의 각 범주에 대한 층화추출이 이루어진다. 전체 자료의 80%를 training data로 분리하고, 20%의 자료는 test data로 분리하자. data(Mroz, package = &quot;carData&quot;) library(tidyverse) library(caret) set.seed(123) x.id &lt;- createDataPartition(Mroz$lfp, p = 0.8, list = FALSE)[,1] train_M &lt;- Mroz |&gt; slice(x.id) test_M &lt;- Mroz |&gt; slice(-x.id) 변수 lfp의 \"yes\"와 \"no\"의 비율을 Mroz와 train_M, test_M에서 각각 확인해 보자. Mroz |&gt; count(lfp) |&gt; mutate(p = n/sum(n)) ## lfp n p ## 1 no 325 0.4316069 ## 2 yes 428 0.5683931 train_M |&gt; count(lfp) |&gt; mutate(p = n/sum(n)) ## lfp n p ## 1 no 260 0.4311774 ## 2 yes 343 0.5688226 test_M |&gt; count(lfp) |&gt; mutate(p = n/sum(n)) ## lfp n p ## 1 no 65 0.4333333 ## 2 yes 85 0.5666667 이제 training data를 대상으로 ‘최적’ 모형을 적합을 위한 변수선택을 진행해 보자. 변수선택은 best subset selection과 stepwise selection을 사용할 것이며, 먼저 best subset selection 방법을 함수 bestglm()으로 수행해 보자. Xy &lt;- train_M |&gt; relocate(lfp, .after=last_col()) library(bestglm) fit1 &lt;- bestglm(Xy, family = binomial)$BestModel fit2 &lt;- bestglm(Xy, family = binomial, IC = &quot;AIC&quot;)$BestModel 함수 stepAIC()로 stepwise selection에 의한 변수선택을 진행해 보자. fit_full &lt;- glm(lfp ~ ., family = binomial, train_M) fit_null &lt;- glm(lfp ~ 1, family = binomial, train_M) library(MASS) fit3 &lt;- stepAIC(fit_null, scope = list(upper = fit_full, lower = fit_null), trace = FALSE) fit4 &lt;- stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) fit5 &lt;- stepAIC(fit_null, scope = list(upper = fit_full, lower = fit_null), trace = FALSE, k = log(nrow(train_M))) fit6 &lt;- stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE, k = log(nrow(train_M))) 선택된 모형의 변수선택 결과를 확인해 보자. fit1$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit2$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit3$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit4$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit5$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; fit6$terms |&gt; attr(&quot;term.labels&quot;) |&gt; sort() ## [1] &quot;age&quot; &quot;inc&quot; &quot;k5&quot; &quot;lwg&quot; &quot;wc&quot; 모두 동일한 모형임을 확인할 수 있다. 적합 내용을 확인해 보자. fit_1 &lt;- fit3 summary(fit_1) ## ## Call: ## glm(formula = lfp ~ lwg + k5 + age + inc + wc, family = binomial, ## data = train_M) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 2.910361 0.607796 4.788 1.68e-06 *** ## lwg 0.646646 0.169648 3.812 0.000138 *** ## k5 -1.356904 0.220477 -6.154 7.54e-10 *** ## age -0.061678 0.012797 -4.820 1.44e-06 *** ## inc -0.030294 0.008737 -3.467 0.000526 *** ## wcyes 0.773789 0.224042 3.454 0.000553 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 824.47 on 602 degrees of freedom ## Residual deviance: 735.30 on 597 degrees of freedom ## AIC: 747.3 ## ## Number of Fisher Scoring iterations: 4 선택된 모형 fit_1에 대한 검진을 실시해 보자. 함수 residualPlots()으로 잔차 산점도 및 curvature test를 실시해 보자. library(car) residualPlots(fit_1) 그림 2.14: 모형 fit_1의 잔차 산점도 ## Test stat Pr(&gt;|Test stat|) ## lwg 124.0900 &lt; 2e-16 *** ## k5 0.0315 0.85908 ## age 1.3815 0.23985 ## inc 4.2999 0.03811 * ## wc ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 변수 lwg의 경우에는 2차항을 포함시키는 것이 필요한 것으로 보인다. 변수 inc의 경우에도 curvature test에서는 p-값이 0.05보다 작게 나왔으나, 잔차 산점도에서는 자료의 개수가 많지 않은 부분에서만 약간의 비선형 관계가 보이고 있는 것으로 나타나서, 변수 inc의 2차항은 포함시키지 않는 것으로 하겠다. 변수 lwg의 2차항을 포함한 모형을 적합시키고, 개별 회귀계수의 유의성을 95% 신뢰구간으로 확인해 보자. fit_2 &lt;- update(fit_1, . ~ . + I(lwg^2)) confint(fit_2) ## 2.5 % 97.5 % ## (Intercept) 4.62945865 8.12981369 ## lwg -9.72717273 -5.14783290 ## k5 -1.96295227 -0.95440249 ## age -0.09877261 -0.04239908 ## inc -0.05075725 -0.01139112 ## wcyes -0.42920987 0.59066937 ## I(lwg^2) 3.21738759 5.57124912 변수 lwg의 2차항이 포함되면서 비유의적인 변수가 된 wc를 제외한 모형도 적합시키고 비교 대상으로 고려하자. fit_3 &lt;- update(fit_2, . ~ . - wc) 세 모형의 AIC와 BIC를 비교해 보자. AIC(fit_1, fit_2, fit_3) ## df AIC ## fit_1 6 747.3042 ## fit_2 7 625.2142 ## fit_3 6 623.3124 BIC(fit_1, fit_2, fit_3) ## df BIC ## fit_1 6 773.7157 ## fit_2 7 656.0276 ## fit_3 6 649.7239 큰 차이가 있는 fit_1은 제외하고 나머지 두 모형의 분류성능을 비교해서 최종 모형을 선택하기로 하자. Training data에 대한 두 모형의 예측을 실시해 보자. pred_2 &lt;- predict(fit_2, type = &quot;response&quot;) pred_3 &lt;- predict(fit_3, type = &quot;response&quot;) 모형 fit_2의 분류결과에 대한 평가 결과를 살펴보자. \\(d=0.5\\) 를 분류 기준값으로 해서 함수 if_else()로 분류하고, 그 결과를 요인으로 전환하여 함수 confusionMatrix()에 적용시켜 보자. train_M |&gt; mutate(lfp_hat = factor(if_else(pred_2 &gt;= 0.5, &quot;yes&quot;, &quot;no&quot;))) |&gt; with(confusionMatrix(data = lfp_hat, reference = lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;)) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 186 88 ## yes 74 255 ## ## Accuracy : 0.7313 ## 95% CI : (0.6941, 0.7663) ## No Information Rate : 0.5688 ## P-Value [Acc &gt; NIR] : &lt;2e-16 ## ## Kappa : 0.4559 ## ## Mcnemar&#39;s Test P-Value : 0.3071 ## ## Sensitivity : 0.7434 ## Specificity : 0.7154 ## Pos Pred Value : 0.7751 ## Neg Pred Value : 0.6788 ## Precision : 0.7751 ## Recall : 0.7434 ## F1 : 0.7589 ## Prevalence : 0.5688 ## Detection Rate : 0.4229 ## Detection Prevalence : 0.5456 ## Balanced Accuracy : 0.7294 ## ## &#39;Positive&#39; Class : yes ## Confusion matrix와 정분류율, 민감도와 특이도 외에 많은 평가 측도의 값이 계산되어 있다. 다른 평가 측도의 자세한 설명은 함수 confusionMatrix()의 도움말에서 참고할 수 있다. 모형 fit_3의 분류결과에 대한 평가 결과도 확인해 보자. 함수 confusionMatrix()의 많은 결과 중 다음과 같이 원하는 결과만을 선택해서 출력할 수도 있다. table_3 &lt;- train_M |&gt; mutate(lfp_hat = factor(if_else(pred_3 &gt;= 0.5, &quot;yes&quot;, &quot;no&quot;))) |&gt; with(confusionMatrix(data = lfp_hat, reference = lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;)) 함수 confusionMatrix()의 결과가 할당된 객체 table_3의 요소 table에는 confusion matrix가 입력되어 있다. table_3$table ## Reference ## Prediction no yes ## no 186 86 ## yes 74 257 정분류율 및 민감도, 특이도, F1 score 등은 다음과 같이 확인할 수 있다. table_3$overall[1] ## Accuracy ## 0.73466 table_3$byClass[c(&quot;Sensitivity&quot;, &quot;Specificity&quot;, &quot;Precision&quot;, &quot;F1&quot;)] ## Sensitivity Specificity Precision F1 ## 0.7492711 0.7153846 0.7764350 0.7626113 이번에는 두 모형의 ROC curve를 작성해서 비교해 보자. library(pROC) roc(train_M$lfp, pred_2, plot = TRUE) 그림 2.15: 모형 fit_2의 ROC curve roc(train_M$lfp, pred_3, plot = TRUE) 그림 2.16: 모형 fit_3의 ROC curve AUC만 비교하고자 하는 경우에는 함수 auc()에 roc()의 결과를 입력하면 된다. roc(train_M$lfp, pred_2, quiet = TRUE) |&gt; auc() ## Area under the curve: 0.8177 roc(train_M$lfp, pred_3, quiet = TRUE) |&gt; auc() ## Area under the curve: 0.8175 두 모형의 분류성능에는 큰 차이가 없으나, AIC와 BIC, F1 score 등이 미세하게 좋은 fit_3를 최종 모형으로 선택하기로 하자. fit_final &lt;- fit_3 summary(fit_final) ## ## Call: ## glm(formula = lfp ~ lwg + k5 + age + inc + I(lwg^2), family = binomial, ## data = train_M) ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 6.332365 0.891016 7.107 1.19e-12 *** ## lwg -7.339345 1.160271 -6.326 2.52e-10 *** ## k5 -1.440585 0.256668 -5.613 1.99e-08 *** ## age -0.070322 0.014343 -4.903 9.44e-07 *** ## inc -0.029801 0.009723 -3.065 0.00218 ** ## I(lwg^2) 4.357318 0.590937 7.374 1.66e-13 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 824.47 on 602 degrees of freedom ## Residual deviance: 611.31 on 597 degrees of freedom ## AIC: 623.31 ## ## Number of Fisher Scoring iterations: 7 최종 모형 fit_final을 사용하여 test data를 대상으로 분류를 시행하고, 그 결과에 대한 평가를 실시해 보자. fc &lt;- predict(fit_final, newdata = test_M, type = &quot;response&quot;) table_fc &lt;- test_M |&gt; mutate(lfp_hat = factor(if_else(fc &gt;= 0.5, &quot;yes&quot;, &quot;no&quot;))) |&gt; with(confusionMatrix(data = lfp_hat, reference = lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;)) table_fc$table ## Reference ## Prediction no yes ## no 51 27 ## yes 14 58 table_fc$overall[1] ## Accuracy ## 0.7266667 table_fc$byClass[c(1,2,5,7)] ## Sensitivity Specificity Precision F1 ## 0.6823529 0.7846154 0.8055556 0.7388535 roc(test_M$lfp, fc, quiet = TRUE) |&gt; auc() ## Area under the curve: 0.8476 Training data에 대한 분류 결과와 비교해 보면 AUC와 특이도, Precision는 오히려 더 좋은 결과를 보이고 있음을 알 수 있다. 2.6 희귀사건의 분류 이항반응변수의 두 범주 중 “성공” 범주는 우리가 관심을 갖고 있는 Event 범주이다. 대부분의 경우에 Event 범주와 NonEvent 범주의 발생 가능성이 서로 큰 차이가 나지 않는 자료를 대상으로 분석을 진행하게 된다. 하지만 Event 범주의 발생 가능성이 구조적으로 매우 낮을 밖에 없는 상황도 있다. 예를 들면, 어떤 특정 의약품의 부작용 발생, 온라인 광고에 대한 클릭, 그리고 카드의 부정 사용 등은 발생할 가능성은 상대적으로 매우 낮지만 상당히 중요한 사건이라 하겠다. 이와 같이 “성공” 범주가 발생할 가능성이 매우 낮은 희귀사건인 경우에는 통계적 분류모형의 활용도에 큰 문제가 생길 수 있다. 즉, 통계모형에서 Event 범주에 대한 확률이 매우 낮게 추정될 가능성이 있고, 따라서 거의 모든 자료를 NonEvent로 분류하게 되어서 민감도가 너무 낮아지게 되는 것이다. 이렇게 되면 희귀사건을 제대로 분류하지 못하는 쓸모 없는 모형이 된다. 희귀사건의 영향력을 낮출 수 있는 대안으로는 분류기준값을 변경하는 방법과 training data에 대한 sampling 방식을 변경하는 방법이 있다. 분류기준값을 변경하는 방법은 민감도와 특이도의 trade-off를 고려해서 선정한 최적의 분류기준값으로 민감도를 높이는 방법이며, 로지스틱 회귀모형의 적합 과정 자체에는 어떠한 영향도 미치지 않는다. 분류기준값은 일반적으로 \\(d=0.5\\) 를 사용하는데, 0.5보다 더 작은 값을 사용하면 민감도를 높일 수 있다. 희귀사건에 대한 분류 성능이 떨어지는 이유는 training data에 Event 범주에 대한 정보가 부족하기 때문이다. 따라서 training data에서 Event 범주와 NonEvent 범주에 대한 정보량을 동일하게 만들어 줄 수 있다면, 희귀사건에 대한 분류 성능을 높일 수 있을 것이다. 두 범주의 정보량을 동일하게 만드는 방법은 다음과 같다. Up-sampling : Event 범주 자료를 복원추출하여 case 확대 Down-sampling : NonEvent 범주 자료를 삭제하여 case 축소 SMOTE(Synthetic Minority Oversampling TEchnique) : Event 범주의 자료를 기존 자료의 선형결합으로 생성하여 case 확대. 설명변수가 모두 숫자형 변수인 경우에 적용. 예제를 가지고 구체적인 방법을 살펴보도록 하자. \\(\\bullet\\) 예제: ISLR::Caravan 패키지 ISLR의 Caravan은 어떤 보험 회사의 고객 5822명에 대한 자료이다. 86개 변수가 있으며, 마지막 변수인 Purchase는 고객의 캐러밴 보험 가입 여부를 나타낸 요인이다. data(Caravan, package = &quot;ISLR&quot;) Caravan |&gt; count(Purchase) |&gt; mutate(p = n/sum(n)) ## Purchase n p ## 1 No 5474 0.94022673 ## 2 Yes 348 0.05977327 캐러밴 보험에는 단 6%의 고객만이 가입한 상태이다. 분석 목적은 캐러밴 보험에 가입할 가능성이 높은 고객을 식별하는 것이다. 분석의 첫 단계로서 자료 분리를 진행하자. set.seed(123) x.id &lt;- createDataPartition(Caravan$Purchase, p = 0.7, list = FALSE)[,1] train_C &lt;- Caravan |&gt; slice(x.id) test_C &lt;- Caravan |&gt; slice(-x.id) 워낙 규모가 크기 때문에 변수선택 과정에 상당히 많은 시간이 소요되는 자료이다. 예제의 목적이 희귀사건에 대한 대안 탐색이어서 변수선택 및 검진 단계는 생략하고 그냥 모든 설명변수를 포함한 모형을 예측모형으로 사용하도록 하자. fit &lt;- glm(Purchase ~ ., family = binomial, train_C) 모형 fit으로 test data에 대한 예측 및 분류를 실시해 보자. pred &lt;- predict(fit, newdata = test_C, type = &quot;response&quot;) yhat &lt;- factor(if_else(pred &gt;= 0.5, &quot;Yes&quot;,&quot;No&quot;)) table &lt;- confusionMatrix(reference = test_C$Purchase, data = yhat, positive = &quot;Yes&quot;) 분류성능 평가를 위한 결과를 출력해 보자. table$table ## Reference ## Prediction No Yes ## No 1632 101 ## Yes 10 3 table$overall[1] ## Accuracy ## 0.9364261 table$byClass[c(1,2,5,7)] ## Sensitivity Specificity Precision F1 ## 0.02884615 0.99390987 0.23076923 0.05128205 모형 fit은 1746개의 test data 중 단지 13개만 “Yes” 범주로 분류하였고, 결과적으로 지나치게 낮은 민감도가 산출되었다. 분석의 목적이 보험에 가입할 가능성이 높은 고객을 식별해 내는 것인데, test data에서 실제 보험에 가입한 고객 중에 단 2.9%만 제대로 식별했다는 것은 분석 목적에 전혀 부합하지 못한 분류 결과이다. 민감도를 향상시키는 방법으로 먼저 SMOTE 방법을 적용해 보자. SMOTE 방법이 적용된 training data의 생성은 함수 smotefamily::SMOTE()로 할 수 있다. 기본적인 사용법은 SMOTE(X, target, K = 5)인데, X는 숫자형 설명변수의 데이터 프레임 또는 행렬이고, target은 반응변수 벡터, 그리고 K는 자료 생성을 위한 nearest neighbors의 개수를 지정하는 것이다. set.seed(123) library(smotefamily) smote &lt;- SMOTE(dplyr::select(train_C, -Purchase), train_C$Purchase) 함수 SMOTE()로 생성된 객체 smote의 요소 중 smote$data에는 SMOTE 기법으로 생성된 자료가 데이터 프레임 형태로 입력되는데, 반응변수의 이름이 class로, 유형은 문자형으로 변경된다. 따라서 분석의 편리를 위해 반응변수의 이름을 원래의 이름으로 다시 변경하고, 유형도 요인으로 변경하자. train_smote &lt;- smote$data |&gt; rename(Purchase = class) |&gt; mutate(Purchase = factor(Purchase)) SMOTE 기법으로 생성된 training data를 사용하여 예측모형을 다시 적합해 보자. fit_s &lt;- glm(Purchase ~ ., family = binomial, train_smote) 모형 fit_s로 test data에 대한 예측 및 분류를 실시해 보자. pred_s &lt;- predict(fit_s, newdata = test_C, type = &quot;response&quot;) yhat_s &lt;- factor(if_else(pred_s &gt;= 0.5, &quot;Yes&quot;,&quot;No&quot;)) table_s &lt;- confusionMatrix(reference = test_C$Purchase, data = yhat_s, positive = &quot;Yes&quot;) 분류성능 평가를 위한 결과를 출력해 보자. table_s$table ## Reference ## Prediction No Yes ## No 1222 47 ## Yes 420 57 table_s$overall[1] ## Accuracy ## 0.7325315 table_s$byClass[c(1,2,5,7)] ## Sensitivity Specificity Precision F1 ## 0.5480769 0.7442144 0.1194969 0.1962134 모형 fit에 의한 결과와 비교하면 정분류율과 특이도를 제외한 다른 측도는 모두 향상되었고, 특히 민감도는 크게 향상된 것을 볼 수 있다. 이번에는 모형 fit을 대상으로 분류기준값을 변경하는 방법을 적용해 보자. 모형 fit의 민감도는 0.029에 불구하지만, 특이도는 0.994으로 상당히 높은 값이다. 지나칠 정도로 높은 특이도를 조금 희생하더라도 민감도를 대폭 향상시킬 수 있는 방안으로 (민감도+특이도)를 최대화할 수 있는 최적 분류기준값을 찾아서 적용하는 것을 생각해 볼 수 있다. 여기에서 조심해야 하는 것은 최적 분류 기준값을 찾는 작업을 training data나 test data를 대상으로 진행해서는 안 된다는 것이다. Training data를 대상으로 진행하면 overfitting의 문제가 생길 수 있으며, test data를 대상으로는 모형에 대한 어떠한 튜닝 작업도 진행해서는 안 되기 때문이다. 따라서 기존의 test data를 validation data와 test data로 다시 분리하고, validation data를 대상으로 최적 분류 기준값을 찾아야 할 것이다. test_C를 절반으로 분리해서 validation data valid_C와 test data test_C1으로 나누자. set.seed(123) x.id &lt;- createDataPartition(test_C$Purchase, p = 0.5, list = FALSE)[,1] valid_C &lt;- test_C |&gt; slice(x.id) test_C1 &lt;- test_C |&gt; slice(-x.id) 최적 분류 기준값을 찾기 위해 먼저 모형 fit을 사용하여 validation data를 대상으로 확률 예측을 실시한다. pred_val &lt;- predict(fit, newdata = valid_C, type=&quot;response&quot;) 최적 분류 기준값을 찾는 작업은 패키지 pROC의 함수 coords()으로 할 수 있다. 기본적인 사용법은 coords(roc, x = \"best\", transpose = TRUE)이다. roc는 함수 roc()로 생성된 객체이고, x = \"best\"를 지정하면 (민감도+특이도)를 최대화하는 분류 기준값을 찾는다. transpose는 결과의 출력 형태에 대한 것으로, TRUE를 지정하면 벡터 형태로 결과가 출력된다. library(pROC) thres &lt;- roc(valid_C$Purchase, pred_val) |&gt; coords(x = &quot;best&quot;, transpose = TRUE) thres ## threshold specificity sensitivity ## 0.07685473 0.78197320 0.51923077 (민감도+특이도)를 최대화하는 최적 분류 기준값은 0.0768547로 구해졌고, validation data를 대상으로는 민감도가 0.5192308, 특이도는 0.7819732로 계산되었다. 이제 변경된 분류 기준값을 사용하여 test data를 대상으로 다시 분류 작업을 진행해 보자. pred_t &lt;- predict(fit, newdata = test_C1, type = &quot;response&quot;) yhat_t &lt;- factor(if_else(pred_t &gt;= thres[1], &quot;Yes&quot;, &quot;No&quot;)) table_t &lt;- confusionMatrix(reference = test_C1$Purchase, data = yhat_t, positive = &quot;Yes&quot;) 분류성능 평가를 위한 결과를 출력해 보자. table_t$table ## Reference ## Prediction No Yes ## No 636 21 ## Yes 185 31 table_t$overall[1] ## Accuracy ## 0.7640321 table_t$byClass[c(1,2,5,7)] ## Sensitivity Specificity Precision F1 ## 0.5961538 0.7746650 0.1435185 0.2313433 민감도가 크게 향상된 것을 볼 수 있다. 민감도가 특히 중요한 분석에서 시도할 수 있는 방법이다. 2.7 실습 예제 "],["chapter-tree-model.html", "3 장 Tree-based 모형 3.1 Decision tree 모형 3.2 Bagging 3.3 Random Forest 3.4 Boosting", " 3 장 Tree-based 모형 1장에서는 반응변수가 연속형인 경우에 적용되는 회귀모형에 대해 살펴보았고, 2장에서는 반응변수가 이항변수인 경우에 적용되는 분류모형에 대해 살펴보았다. 지금부터는 연속형 반응변수에 적용되는 회귀모형과 이항반응변수에 적용되는 분류모형에 모두 적용될 수 있는 “tree-based” 모형에 대해 살펴보고자 한다. Tree-based 모형은 기본적으로 설명변수 공간의 분할을 근거로 하고 있다. 설명변수의 공간의 분할 결과는 tree 형태로 표현되며, 모형의 예측은 동일 공간에 속한 자료의 평균값(회귀모형)이거나, 최빈값(분류모형)으로 이루어진다. Tree-based 모형은 해석이 쉽고 편하다는 장점이 있으나, 1장이나 2장에서 살펴본 모형에 비하여 예측의 정확도나 효율성이 상당히 떨어진다는 문제가 있다. 예측의 정확도를 높이기 위해 대안으로 제시되는 tree-based 모형에는 Bagging, random forest, boosting 모형이 있다. 이 방식들의 공통점은 많은 수의 tree 모형을 적합시키고, 그 결과를 결합해서 예측하는 것이다. 3.1 Decision tree 모형 3.1.1 Regression tree 모형 연속형 반응변수에 대한 tree 모형을 regression tree 모형이라고 한다. Regression tree 모형에 대한 예제 자료로 패키지 ISLR에 있는 데이터 프레임 Hitters를 사용해 보자. Hitters는 MLB 1986~87 시즌 322명 선수의 연봉에 대한 자료이다. data(Hitters, package=&quot;ISLR&quot;) library(tidyverse) Hitters |&gt; as_tibble() |&gt; print(n = 5) ## # A tibble: 322 × 20 ## AtBat Hits HmRun Runs RBI Walks Years CAtBat CHits CHmRun CRuns CRBI ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 293 66 1 30 29 14 1 293 66 1 30 29 ## 2 315 81 7 24 38 39 14 3449 835 69 321 414 ## 3 479 130 18 66 72 76 3 1624 457 63 224 266 ## 4 496 141 20 65 78 37 11 5628 1575 225 828 838 ## 5 321 87 10 39 42 30 2 396 101 12 48 46 ## # ℹ 317 more rows ## # ℹ 8 more variables: CWalks &lt;int&gt;, League &lt;fct&gt;, Division &lt;fct&gt;, ## # PutOuts &lt;int&gt;, Assists &lt;int&gt;, Errors &lt;int&gt;, Salary &lt;dbl&gt;, NewLeague &lt;fct&gt; log(Salary)를 반응변수, Years와 Hits를 설명변수로 하는 regression tree를 구성해 보자. 적합된 tree 모형은 그림 3.1에서 볼 수 있다. 적합된 tree 모형은 ‘root node’를 시작으로 Years &lt; 4.5를 기준으로 첫 번째 분리가 이루어졌고, Hits &lt; 117.5를 기준으로 두 번째 분리가 이루어진 것을 알 수 있다. 두 번의 분리로 새 개의 ’terminal node’ 혹은 ’leaves’가 생성되었다. 그림 3.1: Hitters 자료에 대한 regression tree 모형 적합 결과 Tree 모형의 적합 과정은 설명변수의 공간을 분할하여, 동일 공간에 속한 반응변수 자료의 동질성을 최대화하는 것이다. 그림 3.1에 표현된 tree 모형은 설명변수 Years와 Hits로 구성된 2차원 공간을 log(Salary)가 동일 공간에서는 최대한 유사한 값을 갖도록 세 개 공간으로 분할한 것이다. 분할된 공간 R1, R2, R3를 표현한 그래프는 그림 3.2에서 볼 수 있다. 그림 3.2: Hitters 자료에 대한 regression tree 모형의 설명변수 공간 분할 \\(\\bullet\\) Tree 모형 적합: 설명변수 공간의 분할 Tree 모형 적합을 위한 설명변수의 공간 분할 방법에 대해 살펴보자. 공간 분할의 기본적인 목표는 동일 공간에 속한 자료들의 동질성을 최대화하는 것이며, 연속형 자료의 동질성은 분산의 개념을 이용해서 측정할 수 있다. 즉, 설명변수 \\(X_{1}, X_{2}, \\ldots, X_{k}\\) 로 구성되는 전체 공간을 서로 겹치지 않는 \\(J\\) 개의 공간으로 분리하고자 한다면, 예측오차의 평가측도인 식 (3.1)의 RSS가 최소가 되도록 \\(R_{1}, R_{2}, \\ldots, R_{J}\\) 를 구성하는 것이다. \\[\\begin{equation} RSS = \\sum_{j=1}^{J} \\sum_{i \\in R_{j}} \\left(y_{i}-\\overline{y}_{R_{j}} \\right)^{2} \\tag{3.1} \\end{equation}\\] 그러나 공간 분할에 대한 구체적인 제약 조건이 없는 상태에서 식 (3.1)을 최소화시키는 최적 분할을 찾는 것은 불가능한 문제이다. 우리가 적용하려는 방식은 이른바 “top-down” 방식 혹은 “recursive binary splitting”이라고 하는 방식이다. 단계적으로 RSS를 가장 크게 감소시킬 수 있는 분할을 실시하되, 이전 단계의 분할로 구성된 영역 중 하나를 선택해서 두 개의 영역으로 분리하는 것인데, 이러한 분할은 설명변수 \\(X_{1}, X_{2}, \\ldots, X_{k}\\) 중 한 변수를 선택하고, 이어서 해당 변수의 분할 기준점을 찾는 작업으로 이루어진다. 분할 전 RSS는 전체 자료가 대상이므로 \\(RSS = \\sum \\left(y_{i}-\\overline{y}\\right)^{2}\\) 가 된다. 첫 번째 분할은 식 (3.2)의 RSS를 최소화시키는 분할 \\(R_{1}\\) 과 \\(R_{2}\\) 를 구성하는 것이다. 단, \\(R_{1}\\)은 \\(X_{j}&lt;s\\) 를 만족시키는 공간이고, \\(R_{2}\\)는 \\(X_{j} \\geq s\\) 를 만족시키는 공간이다. \\[\\begin{equation} RSS = \\sum_{i \\in R_{1}} \\left(y_{i}-\\overline{y}_{R_{1}}\\right)^{2} + \\sum_{i \\in R_{2}} \\left(y_{i}-\\overline{y}_{R_{2}}\\right)^{2} \\tag{3.2} \\end{equation}\\] 두 번째 분할은 첫 번째 분할로 구성된 2개의 영역 중에서 RSS를 가장 크게 감소시킬 수 있도록 한 영역을 선택하고 분할 기준점을 찾아 두 영역으로 분리하는 것이다. 두 번째 분할이 이루어지면 총 3개의 영역이 구성된다. 이후의 분할 과정도 동일한 방식으로 진행되며, 미리 설정된 stopping rule이 만족될 때까지 반복된다. \\(\\bullet\\) Tree pruning 예제로 ISLR::Hitters에서 log(Salary)를 반응변수로, Years, Hits, RBI, Walks, PutOuts를 설명변수로 하는 tree 모형의 적합결과를 그래프로 나태내 보자. 그림 3.3: Hitters 자료에 대한 regression tree 모형 적합 결과 적합된 tree 모형은 19개의 terminal nodes가 있는 비교적 복잡한 형태의 모형이다. 지나치게 많은 분할이 이루어진 tree 모형은 training data의 세밀한 특징을 잘 나타낼 수 있겠지만, test data와 같은 새로운 자료에 대해서는 매우 부정확한 예측을 하게 되는 이른바 overfitting의 문제가 있을 수 있다. 반면에 작은 횟수의 분할로 적합된 작은 크기의 tree 모형은 training data에 대한 설명력이 조금은 떨어져서 bias가 커지는 문제가 있겠지만, 예측 결과의 변동성은 낮아지는, 즉 variance가 작아지는 효과를 볼 수 있다. 따라서 가장 적절한 크기의 tree 모형을 선택할 수 있는 방법이 필요하며, 이것은 tree pruning이라고 한다. 우리가 적용할 방법은 모형의 복잡성, 즉 tree 모형의 크기가 미치는 영향력을 조절하는 tuning parameter를 사용하는 방법이다. 식 (3.1)에 정의된 RSS는 분할이 진행되어 모형의 복잡성이 높아질수록 감소하는 특징을 갖고 있는데, 여기에 모형의 복잡성을 penalty로 추가함으로써, 모형의 크기를 고려한 예측오차를 정의할 수 있다. \\[\\begin{equation} RSS_{c_{p}} = RSS + c_{p} \\cdot |T| \\tag{3.3} \\end{equation}\\] 단, \\(|T|\\) 는 tree 모형의 terminal nodes의 개수이고, \\(c_{p}\\) 는 0 또는 양수를 값으로 갖는 tuning parameter이다. \\(c_{p}\\) 의 역할은 모형의 복잡성, 즉 tree 모형 크기의 영향력을 조절하는 것으로써, 만일 \\(c_{p}=0\\) 이면 \\(RSS_{c_{p}}\\) 는 식 (3.1)에 정의된 RSS와 동일한 것이 되며, \\(c_{p}\\) 의 값이 증가함에 따라 tree 모형의 크기는 감소하게 된다. 최적 \\(c_{p}\\) 값은 cross-validataion에 의한 예측오차 계산으로 선택할 수 있다. \\(\\bullet\\) Cross-validation Test error는 통계모형의 적합에 사용되지 않은 새로운 자료에 대한 예측 오차를 의미하며, 특정 통계모형에 의한 예측의 정당성을 확보하기 위해서는 test error가 낮다는 것을 확인할 수 있어야 한다. 그렇다면 Test error를 효과적으로 추정할 수 있는 방법이 무엇인지 살펴보자. 전체 data를 training data와 test data로 분리해서 test data set을 확보할 수 있지만, 충분한 양을 확보하기에는 현실적인 어려움이 있을 수 있다. Cross-validation은 training data를 이용한 test error의 추정방법이다. Training data 중 일부분을 모형적합 과정에서 제외해서 test 목적으로 사용하는 것인데, 자료의 제외 방식 등에 따라서 몇 가지 방법으로 구분된다. 그 중에서 “leave-one-out cross-validation”과 “k-fold cross-validation”에 대해 살펴보자. Leave-one-out cross-validation (LOOCV) 전체 \\(n\\) 개의 자료 중 한 개의 자료를 모형 적합에서 차례로 제외하고 test 목적으로 사용하는 방식이다. 즉, 자료 \\((\\mathbf{x}_{1}, y_{1}), \\ldots, (\\mathbf{x}_{n}, y_{n})\\) 중 \\((\\mathbf{x}_{i}, y_{i})\\), \\(i=1, 2, \\ldots, n\\) 을 제외하고 적합한 모형으로 \\(y_{i}\\) 를 예측하는 과정을 \\(n\\) 번 반복하며, 그 과정에서 발생한 오차를 근거로 test error를 다음과 같이 추정하는 방식이다. \\[\\begin{equation} \\frac{1}{n} \\sum_{i=1}^{n} \\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\tag{3.4} \\end{equation}\\] 전체 자료 중 한 개의 자료만 제외되므로 자료의 크기가 큰 대규모의 자료인 경우에는 많은 계산 과정이 필요한 방식이다. 대부분의 자료가 사용되기 때문에 예측 bias는 작겠지만, \\(n\\) 번의 적합 과정에서 사용된 자료가 거의 비슷하기 떄문에, 각 모형의 예측 결과 사이에는 높은 상관관계가 존재하게 되고, 따라서 예측 결과의 분산이 커지는 문제가 있다. k-fold cross-validation 먼저 training data를 비슷한 크기의 \\(k\\) 개의 그룹(fold)으로 구분한다. 이어서 그 중 한 그룹의 자료를 제외하고 나머지 자료만으로 모형을 적합한 후에 제외된 한 그룹 자료의 반응변수를 예측하고 예측 오차를 계산한다. 이 과정을 \\(k\\) 번 차례로 반복하면 \\(k\\) 개의 예측 오차를 얻게 되는데, 그 오차의 평균으로 test error를 추정하는 방식이다. LOOCV에 비해 계산 과정이 단순한 방식인데, 모형적합 과정에 사용된 자료의 개수가 LOOCV 보다 작기 때문에 bias는 더 클 수 있다. 그러나 한 그룹의 자료가 차례로 제외된 상태에서 \\(k\\) 번의 모형 적합이 이루어지기 때문에, 각 모형 적합에 사용된 자료 중 겹치는 자료의 비율은 더 낮아지게 되고, 따라서 예측 결과 사이의 상관관계가 더 낮게 형성될 수 있어서, 예측의 분산을 낮출 수 있는 효과가 있다. \\(k\\) 값을 조정하면 bias와 variance 사이의 trade-off가 가능한데, 일반적으로 사용되는 것은 \\(k=5\\) 또는 \\(k=10\\) 이다. \\(\\bullet\\) 예제: MASS::Boston Boston은 보스톤 지역의 주택 가격에 대한 자료이다. data(Boston, package = &quot;MASS&quot;) str(Boston) ## &#39;data.frame&#39;: 506 obs. of 14 variables: ## $ crim : num 0.00632 0.02731 0.02729 0.03237 0.06905 ... ## $ zn : num 18 0 0 0 0 0 12.5 12.5 12.5 12.5 ... ## $ indus : num 2.31 7.07 7.07 2.18 2.18 2.18 7.87 7.87 7.87 7.87 ... ## $ chas : int 0 0 0 0 0 0 0 0 0 0 ... ## $ nox : num 0.538 0.469 0.469 0.458 0.458 0.458 0.524 0.524 0.524 0.524 ... ## $ rm : num 6.58 6.42 7.18 7 7.15 ... ## $ age : num 65.2 78.9 61.1 45.8 54.2 58.7 66.6 96.1 100 85.9 ... ## $ dis : num 4.09 4.97 4.97 6.06 6.06 ... ## $ rad : int 1 2 2 3 3 3 5 5 5 5 ... ## $ tax : num 296 242 242 222 222 222 311 311 311 311 ... ## $ ptratio: num 15.3 17.8 17.8 18.7 18.7 18.7 15.2 15.2 15.2 15.2 ... ## $ black : num 397 397 393 395 397 ... ## $ lstat : num 4.98 9.14 4.03 2.94 5.33 ... ## $ medv : num 24 21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 ... 변수 medv를 반응변수로 하는 tree 모형을 적합해 보자. 자료탐색 과정은 생략하고 tree 모형적합 절차만을 살펴보도록 하자. 분석의 첫 번째 단계는 전체 자료를 training data와 test data로 분리하는 것이다. library(caret) library(tidyverse) set.seed(123) train.id &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)[,1] train_B &lt;- Boston |&gt; slice(train.id) test_B &lt;- Boston |&gt; slice(-train.id) Tree 모형의 적합은 패키지 caret의 함수 train()으로 진행할 것이다. 패키지 caret은 Machine learning에 최적화된 패키지로써 분석에 필수적인 기능을 모두 포함하고 있으며, 다양한 modelling 기법을 표준화된 방식으로 사용할 수 있다. 함수 train()은 ML 모형의 적합을 위해 사용되는 함수인데, 239 종류의 ML 모형 적합이 가능하다. 작동 방식은 각 ML 모형의 적합을 위한 패키지에서 필요한 함수를 불러와 적용시키는 것인데, 표준화된 방식을 사용하기 때문에 사용자가 편하게 사용할 수 있다. 모형 선택은 method에 각 ML 모형의 키워드를 지정하면 된다. Tree 모형의 경우에는 method = 'rpart'를 입력하면 되며, 필요한 패키지인 rpart는 설치되어야 한다. 또한 각 ML 모형마다 필요한 tuning parameter를 TuneLength 또는 TuneGrid를 통해서 지정할 수 있다. 함수 train()을 사용해서 tree 모형을 적합해 보자. 모형 적합은 식 (3.3)에 정의된 방식을 사용하되, 10-fold CV으로 tuning parameter \\(c_{p}\\) 의 최적 값을 찾아보자. set.seed(1) m1 &lt;- train(medv ~ ., data = train_B, method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &#39;cv&#39;, number = 10)) 함수 train()에서 모형 설정은 함수 lm()과 동일하게 R formula 방식을 사용할 수 있으며, 반응변수가 연속형이면 regression tree 모형이 적합된다. 요소 method에 \"rpart\"를 지정하면, 예측 오차가 최소인 tree 모형을 적합한다. 요소 tuneLength에는 최적 tuning parameter 값을 찾기 위한 grid의 길이를 지정할 수 있다. 요소 trControl은 모형의 적합 및 평가 방식 등을 설정하는 기능을 갖고 있는데, 함수 trainControl()을 사용해서 지정하게 된다. 함수 trainControl()의 요소 method에는 적용되는 CV 방식을 정할 수 있는데, method = 'cv', number = 10은 10-fold CV를 의미한다. 적합된 결과를 살펴보자. m1 ## CART ## ## 356 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 320, 320, 320, 321, 321, 320, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.007670005 4.357423 0.7517554 3.106724 ## 0.008610415 4.397989 0.7485334 3.119257 ## 0.011358068 4.557210 0.7303147 3.248633 ## 0.011952330 4.559965 0.7298686 3.259467 ## 0.021800452 4.646888 0.7229350 3.341879 ## 0.027794282 4.731511 0.7157403 3.421950 ## 0.034497434 4.879750 0.6953376 3.505237 ## 0.080160736 5.497892 0.6318944 3.903550 ## 0.165305341 6.862030 0.4455311 4.986384 ## 0.462979695 8.276882 0.2938345 6.124176 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was cp = 0.007670005. 10-fold CV로 계산된 예측 오차의 RMSE를 근거로 최적 모형이 선택되었다. 다른 측도인 Rsquared 또는 MAE를 근거로 모형을 선택하기 위해서는 요소 metric에 해당 문자를 지정해야 한다. set.seed(1) train(medv ~ ., data = train_B, method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &#39;cv&#39;, number = 10), metric = &quot;Rsquared&quot;) ## CART ## ## 356 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 320, 320, 320, 321, 321, 320, ... ## Resampling results across tuning parameters: ## ## cp RMSE Rsquared MAE ## 0.007670005 4.357423 0.7517554 3.106724 ## 0.008610415 4.397989 0.7485334 3.119257 ## 0.011358068 4.557210 0.7303147 3.248633 ## 0.011952330 4.559965 0.7298686 3.259467 ## 0.021800452 4.646888 0.7229350 3.341879 ## 0.027794282 4.731511 0.7157403 3.421950 ## 0.034497434 4.879750 0.6953376 3.505237 ## 0.080160736 5.497892 0.6318944 3.903550 ## 0.165305341 6.862030 0.4455311 4.986384 ## 0.462979695 8.276882 0.2938345 6.124176 ## ## Rsquared was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.007670005. 적합된 tree 모형을 그래프로 표현해 보자. 패키지 rpart.plot의 함수 rpart.plot()을 사용하면 된다. library(rpart.plot) rpart.plot(m1$finalModel, roundint=FALSE, digits = 3) 그림 3.4: Boston 자료의 tree 모형 적합 결과 함수 rpart.plot()의 요소 roundint는 분리 기준 숫자를 정수로 반올림해서 표시할 것인지를 정하는 것이고, digits는 유효숫자를 지정하는 것이다. 그림 3.4의 각 node에 숫자가 표시되어 있는데, 첫 번째 숫자는 각 node에 속한 자료의 변수 medv의 평균값이고, 두 번째 숫자는 각 node에 속한 자료의 비율이다. 최적 tree 모형을 선택하는 기준으로 ’최소예측오차’가 많이 사용되고 있는데, ’one-standard-error(1SE) rule’을 사용하는 것도 괜찮은 대안이 될 수 있다. 1SE rule은 (최소 예측 오차 + 1SE) 범위에 포함되는 tree 모형 중 크기가 가장 작은 모형을 최적 모형으로 선택하는 방법이다. 함수 train()에서는 method = \"rpart1SE\"를 지정하면 된다. 이 경우에는 적용되는 tuning parameter가 없기 때문에 tuneLength는 필요 없다. set.seed(1) m2 &lt;- train(medv ~ ., data = train_B, method = &quot;rpart1SE&quot;, trControl = trainControl(method = &#39;cv&#39;, number = 10)) 적합 결과를 그래프로 확인해 보자. rpart.plot(m2$finalModel, roundint=FALSE, digits = 3) 그림 3.5: Boston 자료의 1SE rule에 의한 tree 모형 적합 결과 그림 3.4에 표현된 최소 RMSE 모형보다 node의 개수가 하나 작은 것을 알 수 있다. 이제 적합된 두 tree 모형을 이용하여 test data에 대한 예측 실시해 보자. 예측은 함수 predict()로 할 수 있다. pred_1 &lt;- predict(m1, newdata = test_B) pred_2 &lt;- predict(m2, newdata = test_B) 예측 오차의 확인은 caret의 함수 defaultSummary()로 할 수 있다. Test data의 반응변수와 예측 결과를 각각 obs와 pred라는 이름의 열로 구성한 데이터 프레임을 만들어서 입력하면 된다. defaultSummary(data.frame(pred = pred_1, obs = test_B$medv)) ## RMSE Rsquared MAE ## 5.4362182 0.6820792 3.2849165 defaultSummary(data.frame(pred = pred_2, obs = test_B$medv)) ## RMSE Rsquared MAE ## 5.5684877 0.6671744 3.4993889 3.1.2 Classification tree 모형 Classification tree 모형은 반응변수가 범주형 변수인 경우에 적용되는 tree 모형으로써, 로지스틱 회귀모형처럼 분류가 주된 목적으로 사용되는 모형이다. 모형 설정 방법은 3.1.1절에서 살펴본 regression tree 모형의 경우와 동일하게 설명변수 공간에 대한 recursive binary splitting으로 같은 공간에 속한 자료의 동질성을 더 높이도록 분할이 이루어진다. 자료의 동질성 측도로 regression tree에서는 RSS를 사용했는데, 분류 목적에는 적합하지 않은 측도가 된다. 분류가 목적인 모형에서는 동일 범주에 속한 자료의 비율이 중요한 동질성 측도가 되는데, classification tree에서는 Gini index와 entropy로 동질성을 측정한다. 두 개의 범주로 이루어진 이항 반응변수에 대한 Gini index와 entropy의 정의를 살펴보자. \\(p_{1}\\) 과 \\(p_{2}\\) 를 각각 첫 번째 범주와 두 번째 범주에 속할 확률이라고 하면, Gini index는 다음과 같다. \\[\\begin{equation} p_{1}(1-p_{1}) + p_{2}(1-p_{2}) = 2p_{1}p_{2} \\tag{3.5} \\end{equation}\\] Entropy는 다음과 같다. \\[\\begin{equation} -(p_{1} \\log p_{1} + p_{2} \\log p_{2}) \\tag{3.6} \\end{equation}\\] Gini index와 entropy는 모두 \\(p_{i}\\) 가 0 또는 1에 가까운 값을 가질수록 작은 값을 갖게 되는 측도인데, \\(p_{i}\\) 가 0 또는 1에 가까운 값을 갖는다는 것은 곧 자료의 동질성이 높다는 것을 의미한다. 따라서 자료의 동질성이 높아질수록 작은 값을 갖게 되는 측도이다. \\(\\bullet\\) 예제: Mroz 자료 2.3절에서 살펴본 Mroz 자료를 대상으로 classification tree 모형을 적합해 보자. Mroz 자료는 미국 여성의 직업 참여에 대한 자료이며, 반응변수는 lfp이다. data(Mroz, package = &quot;carData&quot;) str(Mroz) ## &#39;data.frame&#39;: 753 obs. of 8 variables: ## $ lfp : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ k5 : int 1 0 1 0 1 0 0 0 0 0 ... ## $ k618: int 0 2 3 3 2 0 2 0 2 2 ... ## $ age : int 32 30 35 34 31 54 37 54 48 39 ... ## $ wc : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 2 1 2 1 1 1 ... ## $ hc : Factor w/ 2 levels &quot;no&quot;,&quot;yes&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ lwg : num 1.2102 0.3285 1.5141 0.0921 1.5243 ... ## $ inc : num 10.9 19.5 12 6.8 20.1 ... 함수 caret::createDataPartition()으로 자료분리를 진행해 보자. library(tidyverse) library(caret) set.seed(123) x.id &lt;- createDataPartition(Mroz$lfp, p = 0.8, list = FALSE)[,1] train_M &lt;- Mroz |&gt; slice(x.id) test_M &lt;- Mroz |&gt; slice(-x.id) 함수 caret::train()으로 tree 모형을 적합시켜 보자. 요인이 반응변수로 지정되면 classification tree 모형이 적합된다. set.seed(1234) m1 &lt;- train(lfp ~ ., data = train_M, method = &quot;rpart&quot;, tuneLength = 10, trControl = trainControl(method = &#39;cv&#39;, number = 10)) 적합 결과를 확인해 보자. m1 ## CART ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... ## Resampling results across tuning parameters: ## ## cp Accuracy Kappa ## 0.001282051 0.7261749 0.4461664 ## 0.003846154 0.7345082 0.4641999 ## 0.005769231 0.7312022 0.4571493 ## 0.007692308 0.7362022 0.4661105 ## 0.011538462 0.7395902 0.4742985 ## 0.012820513 0.7495902 0.4974608 ## 0.034615385 0.7047814 0.4029266 ## 0.042307692 0.6997814 0.3951448 ## 0.142307692 0.6485246 0.3273295 ## 0.192307692 0.5804372 0.1005759 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01282051. 정분류율인 Accuracy가 최적 모형선택의 디폴트 기준임을 알 수 있다. 다른 분류 평가 측도를 사용해서 최적 모형을 선택해 보자. 다른 평가 측도를 사용하기 위해서는 해당 측도의 값을 계산해야 하는데, 함수 trainControl()의 요소 summaryFunction에 키워드를 지정해야 해당 측도의 계산이 진행된다. ROC(area under ROC curve), Sens(Sensitivity), Spec(Specificity)의 계산을 위한 키워드는 twoClassSummary이고, AUC(area under Precision-Recall curve), Precision, Recall, F(F1 score)의 계산을 위한 키워드는 prSummary이다. 또한 함수 trainControl()의 요소 classProbs에는 TRUE를 지정해야 ROC curve 및 Precision-Recall curve 작성을 위해 각 범주에 속할 확률을 계산한다. 평가 측도 ROC에 의한 최적 모형을 선택해 보자. set.seed(1234) train(lfp ~ ., data = train_M, method = &quot;rpart&quot;, tuneLength = 10, metric = &quot;ROC&quot;, trControl=trainControl(method = &#39;cv&#39;, number = 10, classProbs = TRUE, summaryFunction = twoClassSummary) ) ## CART ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.001282051 0.8066290 0.7153846 0.7347899 ## 0.003846154 0.8070249 0.7346154 0.7347899 ## 0.005769231 0.7883904 0.7269231 0.7347899 ## 0.007692308 0.7843132 0.7230769 0.7465546 ## 0.011538462 0.7666451 0.7346154 0.7434454 ## 0.012820513 0.7654040 0.7653846 0.7376471 ## 0.034615385 0.7424580 0.6923077 0.7142017 ## 0.042307692 0.7361797 0.7038462 0.6965546 ## 0.142307692 0.6881464 0.8538462 0.4929412 ## 0.192307692 0.5550679 0.3730769 0.7370588 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.003846154. 평가 측도 F에 의한 최적 모형을 선택해 보자. set.seed(1234) train(lfp ~ ., data = train_M, method = &quot;rpart&quot;, tuneLength = 10, metric = &quot;F&quot;, trControl=trainControl(method = &#39;cv&#39;, number = 10, classProbs = TRUE, summaryFunction = prSummary) ) ## CART ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 542, 543, 543, 543, 543, 543, ... ## Resampling results across tuning parameters: ## ## cp AUC Precision Recall F ## 0.001282051 0.62943815 0.6756925 0.7153846 0.6910425 ## 0.003846154 0.60254829 0.6809178 0.7346154 0.7032688 ## 0.005769231 0.60796211 0.6803280 0.7269231 0.6983203 ## 0.007692308 0.60584754 0.6916301 0.7230769 0.7008195 ## 0.011538462 0.56073958 0.6888776 0.7346154 0.7090688 ## 0.012820513 0.51928620 0.6956948 0.7653846 0.7264468 ## 0.034615385 0.41024975 0.6486295 0.6923077 0.6680326 ## 0.042307692 0.35974154 0.6387577 0.7038462 0.6673466 ## 0.142307692 0.07811750 0.5653613 0.8538462 0.6770412 ## 0.192307692 0.01272258 0.5189849 0.3730769 0.6664240 ## ## F was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.01282051. 모형 m1의 적합 결과를 그래프로 표현해 보자. library(rpart.plot) rpart.plot(m1$finalModel, roundint = FALSE, digits = 3) 그림 3.6: Mroz 자료의 tree 모형 적합 결과 그림 3.6의 각 node에 3가지 결과가 표시되어 있는데, 첫 번째는 해당 node의 다수 범주를 표시하고 있고, 두 번째 숫자는 두 번째 범주인 “yes” 범주에 속한 자료의 비율이 표시되어 있다. 즉, root node인 전체 자료의 다수 범주는 “yes”이고, 그 비율이 0.569라는 것이다. 세 번째 백분율은 각 node에 속한 자료의 비율을 나타내고 있다. 모형 m1으로 test data에 대한 예측을 함수 predict()로 실시해 보자. pred_1 &lt;- predict(m1, newdata = test_M) pred_1[1:10] ## [1] yes no yes yes yes no yes yes no yes ## Levels: no yes 함수 train()으로 생성된 객체에 함수 predict()로 예측을 실시하면 type = \"raw\"가 디폴트로 적용되는데, 연속형 반응변수인 regression tree 모형의 경우에는 mean 값이 출력되고, 이항 반응변수인 classification tree 모형의 경우에는 class가 출력된다. 만일 각 그룹에 속할 확률을 출력하고자 하면, type = \"prob\"를 지정해야 한다. predict(m1, newdata = test_M[1:5,], type = &quot;prob&quot;) ## no yes ## 3 0.1111111 0.8888889 ## 13 0.7268519 0.2731481 ## 18 0.1111111 0.8888889 ## 22 0.2280702 0.7719298 ## 27 0.1111111 0.8888889 Test data에 대한 모형 m1의 분류 성능을 함수 caret::confusionMatrix()로 평가해 보자. confusionMatrix(data = pred_1, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 45 25 ## yes 20 60 ## ## Accuracy : 0.7 ## 95% CI : (0.6199, 0.772) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 0.0005439 ## ## Kappa : 0.3946 ## ## Mcnemar&#39;s Test P-Value : 0.5509850 ## ## Sensitivity : 0.7059 ## Specificity : 0.6923 ## Pos Pred Value : 0.7500 ## Neg Pred Value : 0.6429 ## Precision : 0.7500 ## Recall : 0.7059 ## F1 : 0.7273 ## Prevalence : 0.5667 ## Detection Rate : 0.4000 ## Detection Prevalence : 0.5333 ## Balanced Accuracy : 0.6991 ## ## &#39;Positive&#39; Class : yes ## 3.2 Bagging 3.1절에서 살펴본 decision tree 모형의 가장 큰 문제는 분산이 매우 크다는 점이다. 분산이 큰 모형은 자료가 조금만 달라져도 적합 결과에 큰 변동이 발생하게 되는데, 예측 모형에게는 심각한 결합이 되는 문제라 할 수 있다. 분산을 낮추는 방법으로 여러 개의 독립된 training data를 생성해서 각각의 training data에 대한 tree 모형을 적합하고, 추정된 각각의 모형에서 생성된 예측값들의 평균을 최종 예측 결과로 사용하는 것을 생각해 볼 수 있다. 이 방법은 분산이 \\(\\sigma^{2}\\) 인 독립된 \\(n\\) 개의 관찰값 \\(X_{1}, X_{2}, \\ldots, X_{n}\\) 의 평균 \\(\\overline{X}\\) 는 분산이 \\(\\sigma^{2}/n\\) 이 되어서 개별 관찰값보다 더 작은 분산을 가질 수 있다는 사실에 근거로 두고 있다. 문제는 여러 개의 독립된 training data를 모집단에서 다시 생성하는 것이 사실상 불가능하다는 것이다. 따라서 기존의 자료에서 여러 개의 표본을 다시 생성하는 방법을 고려해야 하는데, Bootstrap이 좋은 대안이 될 수 있다. Bootstrap은 추정량이나 예측모형 등의 불확실성을 탐색하기 위한 매우 유용한 도구로 사용되는 resampling 기법이다. 어떤 추정량의 추정 결과에 대한 정확도 등을 평가하기 위해서는 해당 추정량의 표본분포가 반드시 필요하다. 추정량의 표본분포란 모집단에서 반복적으로 추출한 임의표본으로 계산한 추정량 값의 분포를 의미한다. 일반적으로는 이론적으로 추정량의 표본분포를 유도하지만, 이론적으로 유도하기 어려운 형태의 추정량도 많이 있다. 이런 경우에 대안으로 사용할 수 있는 방법이 Bootstrap인데, 기본 개념은 모집단에서 독립된 임의표본을 반복적으로 추출하는 것 대신에 원자료, 즉 원래의 표본자료에서 독립된 임의표본을 반복적으로 추출하는 것이다. 즉, 크기가 \\(n\\) 인 원자료에서 복원추출로 크기가 \\(n\\) 인 임의표본을 추출하는 과정을 \\(B\\) 번 반복해서, \\(B\\) 세트의 독립된 표본을 구성하는 것이다. 복원추출을 사용한 이유는 독립된 임의표본을 추출하기 위함이다. Bootstrap aggregating 또는 Bagging은 training data에서 \\(B\\) 개의 bootstrap sample을 추출하는 것으로 시작한다. \\(B\\) 개의 bootstrap sample은 독립된 training data로 간주할 수 있으며, 각 bootstrap sample에 대한 full size tree 모형을 적합하고, 예측 결과를 통합함으로써 모형의 분산을 크게 낮출수 있는 것이다. \\(B\\) 개의 tree 모형의 예측 결과를 통합해서 최종 예측 결과를 산출하는 방식으로 regression tree에서는 예측값의 평균을 사용하며, classification tree에서는 다수로 분류된 범주를 사용한다. \\(\\bullet\\) caret에 의한 Bagging caret에서 함수 train()으로 bagging을 실행하기 위해서 필요한 패키지는 ipred와 e1071이다. 예제 : MASS::Boston Regression tree 모형에 대한 bagging 예제로써 3.1.1절에서 살펴본 MASS::Boston을 사용해 보자. library(tidyverse) library(caret) data(Boston, package=&quot;MASS&quot;) 자료분리를 실시하자. set.seed(123) train.id &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)[,1] train_B &lt;- Boston |&gt; slice(train.id) test_B &lt;- Boston |&gt; slice(-train.id) 함수 train()으로 bagged tree 모형을 적합하기 위해서는 method = \"treebag\"을 지정해야 한다. ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) set.seed(123) m1_bag &lt;- train(medv ~ ., data = train_B, method = &quot;treebag&quot;, trControl = ctrl) 모형 m1_bag에 입력된 적합 결과를 확인해 보자. m1_bag ## Bagged CART ## ## 356 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... ## Resampling results: ## ## RMSE Rsquared MAE ## 3.943609 0.8147499 2.780642 Bagged tree 모형에서 tuning parameter는 bootstrap sample 추출 반복 횟수 \\(B\\) 이다. nbagg에서 그 횟수를 지정할 수 있으며, 디폴트 횟수는 nbagg = 25이다. nbagg = 30과 nbagg = 50에서 적합해서, 그 결과를 확인해 보자. RMSE, Rsquared와 MAE에서 큰 차이가 없음을 알 수 있다. set.seed(123) train(medv ~ ., data = train_B, method = &quot;treebag&quot;, nbagg = 30, trControl = ctrl)$results ## parameter RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 none 3.955934 0.8135511 2.793155 1.389929 0.1221695 0.7202972 set.seed(123) train(medv ~ ., data = train_B, method = &quot;treebag&quot;, nbagg = 50, trControl = ctrl)$results ## parameter RMSE Rsquared MAE RMSESD RsquaredSD MAESD ## 1 none 3.880447 0.8206622 2.741153 1.403033 0.1215254 0.7128191 단일 tree 모형에 비해 bagged tree 모형은 예측 정확도에서 큰 폭의 개선이 이루어졌지만, 예측 결과에 대한 해석이 쉽다는 tree 모형의 장점은 완전히 사라졌고, 사실상 해석이 불가능한 모형이 되었다. 즉, 개별 변수가 최종 예측에 어떤 영향을 미쳤는지 알 수 없다는 것이다. 비록 최종 예측 결과에 대한 정확한 해석은 힘들어졌지만, 최종 모형에서 각 변수가 차지하는 중요도를 측정할 수는 있다. 개별 변수의 중요도는 각 변수가 tree 분할에 사용되며 감소시킨 RSS의 값으로 측정할 수 있는데, bagged tree 모형에서는 최종 예측에 다수의 tree 모형이 사용되고 있으므로, 각 tree 모형에서 개별 변수가 tree 분할에 사용되며 감소시킨 RSS의 평균값으로 중요도를 측정할 수 있다. 평균 감소폭이 가장 큰 변수가 가장 중요한 변수라고 하겠다. 변수의 중요도를 측정할 수 있는 함수는 caret::varImp()이다. 모형 m1_bag에 대한 변수 중요도를 측정해서 그래프로 표현해 보자. varImp(m1_bag) |&gt; ggplot() 그림 3.7: Boston 자료에 대한 bagged tree 모형의 변수 중요도 변수 lstat이 가장 중요한 변수로 측정되었다. 그림 3.7의 X축에 표시된 각 변수의 중요도 값은 가장 중요한 변수의 평균 RSS 감소폭에 대한 비율을 표시하고 있다. 모형 m1_bag을 사용하여 test data에 대한 예측을 실시하고 결과를 평가해 보자. pred_bag &lt;- predict(m1_bag, test_B) defaultSummary(data.frame(pred = pred_bag, obs = test_B$medv)) ## RMSE Rsquared MAE ## 4.8180947 0.7548929 2.9468445 예측 결과를 그래프로 표현해 보자. tibble(pred = pred_bag, obs = test_B$medv) |&gt; ggplot(aes(x = obs, y = pred)) + geom_point() + geom_abline(intercept = 0, slope = 1) + labs(x = &quot;Observed data&quot;, y = &quot;Predicted data&quot;) 그림 3.8: Boston 자료에 대한 bagged tree 모형의 예측 결과 예제 : carData::Mroz Classification tree 모형에 대한 bagging 예제로써 3.1.2절에서 살펴본 carData::Mroz을 사용해 보자. data(Mroz, package=&quot;carData&quot;) 자료분리를 진행하자. set.seed(123) x.id &lt;- createDataPartition(Mroz$lfp, p = 0.8, list = FALSE)[,1] train_M &lt;- Mroz |&gt; slice(x.id) test_M &lt;- Mroz |&gt; slice(-x.id) Bagged tree 모형을 적합해 보자. ctrl &lt;- trainControl(method = &#39;cv&#39;, number = 10) set.seed(123) m2_bag &lt;- train(lfp ~ ., data = train_M, method = &quot;treebag&quot;, trControl = ctrl) 적합 결과를 확인해 보자. m2_bag ## Bagged CART ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... ## Resampling results: ## ## Accuracy Kappa ## 0.7596995 0.5101126 모형 m2_bag의 변수 중요도를 그래프로 나타내자. Classification 문제에 대한 bagged tree 모형에서 변수의 중요도는 Gini index의 평균 감소폭으로 측정한다. varImp(m2_bag) |&gt; ggplot() 그림 3.9: Mroz 자료에 대한 bagged tree 모형의 변수 중요도 모형 m2_bag을 사용해서 test data에 대한 예측 및 분류를 실시하고 결과를 평가해 보자. pred2_bag &lt;- predict(m2_bag, newdata = test_M) confusionMatrix(data = pred2_bag, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 47 26 ## yes 18 59 ## ## Accuracy : 0.7067 ## 95% CI : (0.6269, 0.7781) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 0.0002919 ## ## Kappa : 0.4112 ## ## Mcnemar&#39;s Test P-Value : 0.2912928 ## ## Sensitivity : 0.6941 ## Specificity : 0.7231 ## Pos Pred Value : 0.7662 ## Neg Pred Value : 0.6438 ## Precision : 0.7662 ## Recall : 0.6941 ## F1 : 0.7284 ## Prevalence : 0.5667 ## Detection Rate : 0.3933 ## Detection Prevalence : 0.5133 ## Balanced Accuracy : 0.7086 ## ## &#39;Positive&#39; Class : yes ## 3.3 Random Forest 3.2절에서 살펴본 bagged tree 모형은 bootstrap으로 생성된 다수의 training data를 대상으로 tree 모형을 적합하고 통합하여 최종 예측을 실시하는 방법으로써 single tree 모형보다 예측 분산을 많이 낮출 수 있다는 장점이 있다. 하지만 나름의 한계가 있는데, 그것은 bootstrap으로 생성된 training data 사이에는 유사성이 존재하기 때문에 모든 training data의 tree 모형 구조가 몇몇 중요한 설명변수에 의하여 공통적으로 결정되어 거의 비슷해질 가능성이 높다는 점이다. 비슷한 구조의 tree 모형을 통합하면 예측 결과 사이의 상관관계를 높이는 효과가 있기 때문에, 분산을 더 낮추기 위해서는 tree 모형 사이의 상관관계를 더 낮추어야 한다. Random forest는 bagging과 동일하게 bootstrap으로 생성된 training data를 대상으로 tree 모형을 적합하고 통합하여 예측을 실시하는 방법이다. 차이점은 통합되는 tree 모형 사이에 상관관계를 더 낮출 수 있는 방법이 적용된다는 점이다. Tree 모형의 building 과정은 설명변수의 공간 분할로 이루어지는데, 매번 분할을 실시할 때마다 설명변수 중 임의로 추출한 \\(m\\) 개의 설명변수만을 대상으로 최적 분할을 실시하게 되면 중요 변수가 제외되어 다른 구조의 tree 모형이 생성될 가능성이 있다. 이렇게 되면 통합되는 tree 모형 사이에 상관관계를 많이 낮출 수 있어서 예측 분산이 더 작아질 수 있게 된다. \\(\\bullet\\) caret에 의한 Random Forest caret에서 함수 train()으로 random forest를 실행하기 위해서는 패키지 randomForest가 설치되어야 하며, method = 'rf'를 지정해야 한다. 예제 : MASS::Boston Regression tree 모형에 대한 random forest 예제로써 3.1.1절에서 살펴본 MASS::Boston을 사용해 보자. data(Boston, package=&quot;MASS&quot;) set.seed(123) train.id &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)[,1] train_B &lt;- Boston |&gt; slice(train.id) test_B &lt;- Boston |&gt; slice(-train.id) Tuning parameter는 임의로 선택할 설명변수의 개수인 mtry이며 tuneLength로 적용될 grid의 길이를 조절한다. Bootstrap sample의 개수는 ntree로 조절하는데, 디폴트는 ntree = 500이다. importance에는 변수 중요도의 계산이 필요하면 TRUE를 지정해야 하는데, 적합되는 tree 모형의 개수가 많기 떄문에 시간이 더 걸리게 된다. ctrl &lt;- trainControl(method=&quot;cv&quot;, number=10) set.seed(123) m1_rf &lt;- train(medv ~ ., data = train_B, method = &quot;rf&quot;, tuneLength = 5, importance = TRUE, trControl = ctrl) 모형의 적합 결과를 살펴보자. m1_rf ## Random Forest ## ## 356 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... ## Resampling results across tuning parameters: ## ## mtry RMSE Rsquared MAE ## 2 3.635893 0.8544814 2.456726 ## 4 3.324916 0.8730989 2.268706 ## 7 3.252076 0.8762433 2.243742 ## 10 3.347749 0.8674172 2.311114 ## 13 3.425963 0.8596083 2.341488 ## ## RMSE was used to select the optimal model using the smallest value. ## The final value used for the model was mtry = 7. Tuning parameter인 mtry에 따른 RMSE의 변화를 그래프로 나타내자. ggplot(m1_rf) 그림 3.10: Boston 자료에 대한 Random Forest의 mtry에 따른 RMSE 변화 모형 m1_rf의 변수 중요도를 측정해서 그래프로 표현해 보자. varImp(m1_rf) |&gt; ggplot() 그림 3.11: Boston 자료에 대한 Random Forest의 변수 중요도 Test data에 대한 예측을 실시하고 평가해 보자. pred_rf &lt;- predict(m1_rf, test_B) defaultSummary(data.frame(pred = pred_rf, obs = test_B$medv)) ## RMSE Rsquared MAE ## 3.6502337 0.8629376 2.3044936 예측 결과를 그래프로 나타내자. tibble(pred = pred_rf, obs = test_B$medv) |&gt; ggplot(aes(x = obs, y = pred)) + geom_point() + geom_abline(intercept = 0, slope = 1) + labs(x = &quot;Observed data&quot;, y = &quot;Predicted data&quot;) 그림 3.12: Boston 자료에 대한 Random Forest의 예측 결과 예제 : carData::Mroz Classification tree 모형에 대한 Random Forest 예제로써 3.1.2절에서 살펴본 carData::Mroz을 사용해 보자. data(Mroz, package=&quot;carData&quot;) set.seed(123) x.id &lt;- createDataPartition(Mroz$lfp, p = 0.8, list = FALSE)[,1] train_M &lt;- Mroz |&gt; slice(x.id) test_M &lt;- Mroz |&gt; slice(-x.id) Random Forest 모형을 적합해 보자. ctrl &lt;- trainControl(method = &quot;cv&quot;, number = 10) set.seed(123) m2_rf &lt;- train(lfp ~ ., data = train_M, method = &quot;rf&quot;, tuneLength = 5, importance = TRUE, trControl = ctrl) 적합 결과를 확인해 보자. m2_rf ## Random Forest ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.7729508 0.5381514 ## 3 0.7713115 0.5339499 ## 4 0.7713388 0.5341556 ## 5 0.7680874 0.5275613 ## 7 0.7646721 0.5218882 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 2. Tuning parameter인 mtry의 값에 따른 모형 m2_rf의 CV Accuracy의 그래프를 작성해 보자. mtry = 2에서 최대값을 갖고 있다. ggplot(m2_rf) 그림 3.13: Mroz 자료에 대한 Random Forest의 mtry에 따른 Accuracy의 변화 모형 m2_rf는 Accuracy를 근거로 선택된 모형이다. 다른 평가 측도인 F1 score를 근거로 모형을 선택해 보자. ctrl_1 &lt;- trainControl(method = &quot;cv&quot;, number = 10, classProbs = TRUE, summaryFunction = prSummary) set.seed(123) m21_rf &lt;- train(lfp ~ ., data = train_M, method = &quot;rf&quot;, tuneLength = 5, metric = &quot;F&quot;, trControl = ctrl_1) 적합 결과를 확인해 보자. m21_rf ## Random Forest ## ## 603 samples ## 7 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 543, 543, 542, 543, 543, 543, ... ## Resampling results across tuning parameters: ## ## mtry AUC Precision Recall F ## 2 0.7045656 0.7366678 0.7576923 0.7444784 ## 3 0.7230455 0.7368191 0.7384615 0.7345407 ## 4 0.7306750 0.7452474 0.7538462 0.7466735 ## 5 0.7348559 0.7449061 0.7500000 0.7450132 ## 7 0.7323882 0.7315206 0.7461538 0.7366194 ## ## F was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 4. Tuning parameter인 mtry의 값에 따른 모형 m21_rf의 CV F1 score의 그래프를 작성해 보자. mtry = 4에서 최대값을 갖고 있다. ggplot(m21_rf) 그림 3.14: Mroz 자료에 대한 Random Forest의 mtry에 따른 Accuracy의 변화 모형 m2_rf와 m21_rf의 변수 중요도를 각각 측정해서 그래프로 나타내자. 그림 3.15: Mroz 자료에 대한 Random Forest의 변수 중요도 Test data에 대한 모형 m2_rf와 m21_rf의 예측을 실시하고 분류 성능을 평가해 보자. pred2_rf &lt;- predict(m2_rf, test_M) pred21_rf &lt;- predict(m21_rf, test_M) confusionMatrix(data = pred2_rf, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 45 27 ## yes 20 58 ## ## Accuracy : 0.6867 ## 95% CI : (0.6059, 0.7598) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 0.001728 ## ## Kappa : 0.37 ## ## Mcnemar&#39;s Test P-Value : 0.381471 ## ## Sensitivity : 0.6824 ## Specificity : 0.6923 ## Pos Pred Value : 0.7436 ## Neg Pred Value : 0.6250 ## Precision : 0.7436 ## Recall : 0.6824 ## F1 : 0.7117 ## Prevalence : 0.5667 ## Detection Rate : 0.3867 ## Detection Prevalence : 0.5200 ## Balanced Accuracy : 0.6873 ## ## &#39;Positive&#39; Class : yes ## confusionMatrix(data = pred21_rf, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 47 27 ## yes 18 58 ## ## Accuracy : 0.7 ## 95% CI : (0.6199, 0.772) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 0.0005439 ## ## Kappa : 0.3989 ## ## Mcnemar&#39;s Test P-Value : 0.2330380 ## ## Sensitivity : 0.6824 ## Specificity : 0.7231 ## Pos Pred Value : 0.7632 ## Neg Pred Value : 0.6351 ## Precision : 0.7632 ## Recall : 0.6824 ## F1 : 0.7205 ## Prevalence : 0.5667 ## Detection Rate : 0.3867 ## Detection Prevalence : 0.5067 ## Balanced Accuracy : 0.7027 ## ## &#39;Positive&#39; Class : yes ## 3.4 Boosting Bagging과 random forest는 decision tree 모형의 분산을 낮추기 위해 제안된 모형이다. 공통적으로 적용되는 방식은 bootstrap으로 생성된 다수의 training data를 대상으로 tree 모형을 각각 적합하고 통합하여 최종 예측 결과를 산출하는 것이다. Boosting도 decision tree 모형의 분산을 낮추기 위해 제안된 모형이며, 다수의 tree 모형을 적합하지만 전혀 다른 방법이 적용된다. 우선 bootstrap을 사용하지 않기 때문에 여러 개의 독립된 tree 모형을 통합하는 과정이 없다. 대신 순차적인 통합 방식을 사용하는데, 우선 첫 번째 단계에서 원자료를 대상으로 단순한 형태의 tree 모형을 적합한다. 단순한 형태의 tree 모형이란 작은 횟수의 분할이 이루어진 모형을 의미한다. 이어서 두 번째 단계에서는 원자료가 아닌 첫 번째 단계에서 생성된 잔차를 대상으로 다시 단순한 형태의 tree 모형을 적합한다. 이후 단계에서도 바로 전 단계 모형에서 생성된 잔차를 대상으로 단순한 형태의 tree 모형의 적합 과정을 반복한다. 단순한 tree 모형은 정확도가 낮은 예측 모형이지만, 정확도가 낮은 여러 개의 모형을 순차적으로 통합하여 정확도가 높은 모형을 생성하는 것이 boosting의 기본 개념이다. Boosting에는 몇 가지 다른 방식이 있는데, 여기에서는 Stochastic gradient boosting 기법에 대해 살펴보겠다. 회귀 모형 \\(Y = f(X) + \\varepsilon\\) 에 대한 boosting 기법은 다음과 같다. Tuning parameter인 분할 횟수 \\(d\\) 와 반복 횟수 \\(B\\), 그리고 \\(\\lambda\\) 값 지정 반응변수의 평균 \\(\\overline{y}\\) 를 함수 \\(f(x)\\) 의 첫 번째 추정값으로 지정: \\(\\hat{f}(x) = \\overline{y}\\) 다음의 절차를 \\(b = 1, \\ldots, B\\) 에 대해 반복 Training data 중 일부분 선택 관찰값과 현재 모형의 추정값 차이(잔차) 계산: \\(r = y - \\hat{f}(x)\\) 잔차를 대상으로 분할 횟수 \\(d\\) 에 의한 regression tree 모형 \\(\\hat{f}^{b}(x)\\) 적합 추정 모형 업데이트: \\(\\hat{f}(x) \\leftarrow \\hat{f}(x) + \\lambda \\hat{f}^{b}(x)\\) 최종 boosting 모형의 적합 결과는 다음과 같다. \\[\\begin{equation} \\hat{f}(x) = \\sum_{b=1}^{B} \\lambda \\hat{f}^{b}(x) \\end{equation}\\] 위에서 살펴본 boosting 절차에서 사용된 3가지 tuning parameter의 기능을 살펴보자. 첫 번째 tuning parameter인 \\(d\\) 는 tree 모형의 분할 횟수를 지정하는 것으로써, 대부분 단순한 형태의 tree 모형을 적합하는 것이 좋은 결과를 보이는 것으로 알려져 있다. \\(d=1\\) 으로도 좋은 결과를 얻는 경우도 가끔 있다. 두 번째 tuning parameter인 \\(B\\) 는 boosting 반복 횟수를 지정한다. 큰 값을 지정할수록 training data에 대한 더 정확한 적합이 이루어지겠지만, 지나치게 많은 반복을 실행하는 것은 overfitting의 가능성을 높이는 것이 된다. 세 번째 tuning parameter인 \\(\\lambda\\) 는 추정 모형의 업데이트 속도, 즉 learning rate를 조절하는 역할을 하는 shrinkage parameter이다. 큰 값을 지정하면 작은 횟수의 반복으로도 정확도가 높은 모형을 빠르게 만들 수 있지만, overfitting의 가능성이 매우 높아질 수 있다. 가능한 작은 값을 지정하여 천천히 최적 모형을 만드는 것이 일반적으로 적용되는 방식이다. \\(\\bullet\\) caret에 의한 Stochastic Gradient Boosting caret에서 함수 train()으로 stochastic gradient boosting을 실행하기 위해서는 패키지 gbm이 설치되어야 하며, method = 'gbm'를 지정해야 한다. 예제 : MASS::Boston Regression tree 모형에 대한 random forest 예제로써 3.1.1절에서 살펴본 MASS::Boston을 사용해 보자. data(Boston, package=&quot;MASS&quot;) set.seed(123) train.id &lt;- createDataPartition(Boston$medv, p = 0.7, list = FALSE)[,1] train_B &lt;- Boston |&gt; slice(train.id) test_B &lt;- Boston |&gt; slice(-train.id) 함수 train()에서 사용되는 tuning parameter에는 boosting 반복 횟수를 지정하는 n.trees와 분할 횟수를 지정하는 interaction.depth, \\(\\lambda\\) 값을 지정하는 shrinkage, 그리고 terminal node를 구성하는 최소 자료 수를 지정하는 n.minobsinnode가 있다. 네 종류의 tuning parameter 값을 지정하는 방법은 tuneLength로 할 수 있지만, 사용자가 grid를 조절할 수 있는 tuneGrid를 사용할 수 있다. 먼저 tuneLength에 의한 grid 조절로 boosting 모형을 적합해 보자. ctrl &lt;- trainControl(method = &#39;cv&#39;, number = 10) set.seed(123) m1_gbm &lt;- train(medv ~ ., data = train_B, method = &#39;gbm&#39;, tuneLength = 5, trControl = ctrl, verbose = FALSE) verbose = FALSE을 지정해서 추정 과정에 대한 출력을 생략했다. m1_gbm의 적합 결과를 살펴보자. m1_gbm ## Stochastic Gradient Boosting ## ## 356 samples ## 13 predictor ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 320, 322, 320, 321, 321, 320, ... ## Resampling results across tuning parameters: ## ## interaction.depth n.trees RMSE Rsquared MAE ## 1 50 3.858615 0.8189152 2.730638 ## 1 100 3.696736 0.8309254 2.585992 ## 1 150 3.702450 0.8322276 2.604707 ## 1 200 3.706732 0.8329330 2.627899 ## 1 250 3.658608 0.8373381 2.587262 ## 2 50 3.627679 0.8402229 2.525312 ## 2 100 3.506705 0.8505777 2.462068 ## 2 150 3.460677 0.8560659 2.484307 ## 2 200 3.429762 0.8590447 2.455930 ## 2 250 3.390288 0.8628167 2.460451 ## 3 50 3.509498 0.8480217 2.466777 ## 3 100 3.381205 0.8607337 2.394221 ## 3 150 3.322271 0.8658518 2.360338 ## 3 200 3.312736 0.8672853 2.355567 ## 3 250 3.307265 0.8669862 2.372043 ## 4 50 3.383680 0.8607782 2.390064 ## 4 100 3.262512 0.8716540 2.248759 ## 4 150 3.206835 0.8760505 2.239821 ## 4 200 3.189270 0.8767745 2.223124 ## 4 250 3.207784 0.8760043 2.239024 ## 5 50 3.432479 0.8589551 2.382405 ## 5 100 3.311370 0.8685337 2.346983 ## 5 150 3.308600 0.8696146 2.383152 ## 5 200 3.298083 0.8702767 2.363054 ## 5 250 3.277067 0.8718841 2.350316 ## ## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1 ## ## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10 ## RMSE was used to select the optimal model using the smallest value. ## The final values used for the model were n.trees = 200, interaction.depth = ## 4, shrinkage = 0.1 and n.minobsinnode = 10. 네 종류의 tuning parameter 중 shrinkage와 n.minobsinnode는 0.1과 10으로 각각 고정되었음을 알 수 있다. Tuning parameter n.trees와 interaction.depth의 값에 따른 CV RMSE의 변화를 그래프로 나타내자. ggplot(m1_gbm) 그림 3.16: Boston 자료에 대한 boosting 모형의 n.trees와 interaction.depth의 값에 따른 CV RMSE의 변화 m1_gbm에서 최적 tuning parameter 값은 다음과 같이 알아볼 수 있다. m1_gbm$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 19 200 4 0.1 10 tuneGrid를 이용하여 사용자가 지정한 tuning parameter의 grid를 근거로 모형을 적합해 보자. tuneGrid에는 네 종류 tuning parameter의 grid로 구성된 데이터 프레임을 지정해야 하는데, 함수 expand.grid()를 사용하는 것이 편하다. grid &lt;- expand.grid(n.trees = seq(50, 350, by = 50), interaction.depth = 2:7, shrinkage = c(0.01, 0.1), n.minobsinnode = c(5, 10)) head(grid) ## n.trees interaction.depth shrinkage n.minobsinnode ## 1 50 2 0.01 5 ## 2 100 2 0.01 5 ## 3 150 2 0.01 5 ## 4 200 2 0.01 5 ## 5 250 2 0.01 5 ## 6 300 2 0.01 5 tail(grid) ## n.trees interaction.depth shrinkage n.minobsinnode ## 163 100 7 0.1 10 ## 164 150 7 0.1 10 ## 165 200 7 0.1 10 ## 166 250 7 0.1 10 ## 167 300 7 0.1 10 ## 168 350 7 0.1 10 tuneGrid에 의한 모형을 적합해 보자. set.seed(123) m2_gbm &lt;- train(medv ~ ., data = train_B, method = &#39;gbm&#39;, tuneGrid = grid, trControl = ctrl, verbose = FALSE) m2_gbm에는 4종류 tuning parameter의 조합에 따른 168번의 모형 적합 결과가 입력되어 있다. Tuning parameter 값에 따른 모형 m2_gbm의 CV RMSE의 변화를 그래프로 나타내자. ggplot(m2_gbm) 그림 3.17: Boston 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV RMSE의 변화 m2_gbm에서 최적 tuning parameter 값을 알아보자. m2_gbm$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 157 150 7 0.1 5 두 모형 m1_gbm과 m2_gbm을 사용해서 test data에 대한 예측을 실시하고 결과를 평가해 보자. pred1_gbm &lt;- predict(m1_gbm, test_B) pred2_gbm &lt;- predict(m2_gbm, test_B) defaultSummary(data.frame(pred = pred1_gbm, obs = test_B$medv)) ## RMSE Rsquared MAE ## 3.6560793 0.8553609 2.4901670 defaultSummary(data.frame(pred = pred2_gbm, obs = test_B$medv)) ## RMSE Rsquared MAE ## 3.5168430 0.8682371 2.3597790 두 모형의 예측 결과를 그래프로 나타내 보자. 그림 3.18: Boston 자료에 대한 boosting 모형의 예측 결과 예제 : carData::Mroz Classification tree 모형에 대한 Boosting 예제로써 3.1.2절에서 살펴본 carData::Mroz을 사용해 보자. data(Mroz, package=&quot;carData&quot;) set.seed(123) x.id &lt;- createDataPartition(Mroz$lfp, p = 0.8, list = FALSE)[,1] train_M &lt;- Mroz |&gt; slice(x.id) test_M &lt;- Mroz |&gt; slice(-x.id) tuneLength에 의한 모형 적합을 진행해 보자. ctrl &lt;- trainControl(method=&#39;cv&#39;, number=10) set.seed(123) m3_gbm &lt;- train(lfp ~ ., data = train_M, method = &#39;gbm&#39;, tuneLength = 10, trControl = ctrl, verbose = FALSE) m3_gbm에서 최적 tuning parameter 값을 알아보자. m3_gbm$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 73 150 8 0.1 10 네 종류의 tuning parameter 중 shrinkage와 n.minobsinnode는 0.1과 10으로 각각 고정되어 있다. n.trees와 interaction.depth의 값에 따른 CV RMSE의 변화를 그래프로 나타내자. ggplot(m3_gbm) 그림 3.19: Mroz 자료에 대한 boosting 모형의 n.trees와 interaction.depth의 값에 따른 CV Accuracy의 변화 tuneGrid를 이용하여 사용자가 지정한 tuning parameter의 grid를 근거로 모형을 적합해 보자. grid &lt;- expand.grid(n.trees = seq(50, 450, by = 50), interaction.depth = 1:10, shrinkage = c(0.01, 0.1), n.minobsinnode = c(5, 10)) set.seed(123) m4_gbm &lt;- train(lfp ~ ., data = train_M, method = &#39;gbm&#39;, tuneGrid = grid, trControl = ctrl, verbose = FALSE) Tuning parameter 값에 따른 모형 m4_gbm의 CV Accuracy의 변화를 그래프로 나타내자. ggplot(m4_gbm) 그림 3.20: Boston 자료에 대한 boosting 모형의 4 종류 tuning parameter 값에 따른 CV Accuracy의 변화 m4_gbm에서 최적 tuning parameter 값을 알아보자. m4_gbm$bestTune ## n.trees interaction.depth shrinkage n.minobsinnode ## 282 150 6 0.1 10 Test data에 대한 모형 m3_gbm과 m4_gbm에 의한 예측을 실시하고 분류 성능을 평가해 보자. pred3_gbm &lt;- predict(m3_gbm, test_M) pred4_gbm &lt;- predict(m4_gbm, test_M) confusionMatrix(data = pred3_gbm, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 47 28 ## yes 18 57 ## ## Accuracy : 0.6933 ## 95% CI : (0.6129, 0.7659) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 0.0009839 ## ## Kappa : 0.3867 ## ## Mcnemar&#39;s Test P-Value : 0.1845161 ## ## Sensitivity : 0.6706 ## Specificity : 0.7231 ## Pos Pred Value : 0.7600 ## Neg Pred Value : 0.6267 ## Precision : 0.7600 ## Recall : 0.6706 ## F1 : 0.7125 ## Prevalence : 0.5667 ## Detection Rate : 0.3800 ## Detection Prevalence : 0.5000 ## Balanced Accuracy : 0.6968 ## ## &#39;Positive&#39; Class : yes ## confusionMatrix(data = pred4_gbm, reference = test_M$lfp, positive = &quot;yes&quot;, mode = &quot;everything&quot;) ## Confusion Matrix and Statistics ## ## Reference ## Prediction no yes ## no 48 23 ## yes 17 62 ## ## Accuracy : 0.7333 ## 95% CI : (0.6551, 0.8022) ## No Information Rate : 0.5667 ## P-Value [Acc &gt; NIR] : 1.777e-05 ## ## Kappa : 0.4628 ## ## Mcnemar&#39;s Test P-Value : 0.4292 ## ## Sensitivity : 0.7294 ## Specificity : 0.7385 ## Pos Pred Value : 0.7848 ## Neg Pred Value : 0.6761 ## Precision : 0.7848 ## Recall : 0.7294 ## F1 : 0.7561 ## Prevalence : 0.5667 ## Detection Rate : 0.4133 ## Detection Prevalence : 0.5267 ## Balanced Accuracy : 0.7339 ## ## &#39;Positive&#39; Class : yes ## "],["references.html", "References", " References "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
